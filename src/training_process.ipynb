{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Open this notebook in Google Colab](https://colab.research.google.com/drive/1H-btvdc3lU7wQ4DhZWaiurikzYqko5Ss?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "simAT-bxyzPa",
        "outputId": "2d0acd11-bc36-4b12-c10d-00d8904aa20c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"Filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"/content/drive/MyDrive/DSA_Comp/Dataset/HgB_11.9gdl_Individual06_01.jpeg\",\n          \"/content/drive/MyDrive/DSA_Comp/Dataset/HgB_17.3gdl_Individual05_04.jpeg\",\n          \"/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10.7gdl_Individual03.heic\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hb\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2455792928592215,\n        \"min\": 4.1,\n        \"max\": 17.3,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          4.1,\n          11.9,\n          7.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ethnicity\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"ChineseOrigin\",\n          \"MiddleEasternOrigin\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"IndividualID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 41,\n        \"min\": 1,\n        \"max\": 106,\n        \"num_unique_values\": 19,\n        \"samples\": [\n          100,\n          101\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Image No\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 5,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Extension\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"jpeg\",\n          \"heic\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1e3d4095-1014-4cb9-b60d-06838c9507d5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>Hb</th>\n",
              "      <th>Ethnicity</th>\n",
              "      <th>IndividualID</th>\n",
              "      <th>Image No</th>\n",
              "      <th>Extension</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/Random...</td>\n",
              "      <td>7.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_17...</td>\n",
              "      <td>17.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_12...</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/Random...</td>\n",
              "      <td>13.7</td>\n",
              "      <td>MiddleEasternOrigin</td>\n",
              "      <td>101</td>\n",
              "      <td>0</td>\n",
              "      <td>jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_8....</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_12...</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_17...</td>\n",
              "      <td>17.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_17...</td>\n",
              "      <td>17.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_8....</td>\n",
              "      <td>8.9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>102</td>\n",
              "      <td>2</td>\n",
              "      <td>jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/Random...</td>\n",
              "      <td>11.6</td>\n",
              "      <td>MiddleEasternOrigin</td>\n",
              "      <td>103</td>\n",
              "      <td>0</td>\n",
              "      <td>jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_17...</td>\n",
              "      <td>17.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_8....</td>\n",
              "      <td>8.9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>104</td>\n",
              "      <td>1</td>\n",
              "      <td>jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/Random...</td>\n",
              "      <td>7.8</td>\n",
              "      <td>ChineseOrigin</td>\n",
              "      <td>105</td>\n",
              "      <td>0</td>\n",
              "      <td>jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_12...</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...</td>\n",
              "      <td>10.7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>heic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_17...</td>\n",
              "      <td>17.3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_11...</td>\n",
              "      <td>11.9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/Random...</td>\n",
              "      <td>4.1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>106</td>\n",
              "      <td>0</td>\n",
              "      <td>jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_16...</td>\n",
              "      <td>16.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>jpeg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>/content/drive/MyDrive/DSA_Comp/Dataset/HgB_12...</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e3d4095-1014-4cb9-b60d-06838c9507d5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1e3d4095-1014-4cb9-b60d-06838c9507d5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1e3d4095-1014-4cb9-b60d-06838c9507d5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ebf69059-887b-4abe-90fe-3f11f2b8742f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ebf69059-887b-4abe-90fe-3f11f2b8742f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ebf69059-887b-4abe-90fe-3f11f2b8742f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_94116f86-15c1-4bfc-86a9-7a98d9a9d370\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_94116f86-15c1-4bfc-86a9-7a98d9a9d370 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             Filename    Hb  \\\n",
              "0   /content/drive/MyDrive/DSA_Comp/Dataset/Random...   7.0   \n",
              "1   /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "2   /content/drive/MyDrive/DSA_Comp/Dataset/HgB_17...  17.3   \n",
              "3   /content/drive/MyDrive/DSA_Comp/Dataset/HgB_12...  12.0   \n",
              "4   /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "5   /content/drive/MyDrive/DSA_Comp/Dataset/Random...  13.7   \n",
              "6   /content/drive/MyDrive/DSA_Comp/Dataset/HgB_8....   8.0   \n",
              "7   /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "8   /content/drive/MyDrive/DSA_Comp/Dataset/HgB_12...  12.0   \n",
              "9   /content/drive/MyDrive/DSA_Comp/Dataset/HgB_17...  17.3   \n",
              "10  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_17...  17.3   \n",
              "11  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_8....   8.9   \n",
              "12  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "13  /content/drive/MyDrive/DSA_Comp/Dataset/Random...  11.6   \n",
              "14  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "15  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_17...  17.3   \n",
              "16  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_8....   8.9   \n",
              "17  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "18  /content/drive/MyDrive/DSA_Comp/Dataset/Random...   7.8   \n",
              "19  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "20  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_12...  12.0   \n",
              "21  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "22  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "23  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "24  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "25  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_10...  10.7   \n",
              "26  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_17...  17.3   \n",
              "27  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_11...  11.9   \n",
              "28  /content/drive/MyDrive/DSA_Comp/Dataset/Random...   4.1   \n",
              "29  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_16...  16.0   \n",
              "30  /content/drive/MyDrive/DSA_Comp/Dataset/HgB_12...  12.0   \n",
              "\n",
              "              Ethnicity  IndividualID  Image No Extension  \n",
              "0                   NaN           100         0      jpeg  \n",
              "1                   NaN             7         0      heic  \n",
              "2                   NaN             5         2      jpeg  \n",
              "3                   NaN             2         4       jpg  \n",
              "4                   NaN             9         0      heic  \n",
              "5   MiddleEasternOrigin           101         0       jpg  \n",
              "6                   NaN             3         1       jpg  \n",
              "7                   NaN             2         0      heic  \n",
              "8                   NaN             2         1       jpg  \n",
              "9                   NaN             5         1      jpeg  \n",
              "10                  NaN             5         5      jpeg  \n",
              "11                  NaN           102         2      jpeg  \n",
              "12                  NaN            11         0      heic  \n",
              "13  MiddleEasternOrigin           103         0       jpg  \n",
              "14                  NaN            10         0      heic  \n",
              "15                  NaN             5         4      jpeg  \n",
              "16                  NaN           104         1      jpeg  \n",
              "17                  NaN             8         0      heic  \n",
              "18        ChineseOrigin           105         0       jpg  \n",
              "19                  NaN             5         0      heic  \n",
              "20                  NaN             2         2       jpg  \n",
              "21                  NaN             1         0      heic  \n",
              "22                  NaN            12         0      heic  \n",
              "23                  NaN             3         0      heic  \n",
              "24                  NaN             6         0      heic  \n",
              "25                  NaN             4         0      heic  \n",
              "26                  NaN             5         3      jpeg  \n",
              "27                  NaN             6         1      jpeg  \n",
              "28                  NaN           106         0       jpg  \n",
              "29                  NaN             4         1      jpeg  \n",
              "30                  NaN             2         3       jpg  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/DSA_Comp/dataset.csv\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW5hAmwFxArL"
      },
      "source": [
        "### Training SSL Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjEc1DPTw5xm",
        "outputId": "40d056e4-afee-4e41-e214-92c864ea8696"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import joblib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pillow_heif\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Register HEIF opener (if using HEIC/HEIF images)\n",
        "pillow_heif.register_heif_opener()\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration / Reproducibility\n",
        "# ---------------------------\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def make_run_dir(base=\"models\", prefix=\"run\"):\n",
        "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    run_dir = os.path.join(base, f\"{prefix}_{ts}\")\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    return run_dir\n",
        "\n",
        "def safe_save_json(df, path):\n",
        "    try:\n",
        "        df.to_json(path, orient=\"records\", lines=True)\n",
        "    except Exception:\n",
        "        df.to_csv(path.replace(\".json\", \".csv\"), index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset Classes\n",
        "# ---------------------------\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "import os\n",
        "\n",
        "class HbImageDataset(Dataset):\n",
        "    \"\"\"Labeled dataset for supervised + SSL training\"\"\"\n",
        "    def __init__(self, df, transform=None, path_col=\"Filename\", target_col=\"Hb\", n_views=2):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.path_col = path_col\n",
        "        self.target_col = target_col\n",
        "        self.n_views = n_views\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[self.path_col]).convert(\"RGB\")\n",
        "        if self.n_views == 1:\n",
        "            return self.transform(img).unsqueeze(0), torch.tensor(row[self.target_col], dtype=torch.float32)\n",
        "        else:\n",
        "            views = [self.transform(img) for _ in range(self.n_views)]\n",
        "            return torch.stack(views), torch.tensor(row[self.target_col], dtype=torch.float32)\n",
        "\n",
        "class UnlabelledImageDataset(Dataset):\n",
        "    \"\"\"Unlabeled dataset for SSL pretraining, supports subfolders\"\"\"\n",
        "    def __init__(self, root_dir, transform=None, n_views=2):\n",
        "        self.image_paths = []\n",
        "        for dirpath, _, filenames in os.walk(root_dir):\n",
        "            for fname in filenames:\n",
        "                if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".heic\", \".heif\")):\n",
        "                    self.image_paths.append(os.path.join(dirpath, fname))\n",
        "        self.transform = transform\n",
        "        self.n_views = n_views\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.n_views == 1:\n",
        "            return self.transform(img).unsqueeze(0)\n",
        "        else:\n",
        "            views = [self.transform(img) for _ in range(self.n_views)]\n",
        "            return torch.stack(views)\n",
        "\n",
        "class CombinedImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Combines a labeled DataFrame dataset and an unlabelled image directory for SSL.\n",
        "    Supports multi-view transformations for contrastive learning.\n",
        "    Returns (views, target, is_labeled).\n",
        "    Logs the number of labeled and unlabeled images loaded.\n",
        "    \"\"\"\n",
        "    def __init__(self, labelled_df=None, unlabelled_dir=None, transform=None, path_col=\"Filename\", target_col=\"Hb\", n_views=2):\n",
        "        self.labelled_dataset = None\n",
        "        self.unlabelled_dataset = None\n",
        "\n",
        "        if labelled_df is not None:\n",
        "            self.labelled_dataset = HbImageDataset(\n",
        "                labelled_df, transform=transform, path_col=path_col, target_col=target_col, n_views=n_views\n",
        "            )\n",
        "            print(f\"[INFO] Loaded {len(self.labelled_dataset)} labeled images for SSL pretraining.\")\n",
        "\n",
        "        if unlabelled_dir is not None:\n",
        "            self.unlabelled_dataset = UnlabelledImageDataset(\n",
        "                unlabelled_dir, transform=transform, n_views=n_views\n",
        "            )\n",
        "            print(f\"[INFO] Loaded {len(self.unlabelled_dataset)} unlabeled images for SSL pretraining.\")\n",
        "\n",
        "        # Compute total length\n",
        "        self.labelled_len = len(self.labelled_dataset) if self.labelled_dataset else 0\n",
        "        self.unlabelled_len = len(self.unlabelled_dataset) if self.unlabelled_dataset else 0\n",
        "        self.total_len = self.labelled_len + self.unlabelled_len\n",
        "        print(f\"[INFO] Total images in combined dataset: {self.total_len}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.labelled_dataset and idx < self.labelled_len:\n",
        "            views, target = self.labelled_dataset[idx]\n",
        "            return views, target, True  # True indicates labeled\n",
        "        else:\n",
        "            unlabelled_idx = idx - self.labelled_len\n",
        "            views = self.unlabelled_dataset[unlabelled_idx]\n",
        "            dummy_target = torch.tensor(-1.0)  # Dummy target for unlabeled\n",
        "            return views, dummy_target, False  # False indicates unlabelled\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Transforms\n",
        "# ---------------------------\n",
        "ssl_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    # transforms.ColorJitter(0.4,0.4,0.4,0.1), => May be too strong for hb predictions\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# ResNet Backbone + Projection Head\n",
        "# ---------------------------\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim=256, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def get_ssl_backbone(pretrained=True):\n",
        "    resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "    return backbone.to(device)\n",
        "\n",
        "# ---------------------------\n",
        "# NT-Xent Loss\n",
        "# ---------------------------\n",
        "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
        "    z_i = nn.functional.normalize(z_i, dim=1)\n",
        "    z_j = nn.functional.normalize(z_j, dim=1)\n",
        "    batch_size = z_i.size(0)\n",
        "    representations = torch.cat([z_i, z_j], dim=0)\n",
        "    similarity_matrix = torch.matmul(representations, representations.T) / temperature\n",
        "    mask = torch.eye(2*batch_size, device=z_i.device).bool()\n",
        "    similarity_matrix = similarity_matrix.masked_fill(mask, -1e9)\n",
        "    labels = torch.arange(batch_size, device=z_i.device)\n",
        "    labels = torch.cat([labels + batch_size, labels], dim=0)\n",
        "    return nn.CrossEntropyLoss()(similarity_matrix, labels)\n",
        "\n",
        "# ---------------------------\n",
        "# SSL Pretraining (supports unlabeled or labeled)\n",
        "# ---------------------------\n",
        "def pretrain_ssl(labelled_df=None,\n",
        "                          unlabelled_dir=None,\n",
        "                          transform=ssl_transform,\n",
        "                          epochs=20,\n",
        "                          batch_size=8,\n",
        "                          lr=1e-3,\n",
        "                          num_workers=2,\n",
        "                          run_dir=None):\n",
        "\n",
        "    dataset = CombinedImageDataset(labelled_df=labelled_df,\n",
        "                                   unlabelled_dir=unlabelled_dir,\n",
        "                                   transform=transform,\n",
        "                                   n_views=2)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    backbone = get_ssl_backbone(pretrained=True)\n",
        "    proj_head = ProjectionHead().to(device)\n",
        "    optimizer = torch.optim.Adam(list(backbone.parameters()) + list(proj_head.parameters()), lr=lr)\n",
        "\n",
        "    backbone.train()\n",
        "    proj_head.train()\n",
        "    ssl_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for batch in loader:\n",
        "            views, target, is_labeled = batch\n",
        "            v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "\n",
        "            feats1 = backbone(v1).view(v1.size(0), -1)\n",
        "            feats2 = backbone(v2).view(v2.size(0), -1)\n",
        "            z1 = proj_head(feats1)\n",
        "            z2 = proj_head(feats2)\n",
        "            loss = nt_xent_loss(z1, z2)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader) if len(loader) > 0 else float(\"nan\")\n",
        "        ssl_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - SSL Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    if run_dir:\n",
        "        torch.save(backbone.state_dict(), os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"))\n",
        "        torch.save(proj_head.state_dict(), os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"))\n",
        "        pd.DataFrame({\"ssl_loss\": ssl_losses}).to_csv(os.path.join(run_dir, \"ssl_loss_history.csv\"), index=False)\n",
        "\n",
        "    return backbone, proj_head, ssl_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf7bc6rAx13L"
      },
      "source": [
        "#### Train SSL Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAzqUvEpxzZJ",
        "outputId": "de75a3d7-2df2-4cd4-99ca-a4b649b801ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Loaded 31 labeled images for SSL pretraining.\n",
            "[INFO] Loaded 710 unlabeled images for SSL pretraining.\n",
            "[INFO] Total images in combined dataset: 741\n",
            "Epoch 1/20 - SSL Loss: 1.9943\n",
            "Epoch 2/20 - SSL Loss: 1.8129\n",
            "Epoch 3/20 - SSL Loss: 1.7438\n",
            "Epoch 4/20 - SSL Loss: 1.7839\n",
            "Epoch 5/20 - SSL Loss: 1.7296\n",
            "Epoch 6/20 - SSL Loss: 1.7198\n",
            "Epoch 7/20 - SSL Loss: 1.7132\n",
            "Epoch 8/20 - SSL Loss: 1.6955\n",
            "Epoch 9/20 - SSL Loss: 1.6752\n",
            "Epoch 10/20 - SSL Loss: 1.6970\n",
            "Epoch 11/20 - SSL Loss: 1.6768\n",
            "Epoch 12/20 - SSL Loss: 1.6744\n",
            "Epoch 13/20 - SSL Loss: 1.6557\n",
            "Epoch 14/20 - SSL Loss: 1.6431\n",
            "Epoch 15/20 - SSL Loss: 1.6359\n",
            "Epoch 16/20 - SSL Loss: 1.6471\n",
            "Epoch 17/20 - SSL Loss: 1.6447\n",
            "Epoch 18/20 - SSL Loss: 1.6338\n",
            "Epoch 19/20 - SSL Loss: 1.6317\n",
            "Epoch 20/20 - SSL Loss: 1.6365\n"
          ]
        }
      ],
      "source": [
        "run_dir = make_run_dir(base=\"models\", prefix=\"ssl_combined\")\n",
        "\n",
        "backbone, proj_head, ssl_losses = pretrain_ssl(\n",
        "    labelled_df=df,       # your labelled DataFrame\n",
        "    unlabelled_dir=\"/content/drive/MyDrive/DSA_Comp/Lip Images\",  # path to unlabelled images\n",
        "    transform=ssl_transform,\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    lr=1e-3,\n",
        "    num_workers=4,\n",
        "    run_dir=run_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76bNztcp3gzH",
        "outputId": "38e386c7-90ee-431f-e233-033e4ac79385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime\n",
        "import joblib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pillow_heif\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Register HEIF opener (if using HEIC/HEIF images)\n",
        "pillow_heif.register_heif_opener()\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration / Reproducibility\n",
        "# ---------------------------\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def make_run_dir(base=\"models\", prefix=\"run\"):\n",
        "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    run_dir = os.path.join(base, f\"{prefix}_{ts}\")\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    return run_dir\n",
        "\n",
        "def safe_save_json(df, path):\n",
        "    try:\n",
        "        df.to_json(path, orient=\"records\", lines=True)\n",
        "    except Exception:\n",
        "        df.to_csv(path.replace(\".json\", \".csv\"), index=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset Classes\n",
        "# ---------------------------\n",
        "class HbImageDataset(Dataset):\n",
        "    \"\"\"Labeled dataset for supervised + SSL training\"\"\"\n",
        "    def __init__(self, df, transform=None, path_col=\"Filename\", target_col=\"Hb\", n_views=2):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.path_col = path_col\n",
        "        self.target_col = target_col\n",
        "        self.n_views = n_views\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[self.path_col]).convert(\"RGB\")\n",
        "        if self.n_views == 1:\n",
        "            return self.transform(img).unsqueeze(0), torch.tensor(row[self.target_col], dtype=torch.float32)\n",
        "        else:\n",
        "            views = [self.transform(img) for _ in range(self.n_views)]\n",
        "            return torch.stack(views), torch.tensor(row[self.target_col], dtype=torch.float32)\n",
        "\n",
        "class UnlabelledImageDataset(Dataset):\n",
        "    \"\"\"Unlabeled dataset for SSL pretraining\"\"\"\n",
        "    def __init__(self, image_dir, transform=None, n_views=2):\n",
        "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
        "                            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".heic\", \".heif\"))]\n",
        "        self.transform = transform\n",
        "        self.n_views = n_views\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.n_views == 1:\n",
        "            return self.transform(img).unsqueeze(0)\n",
        "        else:\n",
        "            views = [self.transform(img) for _ in range(self.n_views)]\n",
        "            return torch.stack(views)\n",
        "\n",
        "# ---------------------------\n",
        "# Transforms\n",
        "# ---------------------------\n",
        "ssl_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(0.4,0.4,0.4,0.1),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# ResNet Backbone + Projection Head\n",
        "# ---------------------------\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim=256, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def get_ssl_backbone(pretrained=True):\n",
        "    resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "    return backbone.to(device)\n",
        "\n",
        "# ---------------------------\n",
        "# NT-Xent Loss\n",
        "# ---------------------------\n",
        "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
        "    z_i = nn.functional.normalize(z_i, dim=1)\n",
        "    z_j = nn.functional.normalize(z_j, dim=1)\n",
        "    batch_size = z_i.size(0)\n",
        "    representations = torch.cat([z_i, z_j], dim=0)\n",
        "    similarity_matrix = torch.matmul(representations, representations.T) / temperature\n",
        "    mask = torch.eye(2*batch_size, device=z_i.device).bool()\n",
        "    similarity_matrix = similarity_matrix.masked_fill(mask, -1e9)\n",
        "    labels = torch.arange(batch_size, device=z_i.device)\n",
        "    labels = torch.cat([labels + batch_size, labels], dim=0)\n",
        "    return nn.CrossEntropyLoss()(similarity_matrix, labels)\n",
        "\n",
        "# ---------------------------\n",
        "# SSL Pretraining (supports unlabeled or labeled)\n",
        "# ---------------------------\n",
        "def pretrain_ssl(dataset_or_df,\n",
        "                 transform,\n",
        "                 epochs=20,\n",
        "                 batch_size=8,\n",
        "                 lr=1e-3,\n",
        "                 num_workers=2,\n",
        "                 run_dir=None,\n",
        "                 save_projection=True,\n",
        "                 unlabelled=False):\n",
        "    if unlabelled:\n",
        "        dataset = UnlabelledImageDataset(dataset_or_df, transform=transform, n_views=2)\n",
        "    else:\n",
        "        dataset = HbImageDataset(dataset_or_df, transform=transform, n_views=2)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    backbone = get_ssl_backbone(pretrained=True)\n",
        "    proj_head = ProjectionHead().to(device)\n",
        "    optimizer = torch.optim.Adam(list(backbone.parameters()) + list(proj_head.parameters()), lr=lr)\n",
        "\n",
        "    backbone.train()\n",
        "    proj_head.train()\n",
        "    ssl_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for batch in loader:\n",
        "            if unlabelled:\n",
        "                v1, v2 = batch[:,0].to(device), batch[:,1].to(device)\n",
        "            else:\n",
        "                views, _ = batch\n",
        "                v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "\n",
        "            feats1 = backbone(v1).view(v1.size(0), -1)\n",
        "            feats2 = backbone(v2).view(v2.size(0), -1)\n",
        "            z1 = proj_head(feats1)\n",
        "            z2 = proj_head(feats2)\n",
        "            loss = nt_xent_loss(z1, z2)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(loader) if len(loader) > 0 else float(\"nan\")\n",
        "        ssl_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - SSL Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    if run_dir:\n",
        "        torch.save(backbone.state_dict(), os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"))\n",
        "        torch.save(proj_head.state_dict(), os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"))\n",
        "        pd.DataFrame({\"ssl_loss\": ssl_losses}).to_csv(os.path.join(run_dir, \"ssl_loss_history.csv\"), index=False)\n",
        "\n",
        "    return backbone, proj_head, ssl_losses\n",
        "\n",
        "# ---------------------------\n",
        "# Feature Extraction\n",
        "# ---------------------------\n",
        "def extract_embeddings(df, backbone, transform, path_col=\"Filename\", target_col=\"Hb\", batch_size=8, num_workers=2):\n",
        "    dataset = HbImageDataset(df, transform, n_views=1, path_col=path_col, target_col=target_col)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    backbone.eval()\n",
        "\n",
        "    feats, targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for views, hb in loader:\n",
        "            img = views[:,0].to(device)\n",
        "            emb = backbone(img).view(img.size(0), -1)\n",
        "            feats.append(emb.cpu().numpy())\n",
        "            targets.extend(hb.cpu().numpy())\n",
        "    if len(feats) == 0:\n",
        "        return np.zeros((0,512)), np.array([])\n",
        "    return np.vstack(feats), np.array(targets)\n",
        "\n",
        "# ---------------------------\n",
        "# Combine Metadata\n",
        "# ---------------------------\n",
        "def combine_metadata(features, df, cols_to_include=None):\n",
        "    df_reset = df.reset_index(drop=True)\n",
        "    if cols_to_include is None:\n",
        "        cols_to_include = [c for c in [\"IndividualID\",\"ImageNo\"] if c in df_reset.columns]\n",
        "    if cols_to_include:\n",
        "        metadata = df_reset[cols_to_include].values\n",
        "        if len(metadata) != len(features):\n",
        "            print(f\"Warning: metadata rows ({len(metadata)}) != feature rows ({len(features)}) -> ignoring metadata\")\n",
        "            return features\n",
        "        try:\n",
        "            metadata = metadata.astype(np.float32)\n",
        "        except Exception:\n",
        "            print(\"Metadata not numeric; skipping metadata concatenation.\")\n",
        "            return features\n",
        "        return np.hstack([features, metadata])\n",
        "    return features\n",
        "\n",
        "# ---------------------------\n",
        "# XGBoost + Optuna Objective\n",
        "# ---------------------------\n",
        "def objective(trial, X, y, cv_splits=5):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\",50,500),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\",2,10),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\",0.01,0.3,log=True),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\",0.5,1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.5,1.0),\n",
        "        \"random_state\": RANDOM_SEED,\n",
        "        \"verbosity\": 0,\n",
        "        \"n_jobs\": 1,\n",
        "        \"tree_method\": \"hist\"\n",
        "    }\n",
        "    rmses = []\n",
        "    kf = KFold(n_splits=cv_splits, shuffle=True, random_state=RANDOM_SEED)\n",
        "    for train_idx, val_idx in kf.split(X):\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        model.fit(X[train_idx], y[train_idx],\n",
        "                  eval_set=[(X[val_idx], y[val_idx])],\n",
        "                  verbose=False)\n",
        "        y_pred = model.predict(X[val_idx])\n",
        "        rmses.append(np.sqrt(mean_squared_error(y[val_idx], y_pred)))\n",
        "    return np.mean(rmses)\n",
        "\n",
        "# ---------------------------\n",
        "# Full Modular Pipeline\n",
        "# ---------------------------\n",
        "def run_ssl_supervised_pipeline(labeled_df,\n",
        "                                unlabelled_dir=None,\n",
        "                                ssl_epochs=20,\n",
        "                                ssl_batch=8,\n",
        "                                extract_batch=8,\n",
        "                                optuna_trials=30,\n",
        "                                fine_tune_backbone=True,\n",
        "                                run_base_dir=\"models\"):\n",
        "    run_dir = make_run_dir(run_base_dir)\n",
        "    print(f\"Run dir: {run_dir}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Step 1: SSL Pretraining\n",
        "    # -------------------\n",
        "    print(\"=== SSL Pretraining ===\")\n",
        "    if unlabelled_dir:\n",
        "        print(f\"Using unlabelled dataset: {unlabelled_dir}\")\n",
        "        backbone, proj_head, ssl_losses = pretrain_ssl(unlabelled_dir, ssl_transform,\n",
        "                                                       epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                       run_dir=run_dir, unlabelled=True)\n",
        "    else:\n",
        "        print(\"Using labeled dataset for SSL\")\n",
        "        backbone, proj_head, ssl_losses = pretrain_ssl(labeled_df, ssl_transform,\n",
        "                                                       epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                       run_dir=run_dir, unlabelled=False)\n",
        "\n",
        "    # -------------------\n",
        "    # Step 1b: Fine-tune on labeled images (optional)\n",
        "    # -------------------\n",
        "    if fine_tune_backbone:\n",
        "        print(\"=== Fine-tuning backbone on labeled data ===\")\n",
        "        dataset = HbImageDataset(labeled_df, transform=ssl_transform, n_views=2)\n",
        "        loader = DataLoader(dataset, batch_size=ssl_batch, shuffle=True, num_workers=2, pin_memory=True)\n",
        "        backbone.train()\n",
        "        proj_head.train()\n",
        "        optimizer = torch.optim.Adam(list(backbone.parameters()) + list(proj_head.parameters()), lr=1e-4)\n",
        "        for epoch in range(5):  # small fine-tune for demonstration\n",
        "            total_loss = 0.0\n",
        "            for views, _ in loader:\n",
        "                v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "                feats1 = backbone(v1).view(v1.size(0), -1)\n",
        "                feats2 = backbone(v2).view(v2.size(0), -1)\n",
        "                z1 = proj_head(feats1)\n",
        "                z2 = proj_head(feats2)\n",
        "                loss = nt_xent_loss(z1, z2)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            print(f\"Fine-tune Epoch {epoch+1}/5 - Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Step 2: Extract embeddings\n",
        "    # -------------------\n",
        "    print(\"=== Extracting embeddings ===\")\n",
        "    X_train, y_train = extract_embeddings(labeled_df, backbone, val_transform)\n",
        "    X_train = combine_metadata(X_train, labeled_df)\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Step 3: Optuna + XGBoost\n",
        "    # -------------------\n",
        "    print(\"=== Optuna Hyperparameter Tuning ===\")\n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "    study.optimize(lambda t: objective(t, X_train, y_train), n_trials=optuna_trials)\n",
        "    best_params = study.best_params\n",
        "    print(\"Best params:\", best_params)\n",
        "\n",
        "    final_model = xgb.XGBRegressor(**best_params, random_state=RANDOM_SEED, n_jobs=-1, tree_method=\"hist\")\n",
        "    final_model.fit(X_train, y_train)\n",
        "\n",
        "    # -------------------\n",
        "    # Step 4: Evaluation\n",
        "    # -------------------\n",
        "    y_pred = final_model.predict(X_train)\n",
        "    rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
        "    mae = mean_absolute_error(y_train, y_pred)\n",
        "    print(f\"RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
        "\n",
        "    # Save model and artifacts\n",
        "    torch.save(backbone.state_dict(), os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"))\n",
        "    torch.save(proj_head.state_dict(), os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"))\n",
        "    final_model.save_model(os.path.join(run_dir, \"xgb_final_model.json\"))\n",
        "    joblib.dump(final_model, os.path.join(run_dir, \"xgb_final_model.joblib\"))\n",
        "    pd.DataFrame({\"RMSE\":[rmse],\"MAE\":[mae]}).to_csv(os.path.join(run_dir,\"evaluation_metrics.csv\"), index=False)\n",
        "\n",
        "    return {\n",
        "        \"backbone\": backbone,\n",
        "        \"proj_head\": proj_head,\n",
        "        \"xgb_model\": final_model,\n",
        "        \"rmse\": rmse,\n",
        "        \"mae\": mae,\n",
        "        \"run_dir\": run_dir\n",
        "    }\n",
        "\n",
        "# ---------------------------\n",
        "# Example Accuracy Comparison\n",
        "# ---------------------------\n",
        "# unlabelled_dir = \"content/drive/MyDrive/DSA_Comp/Lip Images\"\n",
        "# results_baseline = run_ssl_supervised_pipeline(df, unlabelled_dir=None, fine_tune_backbone=False)\n",
        "# results_ssl_labeled = run_ssl_supervised_pipeline(df, unlabelled_dir=None, fine_tune_backbone=True)\n",
        "# results_ssl_unlabelled = run_ssl_supervised_pipeline(df, unlabelled_dir=unlabelled_dir, fine_tune_backbone=True)\n",
        "# print(\"Baseline RMSE:\", results_baseline[\"rmse\"])\n",
        "# print(\"SSL labeled RMSE:\", results_ssl_labeled[\"rmse\"])\n",
        "# print(\"SSL unlabelled RMSE:\", results_ssl_unlabelled[\"rmse\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXQdpkk8zbWg"
      },
      "outputs": [],
      "source": [
        "def run_ssl_supervised_pipeline(labeled_df,\n",
        "                                unlabelled_dir=None,\n",
        "                                ssl_epochs=20,\n",
        "                                ssl_batch=8,\n",
        "                                extract_batch=8,\n",
        "                                optuna_trials=30,\n",
        "                                fine_tune_backbone=True,\n",
        "                                run_base_dir=\"models\",\n",
        "                                load_run_dir=None):  # <-- new parameter\n",
        "    # -------------------\n",
        "    # Setup run directory\n",
        "    # -------------------\n",
        "    if load_run_dir:\n",
        "        run_dir = load_run_dir\n",
        "        print(f\"Loading models and artifacts from: {run_dir}\")\n",
        "    else:\n",
        "        run_dir = make_run_dir(run_base_dir)\n",
        "        print(f\"Run dir: {run_dir}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Step 1: SSL Pretraining or Load\n",
        "    # -------------------\n",
        "    backbone = get_ssl_backbone(pretrained=False)\n",
        "    proj_head = ProjectionHead(input_dim=512, hidden_dim=256, output_dim=128).to(device)\n",
        "\n",
        "    if load_run_dir:\n",
        "        # Load pretrained backbone and projection head\n",
        "        backbone.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"), map_location=device))\n",
        "        proj_head.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"), map_location=device))\n",
        "        backbone.to(device).eval()\n",
        "        proj_head.to(device).eval()\n",
        "        print(\"Loaded backbone and projection head from saved run.\")\n",
        "    else:\n",
        "        print(\"=== SSL Pretraining ===\")\n",
        "        if unlabelled_dir:\n",
        "            print(f\"Using unlabelled dataset: {unlabelled_dir}\")\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(unlabelled_dir, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=True)\n",
        "        else:\n",
        "            print(\"Using labeled dataset for SSL\")\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(labeled_df, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=False)\n",
        "\n",
        "        # Optional fine-tuning on labeled data\n",
        "        if fine_tune_backbone:\n",
        "            print(\"=== Fine-tuning backbone on labeled data ===\")\n",
        "            dataset = HbImageDataset(labeled_df, transform=ssl_transform, n_views=2)\n",
        "            loader = DataLoader(dataset, batch_size=ssl_batch, shuffle=True, num_workers=2, pin_memory=True)\n",
        "            backbone.train()\n",
        "            proj_head.train()\n",
        "            optimizer = torch.optim.Adam(list(backbone.parameters()) + list(proj_head.parameters()), lr=1e-4)\n",
        "            for epoch in range(5):  # small fine-tune\n",
        "                total_loss = 0.0\n",
        "                for views, _ in loader:\n",
        "                    v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "                    feats1 = backbone(v1).view(v1.size(0), -1)\n",
        "                    feats2 = backbone(v2).view(v2.size(0), -1)\n",
        "                    z1 = proj_head(feats1)\n",
        "                    z2 = proj_head(feats2)\n",
        "                    loss = nt_xent_loss(z1, z2)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "                print(f\"Fine-tune Epoch {epoch+1}/5 - Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "        # Save backbone/projection head after training/fine-tuning\n",
        "        torch.save(backbone.state_dict(), os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"))\n",
        "        torch.save(proj_head.state_dict(), os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"))\n",
        "\n",
        "    # -------------------\n",
        "    # Step 2: Extract embeddings\n",
        "    # -------------------\n",
        "    print(\"=== Extracting embeddings ===\")\n",
        "    X_train, y_train = extract_embeddings(labeled_df, backbone, val_transform)\n",
        "    X_train = combine_metadata(X_train, labeled_df)\n",
        "    print(f\"X_train shape: {X_train.shape}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Step 3: XGBoost\n",
        "    # -------------------\n",
        "    if load_run_dir and os.path.exists(os.path.join(run_dir, \"xgb_final_model.joblib\")):\n",
        "        final_model = joblib.load(os.path.join(run_dir, \"xgb_final_model.joblib\"))\n",
        "        print(\"Loaded XGBoost model from saved run.\")\n",
        "    else:\n",
        "        print(\"=== Optuna Hyperparameter Tuning ===\")\n",
        "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "        study.optimize(lambda t: objective(t, X_train, y_train), n_trials=optuna_trials)\n",
        "        best_params = study.best_params\n",
        "        print(\"Best params:\", best_params)\n",
        "\n",
        "        final_model = xgb.XGBRegressor(**best_params, random_state=RANDOM_SEED, n_jobs=-1, tree_method=\"hist\")\n",
        "        final_model.fit(X_train, y_train)\n",
        "\n",
        "        # Save model\n",
        "        final_model.save_model(os.path.join(run_dir, \"xgb_final_model.json\"))\n",
        "        joblib.dump(final_model, os.path.join(run_dir, \"xgb_final_model.joblib\"))\n",
        "\n",
        "    # -------------------\n",
        "    # Step 4: Evaluation\n",
        "    # -------------------\n",
        "    y_pred = final_model.predict(X_train)\n",
        "    rmse = np.sqrt(mean_squared_error(y_train, y_pred))\n",
        "    mae = mean_absolute_error(y_train, y_pred)\n",
        "    print(f\"RMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
        "    pd.DataFrame({\"RMSE\":[rmse],\"MAE\":[mae]}).to_csv(os.path.join(run_dir,\"evaluation_metrics.csv\"), index=False)\n",
        "\n",
        "    return {\n",
        "        \"backbone\": backbone,\n",
        "        \"proj_head\": proj_head,\n",
        "        \"xgb_model\": final_model,\n",
        "        \"rmse\": rmse,\n",
        "        \"mae\": mae,\n",
        "        \"run_dir\": run_dir\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIvb52mS00ns"
      },
      "source": [
        "#### LOOCV Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og8g8UiQ1T4z"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneOut, KFold\n",
        "\n",
        "\n",
        "def run_ssl_pipeline_loocv(labeled_df,\n",
        "                                  unlabelled_dir=None,\n",
        "                                  ssl_epochs=20,\n",
        "                                  ssl_batch=8,\n",
        "                                  fine_tune_backbone=True,\n",
        "                                  use_metadata=False,\n",
        "                                  optuna_trials=20,\n",
        "                                  run_base_dir=\"models\",\n",
        "                                  load_run_dir=None):\n",
        "    \"\"\"\n",
        "    SSL + supervised pipeline with LOOCV + Optuna tuning per fold.\n",
        "    Supports loading pretrained SSL backbone and projection head.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    labeled_df : pd.DataFrame\n",
        "        Labeled dataset with image paths and targets.\n",
        "    unlabelled_dir : str, optional\n",
        "        Directory for unlabeled images for SSL pretraining.\n",
        "    ssl_epochs : int\n",
        "        Epochs for SSL pretraining.\n",
        "    ssl_batch : int\n",
        "        Batch size for SSL pretraining.\n",
        "    fine_tune_backbone : bool\n",
        "        Whether to fine-tune backbone per fold on labeled data.\n",
        "    use_metadata : bool\n",
        "        Whether to include metadata columns.\n",
        "    optuna_trials : int\n",
        "        Number of trials for Optuna hyperparameter search per fold.\n",
        "    run_base_dir : str\n",
        "        Base directory to save models.\n",
        "    load_run_dir : str, optional\n",
        "        Directory to load pretrained SSL models from.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        LOOCV RMSE/MAE and backbone/proj_head.\n",
        "    \"\"\"\n",
        "    run_dir = load_run_dir if load_run_dir else make_run_dir(run_base_dir)\n",
        "    print(f\"Run directory: {run_dir}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Step 1: SSL Pretraining or Load\n",
        "    # -------------------\n",
        "    backbone = get_ssl_backbone(pretrained=False)\n",
        "    proj_head = ProjectionHead().to(device)\n",
        "\n",
        "    if load_run_dir and os.path.exists(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\")):\n",
        "        backbone.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"), map_location=device))\n",
        "        proj_head.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"), map_location=device))\n",
        "        backbone.to(device).eval()\n",
        "        proj_head.to(device).eval()\n",
        "        print(\"Loaded pretrained backbone and projection head from saved run.\")\n",
        "    else:\n",
        "        print(\"=== SSL Pretraining ===\")\n",
        "        if unlabelled_dir:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(unlabelled_dir, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=True)\n",
        "        else:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(labeled_df, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=False)\n",
        "\n",
        "    # -------------------\n",
        "    # LOOCV\n",
        "    # -------------------\n",
        "    loo = LeaveOneOut()\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(loo.split(labeled_df)):\n",
        "        print(f\"\\n--- LOOCV Fold {fold_idx+1}/{len(labeled_df)} ---\")\n",
        "        train_df, test_df = labeled_df.iloc[train_idx], labeled_df.iloc[test_idx]\n",
        "\n",
        "        # -------------------\n",
        "        # Fine-tune backbone on train fold (optional)\n",
        "        # -------------------\n",
        "        if fine_tune_backbone:\n",
        "            dataset = HbImageDataset(train_df, transform=ssl_transform, n_views=2)\n",
        "            loader = DataLoader(dataset, batch_size=ssl_batch, shuffle=True, num_workers=2, pin_memory=True)\n",
        "            backbone.train()\n",
        "            proj_head.train()\n",
        "            optimizer = torch.optim.Adam(list(backbone.parameters()) + list(proj_head.parameters()), lr=1e-4)\n",
        "            for epoch in range(2):  # small fine-tune per fold\n",
        "                total_loss = 0.0\n",
        "                for views, _ in loader:\n",
        "                    v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "                    feats1, feats2 = backbone(v1).view(v1.size(0), -1), backbone(v2).view(v2.size(0), -1)\n",
        "                    z1, z2 = proj_head(feats1), proj_head(feats2)\n",
        "                    loss = nt_xent_loss(z1, z2)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "                print(f\"Fold {fold_idx+1} Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "        # -------------------\n",
        "        # Extract embeddings\n",
        "        # -------------------\n",
        "        X_train, y_train = extract_embeddings(train_df, backbone, val_transform)\n",
        "        X_test, y_test = extract_embeddings(test_df, backbone, val_transform)\n",
        "\n",
        "        if use_metadata:\n",
        "            X_train = combine_metadata(X_train, train_df)\n",
        "            X_test = combine_metadata(X_test, test_df)\n",
        "\n",
        "        # -------------------\n",
        "        # Optuna Hyperparameter Tuning on train fold\n",
        "        # -------------------\n",
        "        def objective(trial):\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\",50,300),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\",2,10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\",0.01,0.3,log=True),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\",0.5,1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.5,1.0),\n",
        "                \"random_state\": RANDOM_SEED,\n",
        "                \"verbosity\": 0,\n",
        "                \"n_jobs\": 1,\n",
        "                \"tree_method\": \"hist\"\n",
        "            }\n",
        "            kf = KFold(n_splits=min(5, len(train_df)), shuffle=True, random_state=RANDOM_SEED)\n",
        "            rmses = []\n",
        "            for tr_idx, val_idx in kf.split(X_train):\n",
        "                model = xgb.XGBRegressor(**params)\n",
        "                model.fit(X_train[tr_idx], y_train[tr_idx])\n",
        "                y_pred = model.predict(X_train[val_idx])\n",
        "                rmses.append(np.sqrt(mean_squared_error(y_train[val_idx], y_pred)))\n",
        "            return np.mean(rmses)\n",
        "\n",
        "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "        study.optimize(objective, n_trials=optuna_trials, show_progress_bar=False)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best params fold {fold_idx+1}: {best_params}\")\n",
        "\n",
        "        # -------------------\n",
        "        # Train final XGBoost on train fold\n",
        "        # -------------------\n",
        "        final_model = xgb.XGBRegressor(**best_params, random_state=RANDOM_SEED, n_jobs=-1, tree_method=\"hist\")\n",
        "        final_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on test fold\n",
        "        y_pred = final_model.predict(X_test)\n",
        "        all_preds.append(y_pred[0])\n",
        "        all_targets.append(y_test[0])\n",
        "\n",
        "    # -------------------\n",
        "    # Evaluation\n",
        "    # -------------------\n",
        "    all_preds, all_targets = np.array(all_preds), np.array(all_targets)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "    print(f\"\\n=== LOOCV Evaluation ===\\nRMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
        "\n",
        "    # Save artifacts\n",
        "    pd.DataFrame({\"y_true\": all_targets, \"y_pred\": all_preds}).to_csv(os.path.join(run_dir, \"loocv_predictions.csv\"), index=False)\n",
        "    pd.DataFrame({\"RMSE\":[rmse], \"MAE\":[mae]}).to_csv(os.path.join(run_dir, \"evaluation_metrics.csv\"), index=False)\n",
        "    torch.save(backbone.state_dict(), os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"))\n",
        "    torch.save(proj_head.state_dict(), os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"))\n",
        "\n",
        "    return {\n",
        "        \"backbone\": backbone,\n",
        "        \"proj_head\": proj_head,\n",
        "        \"rmse\": rmse,\n",
        "        \"mae\": mae,\n",
        "        \"run_dir\": run_dir\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CZEsjqyO1X-I",
        "outputId": "d70862fe-51ab-422f-d4fa-b13d5979fc15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run directory: /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_combined\n",
            "Loaded pretrained backbone and projection head from saved run.\n",
            "\n",
            "--- LOOCV Fold 1/31 ---\n",
            "Fold 1 Epoch 1 Loss: 1.0316\n",
            "Fold 1 Epoch 2 Loss: 0.9918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:43:12,217] A new study created in memory with name: no-name-ce718c49-f158-4012-a4eb-c4e295c89a0b\n",
            "[I 2025-10-07 08:43:14,105] Trial 0 finished with value: 1.8764203026984503 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.8764203026984503.\n",
            "[I 2025-10-07 08:43:14,524] Trial 1 finished with value: 2.0132803620219115 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.8764203026984503.\n",
            "[I 2025-10-07 08:43:15,048] Trial 2 finished with value: 2.019119709399719 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 1.8764203026984503.\n",
            "[I 2025-10-07 08:43:15,771] Trial 3 finished with value: 1.8358797170867587 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 1.8358797170867587.\n",
            "[I 2025-10-07 08:43:16,972] Trial 4 finished with value: 1.9532155901045365 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 1.8358797170867587.\n",
            "[I 2025-10-07 08:43:18,162] Trial 5 finished with value: 1.7521197144197387 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:19,746] Trial 6 finished with value: 1.7896638798621538 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:21,717] Trial 7 finished with value: 1.7760752960030501 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:22,435] Trial 8 finished with value: 2.207400200507478 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:24,042] Trial 9 finished with value: 1.8503929122746297 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:27,296] Trial 10 finished with value: 2.0083083279943614 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.02964033652082882, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5072567334400258}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:30,585] Trial 11 finished with value: 1.7990520969297534 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.030241001410559513, 'subsample': 0.8751050092487717, 'colsample_bytree': 0.7758575504056592}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:32,616] Trial 12 finished with value: 1.9347304732197308 and parameters: {'n_estimators': 246, 'max_depth': 5, 'learning_rate': 0.08527855281875678, 'subsample': 0.8651265717543035, 'colsample_bytree': 0.7363570033289744}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:33,759] Trial 13 finished with value: 1.7727640452428404 and parameters: {'n_estimators': 249, 'max_depth': 2, 'learning_rate': 0.020979549542327804, 'subsample': 0.8738214287354175, 'colsample_bytree': 0.9023571111374418}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:34,544] Trial 14 finished with value: 1.8842706539562097 and parameters: {'n_estimators': 158, 'max_depth': 2, 'learning_rate': 0.020586346796016716, 'subsample': 0.90895576588611, 'colsample_bytree': 0.936981967170321}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:35,923] Trial 15 finished with value: 2.088713851171506 and parameters: {'n_estimators': 246, 'max_depth': 8, 'learning_rate': 0.29596284206078843, 'subsample': 0.6274745095999714, 'colsample_bytree': 0.8491537250912986}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:37,446] Trial 16 finished with value: 1.8066598845142465 and parameters: {'n_estimators': 300, 'max_depth': 2, 'learning_rate': 0.04210286679326595, 'subsample': 0.9109452197604795, 'colsample_bytree': 0.8815209748733998}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:39,442] Trial 17 finished with value: 1.930866797379009 and parameters: {'n_estimators': 183, 'max_depth': 3, 'learning_rate': 0.01857989642499113, 'subsample': 0.731057379200564, 'colsample_bytree': 0.8189034395428846}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:42,720] Trial 18 finished with value: 2.2403403007940748 and parameters: {'n_estimators': 229, 'max_depth': 5, 'learning_rate': 0.04427320693068286, 'subsample': 0.9929621534476831, 'colsample_bytree': 0.910236126593346}. Best is trial 5 with value: 1.7521197144197387.\n",
            "[I 2025-10-07 08:43:43,691] Trial 19 finished with value: 1.7453912587973903 and parameters: {'n_estimators': 269, 'max_depth': 2, 'learning_rate': 0.019875635863662207, 'subsample': 0.8312585492417999, 'colsample_bytree': 0.675760829066606}. Best is trial 19 with value: 1.7453912587973903.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 1: {'n_estimators': 269, 'max_depth': 2, 'learning_rate': 0.019875635863662207, 'subsample': 0.8312585492417999, 'colsample_bytree': 0.675760829066606}\n",
            "\n",
            "--- LOOCV Fold 2/31 ---\n",
            "Fold 2 Epoch 1 Loss: 0.9383\n",
            "Fold 2 Epoch 2 Loss: 1.0551\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:43:52,216] A new study created in memory with name: no-name-4f7b3a5d-5a02-4579-a6ee-5173aebda496\n",
            "[I 2025-10-07 08:43:54,448] Trial 0 finished with value: 1.852023453372325 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.852023453372325.\n",
            "[I 2025-10-07 08:43:54,904] Trial 1 finished with value: 1.8591329993989008 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.852023453372325.\n",
            "[I 2025-10-07 08:43:55,414] Trial 2 finished with value: 1.8409629844047217 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.8409629844047217.\n",
            "[I 2025-10-07 08:43:56,165] Trial 3 finished with value: 1.9052100793409323 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.8409629844047217.\n",
            "[I 2025-10-07 08:43:57,393] Trial 4 finished with value: 1.9464882893914826 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.8409629844047217.\n",
            "[I 2025-10-07 08:43:58,582] Trial 5 finished with value: 1.8579616559578134 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.8409629844047217.\n",
            "[I 2025-10-07 08:44:00,198] Trial 6 finished with value: 2.021366784253373 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 1.8409629844047217.\n",
            "[I 2025-10-07 08:44:02,201] Trial 7 finished with value: 1.9970864168463138 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 1.8409629844047217.\n",
            "[I 2025-10-07 08:44:02,921] Trial 8 finished with value: 2.250541068998861 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 1.8409629844047217.\n",
            "[I 2025-10-07 08:44:04,469] Trial 9 finished with value: 1.89287304741173 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 1.8409629844047217.\n",
            "[I 2025-10-07 08:44:06,538] Trial 10 finished with value: 1.9671318348636142 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 2 with value: 1.8409629844047217.\n",
            "[I 2025-10-07 08:44:08,306] Trial 11 finished with value: 1.6246017043719916 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.12040035550822742, 'subsample': 0.5899164086425722, 'colsample_bytree': 0.5037653191432865}. Best is trial 11 with value: 1.6246017043719916.\n",
            "[I 2025-10-07 08:44:08,774] Trial 12 finished with value: 1.7413775868751908 and parameters: {'n_estimators': 58, 'max_depth': 8, 'learning_rate': 0.12594007031355467, 'subsample': 0.5750406363546617, 'colsample_bytree': 0.5083149889286448}. Best is trial 11 with value: 1.6246017043719916.\n",
            "[I 2025-10-07 08:44:09,695] Trial 13 finished with value: 1.7344203662625783 and parameters: {'n_estimators': 134, 'max_depth': 8, 'learning_rate': 0.09839455663059753, 'subsample': 0.5042907647243656, 'colsample_bytree': 0.5032838118197356}. Best is trial 11 with value: 1.6246017043719916.\n",
            "[I 2025-10-07 08:44:10,719] Trial 14 finished with value: 1.6910927388634112 and parameters: {'n_estimators': 140, 'max_depth': 8, 'learning_rate': 0.10612154510784122, 'subsample': 0.5243409833033418, 'colsample_bytree': 0.5073169034307036}. Best is trial 11 with value: 1.6246017043719916.\n",
            "[I 2025-10-07 08:44:13,463] Trial 15 finished with value: 1.782331187533766 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.03208899494279826, 'subsample': 0.633913167764783, 'colsample_bytree': 0.6545175077211666}. Best is trial 11 with value: 1.6246017043719916.\n",
            "[I 2025-10-07 08:44:15,350] Trial 16 finished with value: 1.6363610090968863 and parameters: {'n_estimators': 171, 'max_depth': 7, 'learning_rate': 0.08615959519224084, 'subsample': 0.5614174774342454, 'colsample_bytree': 0.8441147559750997}. Best is trial 11 with value: 1.6246017043719916.\n",
            "[I 2025-10-07 08:44:17,400] Trial 17 finished with value: 1.8656627722150412 and parameters: {'n_estimators': 179, 'max_depth': 6, 'learning_rate': 0.034786566147576886, 'subsample': 0.661597253898524, 'colsample_bytree': 0.8690491710631163}. Best is trial 11 with value: 1.6246017043719916.\n",
            "[I 2025-10-07 08:44:19,158] Trial 18 finished with value: 1.8370681143046839 and parameters: {'n_estimators': 110, 'max_depth': 7, 'learning_rate': 0.07455332754484298, 'subsample': 0.582778140692689, 'colsample_bytree': 0.9240659304942347}. Best is trial 11 with value: 1.6246017043719916.\n",
            "[I 2025-10-07 08:44:21,239] Trial 19 finished with value: 1.9437629816580557 and parameters: {'n_estimators': 187, 'max_depth': 9, 'learning_rate': 0.2702272241241035, 'subsample': 0.56391216214132, 'colsample_bytree': 0.7936376947797952}. Best is trial 11 with value: 1.6246017043719916.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 2: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.12040035550822742, 'subsample': 0.5899164086425722, 'colsample_bytree': 0.5037653191432865}\n",
            "\n",
            "--- LOOCV Fold 3/31 ---\n",
            "Fold 3 Epoch 1 Loss: 0.9719\n",
            "Fold 3 Epoch 2 Loss: 0.9088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:44:29,226] A new study created in memory with name: no-name-dc523d55-2c1c-457a-8bea-1ef8fe259966\n",
            "[I 2025-10-07 08:44:30,569] Trial 0 finished with value: 2.2307383277695263 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.2307383277695263.\n",
            "[I 2025-10-07 08:44:30,989] Trial 1 finished with value: 2.0356282567418327 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.0356282567418327.\n",
            "[I 2025-10-07 08:44:31,518] Trial 2 finished with value: 2.079496644972549 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 2.0356282567418327.\n",
            "[I 2025-10-07 08:44:32,617] Trial 3 finished with value: 1.9430724131531232 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 1.9430724131531232.\n",
            "[I 2025-10-07 08:44:34,658] Trial 4 finished with value: 1.9259150201478152 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 4 with value: 1.9259150201478152.\n",
            "[I 2025-10-07 08:44:36,035] Trial 5 finished with value: 2.03413417839824 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 4 with value: 1.9259150201478152.\n",
            "[I 2025-10-07 08:44:37,750] Trial 6 finished with value: 1.89919606440228 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 6 with value: 1.89919606440228.\n",
            "[I 2025-10-07 08:44:39,783] Trial 7 finished with value: 1.9479387307393963 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 6 with value: 1.89919606440228.\n",
            "[I 2025-10-07 08:44:40,502] Trial 8 finished with value: 2.167296003473499 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 6 with value: 1.89919606440228.\n",
            "[I 2025-10-07 08:44:41,990] Trial 9 finished with value: 2.0557216004136887 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 6 with value: 1.89919606440228.\n",
            "[I 2025-10-07 08:44:48,263] Trial 10 finished with value: 1.9715016207438651 and parameters: {'n_estimators': 289, 'max_depth': 7, 'learning_rate': 0.02367326211211725, 'subsample': 0.9775724145208066, 'colsample_bytree': 0.991948117710163}. Best is trial 6 with value: 1.89919606440228.\n",
            "[I 2025-10-07 08:44:50,304] Trial 11 finished with value: 2.126408197242648 and parameters: {'n_estimators': 178, 'max_depth': 6, 'learning_rate': 0.026229329252283374, 'subsample': 0.5390981377680806, 'colsample_bytree': 0.851264901065792}. Best is trial 6 with value: 1.89919606440228.\n",
            "[I 2025-10-07 08:44:51,187] Trial 12 finished with value: 1.9690432978695642 and parameters: {'n_estimators': 180, 'max_depth': 2, 'learning_rate': 0.02406376977682104, 'subsample': 0.6461192327509931, 'colsample_bytree': 0.9789949448892642}. Best is trial 6 with value: 1.89919606440228.\n",
            "[I 2025-10-07 08:44:52,707] Trial 13 finished with value: 1.924175084119705 and parameters: {'n_estimators': 137, 'max_depth': 5, 'learning_rate': 0.017005609599943586, 'subsample': 0.8999942211813328, 'colsample_bytree': 0.8069333160154517}. Best is trial 6 with value: 1.89919606440228.\n",
            "[I 2025-10-07 08:44:54,285] Trial 14 finished with value: 1.954425015352541 and parameters: {'n_estimators': 134, 'max_depth': 8, 'learning_rate': 0.014106162392752532, 'subsample': 0.8947385331597324, 'colsample_bytree': 0.8859620061900356}. Best is trial 6 with value: 1.89919606440228.\n",
            "[I 2025-10-07 08:44:55,996] Trial 15 finished with value: 1.888123612804889 and parameters: {'n_estimators': 145, 'max_depth': 5, 'learning_rate': 0.016859918769888498, 'subsample': 0.9038232831405426, 'colsample_bytree': 0.9283932522971651}. Best is trial 15 with value: 1.888123612804889.\n",
            "[I 2025-10-07 08:44:58,210] Trial 16 finished with value: 1.9335118709963393 and parameters: {'n_estimators': 152, 'max_depth': 8, 'learning_rate': 0.04057781947590916, 'subsample': 0.9989116618510044, 'colsample_bytree': 0.930483684284034}. Best is trial 15 with value: 1.888123612804889.\n",
            "[I 2025-10-07 08:45:01,980] Trial 17 finished with value: 1.9157037970982767 and parameters: {'n_estimators': 216, 'max_depth': 5, 'learning_rate': 0.010459674875657096, 'subsample': 0.9040330172235821, 'colsample_bytree': 0.927470225059473}. Best is trial 15 with value: 1.888123612804889.\n",
            "[I 2025-10-07 08:45:03,767] Trial 18 finished with value: 1.8245412681878914 and parameters: {'n_estimators': 245, 'max_depth': 3, 'learning_rate': 0.038895132856245775, 'subsample': 0.9360316249969132, 'colsample_bytree': 0.7840147151772217}. Best is trial 18 with value: 1.8245412681878914.\n",
            "[I 2025-10-07 08:45:07,593] Trial 19 finished with value: 1.9499791304822973 and parameters: {'n_estimators': 290, 'max_depth': 7, 'learning_rate': 0.03945034575215899, 'subsample': 0.8505091346961402, 'colsample_bytree': 0.8033506743511043}. Best is trial 18 with value: 1.8245412681878914.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 3: {'n_estimators': 245, 'max_depth': 3, 'learning_rate': 0.038895132856245775, 'subsample': 0.9360316249969132, 'colsample_bytree': 0.7840147151772217}\n",
            "\n",
            "--- LOOCV Fold 4/31 ---\n",
            "Fold 4 Epoch 1 Loss: 0.9984\n",
            "Fold 4 Epoch 2 Loss: 1.0317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:45:17,127] A new study created in memory with name: no-name-2965bb71-8b16-4365-ad70-df4f73bcbd02\n",
            "[I 2025-10-07 08:45:18,501] Trial 0 finished with value: 1.9955131410756448 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.9955131410756448.\n",
            "[I 2025-10-07 08:45:18,929] Trial 1 finished with value: 2.061314364894417 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.9955131410756448.\n",
            "[I 2025-10-07 08:45:19,430] Trial 2 finished with value: 2.405822109001196 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 1.9955131410756448.\n",
            "[I 2025-10-07 08:45:20,567] Trial 3 finished with value: 1.8486373786201746 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 1.8486373786201746.\n",
            "[I 2025-10-07 08:45:21,797] Trial 4 finished with value: 1.912769933906017 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 1.8486373786201746.\n",
            "[I 2025-10-07 08:45:22,951] Trial 5 finished with value: 1.880461349733741 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 3 with value: 1.8486373786201746.\n",
            "[I 2025-10-07 08:45:24,613] Trial 6 finished with value: 2.2191245038124956 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 3 with value: 1.8486373786201746.\n",
            "[I 2025-10-07 08:45:26,765] Trial 7 finished with value: 1.957889816338438 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 3 with value: 1.8486373786201746.\n",
            "[I 2025-10-07 08:45:27,982] Trial 8 finished with value: 2.434408984572474 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 3 with value: 1.8486373786201746.\n",
            "[I 2025-10-07 08:45:30,341] Trial 9 finished with value: 1.9088129524095698 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 3 with value: 1.8486373786201746.\n",
            "[I 2025-10-07 08:45:31,631] Trial 10 finished with value: 2.0641190571965558 and parameters: {'n_estimators': 141, 'max_depth': 7, 'learning_rate': 0.030315725522749647, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8285618345363206}. Best is trial 3 with value: 1.8486373786201746.\n",
            "[I 2025-10-07 08:45:33,888] Trial 11 finished with value: 1.8281010976775742 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.06463513151371238, 'subsample': 0.6758580632546128, 'colsample_bytree': 0.5020730763700768}. Best is trial 11 with value: 1.8281010976775742.\n",
            "[I 2025-10-07 08:45:36,019] Trial 12 finished with value: 1.8808488915387045 and parameters: {'n_estimators': 299, 'max_depth': 7, 'learning_rate': 0.08787850607501545, 'subsample': 0.6871730874891191, 'colsample_bytree': 0.509429036272865}. Best is trial 11 with value: 1.8281010976775742.\n",
            "[I 2025-10-07 08:45:36,852] Trial 13 finished with value: 2.7166474007791903 and parameters: {'n_estimators': 134, 'max_depth': 5, 'learning_rate': 0.2907706115118302, 'subsample': 0.6488796231869034, 'colsample_bytree': 0.6478206630857714}. Best is trial 11 with value: 1.8281010976775742.\n",
            "[I 2025-10-07 08:45:40,858] Trial 14 finished with value: 1.856528142633805 and parameters: {'n_estimators': 282, 'max_depth': 8, 'learning_rate': 0.0324885009096488, 'subsample': 0.5653256167561652, 'colsample_bytree': 0.6738167152482913}. Best is trial 11 with value: 1.8281010976775742.\n",
            "[I 2025-10-07 08:45:44,174] Trial 15 finished with value: 1.9189802300157575 and parameters: {'n_estimators': 169, 'max_depth': 5, 'learning_rate': 0.08962717626942573, 'subsample': 0.7184573645895903, 'colsample_bytree': 0.8304278626461433}. Best is trial 11 with value: 1.8281010976775742.\n",
            "[I 2025-10-07 08:45:45,059] Trial 16 finished with value: 1.9413875496571005 and parameters: {'n_estimators': 108, 'max_depth': 8, 'learning_rate': 0.043419433739477924, 'subsample': 0.6142690860932788, 'colsample_bytree': 0.5383053937181538}. Best is trial 11 with value: 1.8281010976775742.\n",
            "[I 2025-10-07 08:45:47,707] Trial 17 finished with value: 1.8905835254028684 and parameters: {'n_estimators': 247, 'max_depth': 6, 'learning_rate': 0.019673534721787525, 'subsample': 0.8680540253657558, 'colsample_bytree': 0.7628320129992068}. Best is trial 11 with value: 1.8281010976775742.\n",
            "[I 2025-10-07 08:45:49,262] Trial 18 finished with value: 1.938273150621874 and parameters: {'n_estimators': 167, 'max_depth': 5, 'learning_rate': 0.08624422233695168, 'subsample': 0.7276826999536898, 'colsample_bytree': 0.6930132334807433}. Best is trial 11 with value: 1.8281010976775742.\n",
            "[I 2025-10-07 08:45:49,769] Trial 19 finished with value: 1.8174266564314636 and parameters: {'n_estimators': 116, 'max_depth': 2, 'learning_rate': 0.04436070823930985, 'subsample': 0.8808313666551089, 'colsample_bytree': 0.7793544167946737}. Best is trial 19 with value: 1.8174266564314636.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 4: {'n_estimators': 116, 'max_depth': 2, 'learning_rate': 0.04436070823930985, 'subsample': 0.8808313666551089, 'colsample_bytree': 0.7793544167946737}\n",
            "\n",
            "--- LOOCV Fold 5/31 ---\n",
            "Fold 5 Epoch 1 Loss: 0.9862\n",
            "Fold 5 Epoch 2 Loss: 0.9057\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:45:58,973] A new study created in memory with name: no-name-307b210e-70e8-481e-86a6-19dd01fce478\n",
            "[I 2025-10-07 08:46:00,336] Trial 0 finished with value: 1.8576038646358668 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.8576038646358668.\n",
            "[I 2025-10-07 08:46:00,749] Trial 1 finished with value: 2.435985380970339 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.8576038646358668.\n",
            "[I 2025-10-07 08:46:01,260] Trial 2 finished with value: 2.3276740036852543 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 1.8576038646358668.\n",
            "[I 2025-10-07 08:46:01,990] Trial 3 finished with value: 1.9143167004787731 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 1.8576038646358668.\n",
            "[I 2025-10-07 08:46:03,178] Trial 4 finished with value: 2.0107465272829526 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 1.8576038646358668.\n",
            "[I 2025-10-07 08:46:04,324] Trial 5 finished with value: 1.8249113308027318 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:05,871] Trial 6 finished with value: 2.2848718518232936 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:07,903] Trial 7 finished with value: 1.9049814096206705 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:08,634] Trial 8 finished with value: 2.3571167441334966 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:11,055] Trial 9 finished with value: 1.8581688654176154 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:13,531] Trial 10 finished with value: 2.0334730141636865 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.02964033652082882, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5072567334400258}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:14,772] Trial 11 finished with value: 1.8916330410755928 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.11285320502212905, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.5020730763700768}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:16,098] Trial 12 finished with value: 1.9208110878607605 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.10081853902880024, 'subsample': 0.8806736357247846, 'colsample_bytree': 0.5380251134894303}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:17,242] Trial 13 finished with value: 2.5889717376920314 and parameters: {'n_estimators': 159, 'max_depth': 6, 'learning_rate': 0.2882069963494139, 'subsample': 0.6376629900391765, 'colsample_bytree': 0.8229282203516206}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:19,803] Trial 14 finished with value: 2.013348633331044 and parameters: {'n_estimators': 259, 'max_depth': 8, 'learning_rate': 0.07937357644840533, 'subsample': 0.7480206560848571, 'colsample_bytree': 0.6738167152482913}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:21,610] Trial 15 finished with value: 1.871663353447845 and parameters: {'n_estimators': 181, 'max_depth': 9, 'learning_rate': 0.03376610234655322, 'subsample': 0.9140258921869188, 'colsample_bytree': 0.5778918965965928}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:25,855] Trial 16 finished with value: 1.934993351500958 and parameters: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.04562044888767241, 'subsample': 0.8104537952868079, 'colsample_bytree': 0.8441147559750997}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:26,389] Trial 17 finished with value: 2.2553241111912166 and parameters: {'n_estimators': 127, 'max_depth': 2, 'learning_rate': 0.1357039061098737, 'subsample': 0.6738168463735492, 'colsample_bytree': 0.7701341133322941}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:28,708] Trial 18 finished with value: 2.101379075782295 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.08129845818944759, 'subsample': 0.5827781406926889, 'colsample_bytree': 0.5525315087165706}. Best is trial 5 with value: 1.8249113308027318.\n",
            "[I 2025-10-07 08:46:29,752] Trial 19 finished with value: 2.326764703205328 and parameters: {'n_estimators': 227, 'max_depth': 9, 'learning_rate': 0.2702272241241035, 'subsample': 0.7454123929930568, 'colsample_bytree': 0.675760829066606}. Best is trial 5 with value: 1.8249113308027318.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 5: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}\n",
            "\n",
            "--- LOOCV Fold 6/31 ---\n",
            "Fold 6 Epoch 1 Loss: 0.9961\n",
            "Fold 6 Epoch 2 Loss: 0.9467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:46:39,116] A new study created in memory with name: no-name-192958d0-0f82-4f6f-86a3-325881f897a1\n",
            "[I 2025-10-07 08:46:40,643] Trial 0 finished with value: 2.3476451986964677 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.3476451986964677.\n",
            "[I 2025-10-07 08:46:41,073] Trial 1 finished with value: 2.6122167352628822 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.3476451986964677.\n",
            "[I 2025-10-07 08:46:41,627] Trial 2 finished with value: 2.2122132180223337 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.2122132180223337.\n",
            "[I 2025-10-07 08:46:42,370] Trial 3 finished with value: 2.491721291982627 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 2.2122132180223337.\n",
            "[I 2025-10-07 08:46:43,554] Trial 4 finished with value: 2.555914799166662 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 2.2122132180223337.\n",
            "[I 2025-10-07 08:46:44,685] Trial 5 finished with value: 2.1060676222565613 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 2.1060676222565613.\n",
            "[I 2025-10-07 08:46:46,246] Trial 6 finished with value: 3.13501546126024 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 2.1060676222565613.\n",
            "[I 2025-10-07 08:46:48,291] Trial 7 finished with value: 2.6507261577013796 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 5 with value: 2.1060676222565613.\n",
            "[I 2025-10-07 08:46:48,959] Trial 8 finished with value: 2.686656779870143 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 5 with value: 2.1060676222565613.\n",
            "[I 2025-10-07 08:46:51,041] Trial 9 finished with value: 2.382053367236064 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 5 with value: 2.1060676222565613.\n",
            "[I 2025-10-07 08:46:53,849] Trial 10 finished with value: 2.272605971658666 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.02964033652082882, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5072567334400258}. Best is trial 5 with value: 2.1060676222565613.\n",
            "[I 2025-10-07 08:46:54,660] Trial 11 finished with value: 2.3432804010011155 and parameters: {'n_estimators': 136, 'max_depth': 10, 'learning_rate': 0.2762122998112033, 'subsample': 0.5971603596691708, 'colsample_bytree': 0.5020730763700768}. Best is trial 5 with value: 2.1060676222565613.\n",
            "[I 2025-10-07 08:46:56,615] Trial 12 finished with value: 2.2474915688920216 and parameters: {'n_estimators': 298, 'max_depth': 8, 'learning_rate': 0.10916525212605414, 'subsample': 0.6290514757037396, 'colsample_bytree': 0.5390575097504566}. Best is trial 5 with value: 2.1060676222565613.\n",
            "[I 2025-10-07 08:46:59,071] Trial 13 finished with value: 2.065662726736071 and parameters: {'n_estimators': 247, 'max_depth': 6, 'learning_rate': 0.10639623082874453, 'subsample': 0.5046640331111848, 'colsample_bytree': 0.8229282203516206}. Best is trial 13 with value: 2.065662726736071.\n",
            "[I 2025-10-07 08:47:01,671] Trial 14 finished with value: 2.8682101677682774 and parameters: {'n_estimators': 253, 'max_depth': 6, 'learning_rate': 0.08071591825782531, 'subsample': 0.8943260328548344, 'colsample_bytree': 0.8350504632309295}. Best is trial 13 with value: 2.065662726736071.\n",
            "[I 2025-10-07 08:47:05,127] Trial 15 finished with value: 2.283455822349754 and parameters: {'n_estimators': 251, 'max_depth': 8, 'learning_rate': 0.0328853804108092, 'subsample': 0.5392160127366197, 'colsample_bytree': 0.8304278626461432}. Best is trial 13 with value: 2.065662726736071.\n",
            "[I 2025-10-07 08:47:08,485] Trial 16 finished with value: 2.9684264789231154 and parameters: {'n_estimators': 233, 'max_depth': 5, 'learning_rate': 0.04210286679326595, 'subsample': 0.8688325692371892, 'colsample_bytree': 0.9219900508555974}. Best is trial 13 with value: 2.065662726736071.\n",
            "[I 2025-10-07 08:47:09,186] Trial 17 finished with value: 2.365558567321576 and parameters: {'n_estimators': 165, 'max_depth': 2, 'learning_rate': 0.10451518402039733, 'subsample': 0.69519366657285, 'colsample_bytree': 0.7794028774114414}. Best is trial 13 with value: 2.065662726736071.\n",
            "[I 2025-10-07 08:47:11,981] Trial 18 finished with value: 2.4990238245964873 and parameters: {'n_estimators': 275, 'max_depth': 8, 'learning_rate': 0.019130218586287058, 'subsample': 0.7329921304780784, 'colsample_bytree': 0.6850752557404476}. Best is trial 13 with value: 2.065662726736071.\n",
            "[I 2025-10-07 08:47:13,930] Trial 19 finished with value: 2.590663404828954 and parameters: {'n_estimators': 191, 'max_depth': 5, 'learning_rate': 0.07204206521259571, 'subsample': 0.6433592209064516, 'colsample_bytree': 0.7860760029137474}. Best is trial 13 with value: 2.065662726736071.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 6: {'n_estimators': 247, 'max_depth': 6, 'learning_rate': 0.10639623082874453, 'subsample': 0.5046640331111848, 'colsample_bytree': 0.8229282203516206}\n",
            "\n",
            "--- LOOCV Fold 7/31 ---\n",
            "Fold 7 Epoch 1 Loss: 0.9238\n",
            "Fold 7 Epoch 2 Loss: 0.9876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:47:23,836] A new study created in memory with name: no-name-1d96c33c-278c-4f44-acef-55ca0d889466\n",
            "[I 2025-10-07 08:47:25,209] Trial 0 finished with value: 2.1739337433820163 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.1739337433820163.\n",
            "[I 2025-10-07 08:47:25,612] Trial 1 finished with value: 2.128754400904493 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.128754400904493.\n",
            "[I 2025-10-07 08:47:26,131] Trial 2 finished with value: 2.100209055298711 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:26,823] Trial 3 finished with value: 2.3251403349041504 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:28,021] Trial 4 finished with value: 2.2973060938116183 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:29,139] Trial 5 finished with value: 2.2014707503278426 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:30,752] Trial 6 finished with value: 2.7093000783256764 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:34,055] Trial 7 finished with value: 2.4136619229620626 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:34,834] Trial 8 finished with value: 2.5920907071776833 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:36,298] Trial 9 finished with value: 2.3321880015706653 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:37,612] Trial 10 finished with value: 2.7639187798196887 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:38,337] Trial 11 finished with value: 2.7253391595703347 and parameters: {'n_estimators': 59, 'max_depth': 8, 'learning_rate': 0.28088381941259366, 'subsample': 0.5899164086425722, 'colsample_bytree': 0.8692027723625105}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:39,743] Trial 12 finished with value: 2.5901650347072396 and parameters: {'n_estimators': 112, 'max_depth': 7, 'learning_rate': 0.13436119963045842, 'subsample': 0.6427754537375224, 'colsample_bytree': 0.8616315620551012}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:40,530] Trial 13 finished with value: 2.816734281604654 and parameters: {'n_estimators': 52, 'max_depth': 6, 'learning_rate': 0.1361058138823361, 'subsample': 0.8944520699149165, 'colsample_bytree': 0.9608295945492284}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:41,854] Trial 14 finished with value: 2.246066416514288 and parameters: {'n_estimators': 131, 'max_depth': 8, 'learning_rate': 0.1737183744716791, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.7904137304680051}. Best is trial 2 with value: 2.100209055298711.\n",
            "[I 2025-10-07 08:47:42,933] Trial 15 finished with value: 1.9858859650959715 and parameters: {'n_estimators': 292, 'max_depth': 2, 'learning_rate': 0.08962717626942573, 'subsample': 0.6274745095999714, 'colsample_bytree': 0.6924366913699725}. Best is trial 15 with value: 1.9858859650959715.\n",
            "[I 2025-10-07 08:47:46,476] Trial 16 finished with value: 2.3415457279615586 and parameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.08719209000999652, 'subsample': 0.6207774664901698, 'colsample_bytree': 0.6726477117092511}. Best is trial 15 with value: 1.9858859650959715.\n",
            "[I 2025-10-07 08:47:49,095] Trial 17 finished with value: 2.124959258842402 and parameters: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.028069002597357614, 'subsample': 0.5694064187762937, 'colsample_bytree': 0.5048446190119473}. Best is trial 15 with value: 1.9858859650959715.\n",
            "[I 2025-10-07 08:47:50,709] Trial 18 finished with value: 2.3301390459176248 and parameters: {'n_estimators': 178, 'max_depth': 5, 'learning_rate': 0.09031256134732037, 'subsample': 0.6604822481437748, 'colsample_bytree': 0.6863967764833431}. Best is trial 15 with value: 1.9858859650959715.\n",
            "[I 2025-10-07 08:47:51,558] Trial 19 finished with value: 2.125481201012378 and parameters: {'n_estimators': 265, 'max_depth': 2, 'learning_rate': 0.041799869026699674, 'subsample': 0.7112800280091848, 'colsample_bytree': 0.5749614128499309}. Best is trial 15 with value: 1.9858859650959715.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 7: {'n_estimators': 292, 'max_depth': 2, 'learning_rate': 0.08962717626942573, 'subsample': 0.6274745095999714, 'colsample_bytree': 0.6924366913699725}\n",
            "\n",
            "--- LOOCV Fold 8/31 ---\n",
            "Fold 8 Epoch 1 Loss: 0.9556\n",
            "Fold 8 Epoch 2 Loss: 0.9550\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:48:00,646] A new study created in memory with name: no-name-c75c7804-74a5-4d88-bddd-bf69833b4c2b\n",
            "[I 2025-10-07 08:48:02,339] Trial 0 finished with value: 2.260432731053298 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.260432731053298.\n",
            "[I 2025-10-07 08:48:02,759] Trial 1 finished with value: 2.1505838102705837 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.1505838102705837.\n",
            "[I 2025-10-07 08:48:03,272] Trial 2 finished with value: 2.0728760765330883 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:04,018] Trial 3 finished with value: 2.1860676424233807 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:05,244] Trial 4 finished with value: 2.1608632051484515 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:06,428] Trial 5 finished with value: 2.1670777962807475 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:08,077] Trial 6 finished with value: 3.2118928704389695 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:10,151] Trial 7 finished with value: 2.4229413535307396 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:10,871] Trial 8 finished with value: 2.7028619047380986 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:13,029] Trial 9 finished with value: 2.3251748628989946 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:15,090] Trial 10 finished with value: 2.2644830448033932 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:15,832] Trial 11 finished with value: 2.477462105762205 and parameters: {'n_estimators': 59, 'max_depth': 8, 'learning_rate': 0.28088381941259366, 'subsample': 0.5899164086425722, 'colsample_bytree': 0.8692027723625105}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:17,289] Trial 12 finished with value: 2.4249922209844197 and parameters: {'n_estimators': 112, 'max_depth': 7, 'learning_rate': 0.13436119963045842, 'subsample': 0.6427754537375224, 'colsample_bytree': 0.8616315620551012}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:18,102] Trial 13 finished with value: 2.569800876355795 and parameters: {'n_estimators': 52, 'max_depth': 6, 'learning_rate': 0.1361058138823361, 'subsample': 0.8944520699149165, 'colsample_bytree': 0.9608295945492284}. Best is trial 2 with value: 2.0728760765330883.\n",
            "[I 2025-10-07 08:48:19,467] Trial 14 finished with value: 2.0693771129288727 and parameters: {'n_estimators': 131, 'max_depth': 8, 'learning_rate': 0.1737183744716791, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.7904137304680051}. Best is trial 14 with value: 2.0693771129288727.\n",
            "[I 2025-10-07 08:48:22,229] Trial 15 finished with value: 2.350493041320526 and parameters: {'n_estimators': 292, 'max_depth': 9, 'learning_rate': 0.09214174051661962, 'subsample': 0.5268366001044067, 'colsample_bytree': 0.7813571591490867}. Best is trial 14 with value: 2.0693771129288727.\n",
            "[I 2025-10-07 08:48:23,373] Trial 16 finished with value: 2.0754247992320334 and parameters: {'n_estimators': 128, 'max_depth': 8, 'learning_rate': 0.0324525654578974, 'subsample': 0.591638251605233, 'colsample_bytree': 0.6881867212381375}. Best is trial 14 with value: 2.0693771129288727.\n",
            "[I 2025-10-07 08:48:24,503] Trial 17 finished with value: 2.3398113937420413 and parameters: {'n_estimators': 167, 'max_depth': 9, 'learning_rate': 0.19151759562987125, 'subsample': 0.5878828299109579, 'colsample_bytree': 0.5191966301454911}. Best is trial 14 with value: 2.0693771129288727.\n",
            "[I 2025-10-07 08:48:27,500] Trial 18 finished with value: 2.566239592720213 and parameters: {'n_estimators': 168, 'max_depth': 7, 'learning_rate': 0.10046886738520254, 'subsample': 0.6471208002214496, 'colsample_bytree': 0.7895429472174105}. Best is trial 14 with value: 2.0693771129288727.\n",
            "[I 2025-10-07 08:48:29,059] Trial 19 finished with value: 2.345382457230728 and parameters: {'n_estimators': 110, 'max_depth': 9, 'learning_rate': 0.08195530540959603, 'subsample': 0.5054945946218856, 'colsample_bytree': 0.9197253308931902}. Best is trial 14 with value: 2.0693771129288727.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 8: {'n_estimators': 131, 'max_depth': 8, 'learning_rate': 0.1737183744716791, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.7904137304680051}\n",
            "\n",
            "--- LOOCV Fold 9/31 ---\n",
            "Fold 9 Epoch 1 Loss: 0.9517\n",
            "Fold 9 Epoch 2 Loss: 0.9608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:48:37,175] A new study created in memory with name: no-name-524d2073-b86f-487f-a90d-ff121f1595ff\n",
            "[I 2025-10-07 08:48:38,687] Trial 0 finished with value: 2.3290760097260117 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.3290760097260117.\n",
            "[I 2025-10-07 08:48:39,372] Trial 1 finished with value: 2.741057464926479 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.3290760097260117.\n",
            "[I 2025-10-07 08:48:40,203] Trial 2 finished with value: 2.179021182056521 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.179021182056521.\n",
            "[I 2025-10-07 08:48:41,393] Trial 3 finished with value: 2.2955953659487163 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 2.179021182056521.\n",
            "[I 2025-10-07 08:48:42,795] Trial 4 finished with value: 2.329530856902307 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 2.179021182056521.\n",
            "[I 2025-10-07 08:48:43,950] Trial 5 finished with value: 2.185366043841433 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 2.179021182056521.\n",
            "[I 2025-10-07 08:48:45,610] Trial 6 finished with value: 2.4537013266972805 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 2.179021182056521.\n",
            "[I 2025-10-07 08:48:47,684] Trial 7 finished with value: 2.35018773611591 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 2.179021182056521.\n",
            "[I 2025-10-07 08:48:48,419] Trial 8 finished with value: 2.483570449400511 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 2.179021182056521.\n",
            "[I 2025-10-07 08:48:49,917] Trial 9 finished with value: 2.269445930623598 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 2.179021182056521.\n",
            "[I 2025-10-07 08:48:51,245] Trial 10 finished with value: 2.259226204326482 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 2 with value: 2.179021182056521.\n",
            "[I 2025-10-07 08:48:53,716] Trial 11 finished with value: 2.033414805649283 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.1234269340201201, 'subsample': 0.5971603596691708, 'colsample_bytree': 0.5020730763700768}. Best is trial 11 with value: 2.033414805649283.\n",
            "[I 2025-10-07 08:48:56,082] Trial 12 finished with value: 2.02494294849238 and parameters: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.12594007031355467, 'subsample': 0.5783170319382445, 'colsample_bytree': 0.5082283557871199}. Best is trial 12 with value: 2.02494294849238.\n",
            "[I 2025-10-07 08:48:58,007] Trial 13 finished with value: 2.3041034590969507 and parameters: {'n_estimators': 293, 'max_depth': 8, 'learning_rate': 0.10015777390717795, 'subsample': 0.5044302803151955, 'colsample_bytree': 0.5032838460370773}. Best is trial 12 with value: 2.02494294849238.\n",
            "[I 2025-10-07 08:48:59,906] Trial 14 finished with value: 2.0162804967277155 and parameters: {'n_estimators': 293, 'max_depth': 7, 'learning_rate': 0.10797501826100428, 'subsample': 0.6159453697482817, 'colsample_bytree': 0.5136574877150664}. Best is trial 14 with value: 2.0162804967277155.\n",
            "[I 2025-10-07 08:49:02,758] Trial 15 finished with value: 2.2578004848855953 and parameters: {'n_estimators': 298, 'max_depth': 8, 'learning_rate': 0.03208899494279826, 'subsample': 0.6219374804317841, 'colsample_bytree': 0.6534942285527042}. Best is trial 14 with value: 2.0162804967277155.\n",
            "[I 2025-10-07 08:49:05,733] Trial 16 finished with value: 2.3016498221979647 and parameters: {'n_estimators': 254, 'max_depth': 8, 'learning_rate': 0.08719209000999652, 'subsample': 0.6585006136647665, 'colsample_bytree': 0.8441147559750997}. Best is trial 14 with value: 2.0162804967277155.\n",
            "[I 2025-10-07 08:49:07,613] Trial 17 finished with value: 2.2333530435799402 and parameters: {'n_estimators': 269, 'max_depth': 7, 'learning_rate': 0.26663748121620445, 'subsample': 0.5555317961518667, 'colsample_bytree': 0.5549755170861372}. Best is trial 14 with value: 2.0162804967277155.\n",
            "[I 2025-10-07 08:49:10,304] Trial 18 finished with value: 2.2617760520344783 and parameters: {'n_estimators': 237, 'max_depth': 7, 'learning_rate': 0.0360193043174065, 'subsample': 0.5664703400107906, 'colsample_bytree': 0.6884625397901339}. Best is trial 14 with value: 2.0162804967277155.\n",
            "[I 2025-10-07 08:49:12,235] Trial 19 finished with value: 2.3412495891060443 and parameters: {'n_estimators': 171, 'max_depth': 9, 'learning_rate': 0.14920643617808257, 'subsample': 0.6470364438622684, 'colsample_bytree': 0.9072950643400716}. Best is trial 14 with value: 2.0162804967277155.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 9: {'n_estimators': 293, 'max_depth': 7, 'learning_rate': 0.10797501826100428, 'subsample': 0.6159453697482817, 'colsample_bytree': 0.5136574877150664}\n",
            "\n",
            "--- LOOCV Fold 10/31 ---\n",
            "Fold 10 Epoch 1 Loss: 0.9609\n",
            "Fold 10 Epoch 2 Loss: 0.9799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:49:21,443] A new study created in memory with name: no-name-bd9640ee-9455-4a1c-8313-362f1cc698be\n",
            "[I 2025-10-07 08:49:23,259] Trial 0 finished with value: 2.2741815298713046 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.2741815298713046.\n",
            "[I 2025-10-07 08:49:23,688] Trial 1 finished with value: 2.369914243146989 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.2741815298713046.\n",
            "[I 2025-10-07 08:49:24,198] Trial 2 finished with value: 2.343477491332954 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.2741815298713046.\n",
            "[I 2025-10-07 08:49:24,932] Trial 3 finished with value: 2.3394442827951627 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 2.2741815298713046.\n",
            "[I 2025-10-07 08:49:26,147] Trial 4 finished with value: 2.3073123676369582 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 2.2741815298713046.\n",
            "[I 2025-10-07 08:49:27,300] Trial 5 finished with value: 2.2967171355149603 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 2.2741815298713046.\n",
            "[I 2025-10-07 08:49:28,986] Trial 6 finished with value: 1.9922428696726826 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:30,999] Trial 7 finished with value: 2.2043918089728987 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:31,730] Trial 8 finished with value: 2.3427030467725984 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:33,628] Trial 9 finished with value: 2.325349114907566 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:39,228] Trial 10 finished with value: 2.048526613357082 and parameters: {'n_estimators': 289, 'max_depth': 7, 'learning_rate': 0.02367326211211725, 'subsample': 0.9775724145208066, 'colsample_bytree': 0.991948117710163}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:43,510] Trial 11 finished with value: 2.0246591154525384 and parameters: {'n_estimators': 278, 'max_depth': 7, 'learning_rate': 0.022731789983718213, 'subsample': 0.9851129432910448, 'colsample_bytree': 0.9988392431163264}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:49,664] Trial 12 finished with value: 2.4813913117063535 and parameters: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.02376337959095378, 'subsample': 0.9095948184003396, 'colsample_bytree': 0.9970029045473664}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:51,672] Trial 13 finished with value: 2.179821875393358 and parameters: {'n_estimators': 166, 'max_depth': 8, 'learning_rate': 0.017005609599943586, 'subsample': 0.9112320253004816, 'colsample_bytree': 0.8775185785252101}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:54,392] Trial 14 finished with value: 2.3314029332435324 and parameters: {'n_estimators': 252, 'max_depth': 6, 'learning_rate': 0.033916647167414056, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.8859620061900356}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:56,055] Trial 15 finished with value: 2.015164680810879 and parameters: {'n_estimators': 134, 'max_depth': 5, 'learning_rate': 0.010859168583945963, 'subsample': 0.9950696620237881, 'colsample_bytree': 0.9410539763821114}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:57,489] Trial 16 finished with value: 2.2671958763137217 and parameters: {'n_estimators': 127, 'max_depth': 5, 'learning_rate': 0.010907863682082639, 'subsample': 0.8980308161681356, 'colsample_bytree': 0.9219900508555974}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:49:58,289] Trial 17 finished with value: 2.0955888134133573 and parameters: {'n_estimators': 186, 'max_depth': 2, 'learning_rate': 0.03913220492996121, 'subsample': 0.8600075789089481, 'colsample_bytree': 0.8119374832852144}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:50:00,184] Trial 18 finished with value: 2.2500969816084426 and parameters: {'n_estimators': 132, 'max_depth': 5, 'learning_rate': 0.08496195477878793, 'subsample': 0.9973362191075782, 'colsample_bytree': 0.9461631142376274}. Best is trial 6 with value: 1.9922428696726826.\n",
            "[I 2025-10-07 08:50:01,998] Trial 19 finished with value: 2.096393546551764 and parameters: {'n_estimators': 163, 'max_depth': 3, 'learning_rate': 0.016503584806327274, 'subsample': 0.9256412292722526, 'colsample_bytree': 0.7785759879392425}. Best is trial 6 with value: 1.9922428696726826.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 10: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}\n",
            "\n",
            "--- LOOCV Fold 11/31 ---\n",
            "Fold 11 Epoch 1 Loss: 0.9750\n",
            "Fold 11 Epoch 2 Loss: 0.9298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:50:10,489] A new study created in memory with name: no-name-dedf8a5c-19b6-4107-84bf-18e394c7077c\n",
            "[I 2025-10-07 08:50:11,817] Trial 0 finished with value: 2.384638211251214 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.384638211251214.\n",
            "[I 2025-10-07 08:50:12,236] Trial 1 finished with value: 2.137822287648555 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:12,754] Trial 2 finished with value: 2.338391281313954 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:13,662] Trial 3 finished with value: 2.3660327502560445 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:15,687] Trial 4 finished with value: 2.271602205539273 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:17,239] Trial 5 finished with value: 2.272810034866999 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:18,885] Trial 6 finished with value: 2.906614392595958 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:20,940] Trial 7 finished with value: 2.4678897964636892 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:21,644] Trial 8 finished with value: 2.5903549555262493 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:23,145] Trial 9 finished with value: 2.3947110039212958 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:24,523] Trial 10 finished with value: 2.528972800330446 and parameters: {'n_estimators': 139, 'max_depth': 7, 'learning_rate': 0.27047297227177763, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9168158328299749}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:25,204] Trial 11 finished with value: 2.1669373523171833 and parameters: {'n_estimators': 159, 'max_depth': 2, 'learning_rate': 0.024781852487147565, 'subsample': 0.6673975881116181, 'colsample_bytree': 0.8265454206124712}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:25,779] Trial 12 finished with value: 2.1915944891259596 and parameters: {'n_estimators': 130, 'max_depth': 2, 'learning_rate': 0.027678201790960394, 'subsample': 0.6398620628262128, 'colsample_bytree': 0.8443070778541023}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:28,711] Trial 13 finished with value: 2.63953430909205 and parameters: {'n_estimators': 168, 'max_depth': 6, 'learning_rate': 0.028947668127039642, 'subsample': 0.8944520699149165, 'colsample_bytree': 0.8229282203516206}. Best is trial 1 with value: 2.137822287648555.\n",
            "[I 2025-10-07 08:50:29,446] Trial 14 finished with value: 2.080005809248427 and parameters: {'n_estimators': 107, 'max_depth': 2, 'learning_rate': 0.11149130830015597, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.8138214726279749}. Best is trial 14 with value: 2.080005809248427.\n",
            "[I 2025-10-07 08:50:30,936] Trial 15 finished with value: 2.170420301239227 and parameters: {'n_estimators': 106, 'max_depth': 8, 'learning_rate': 0.11263608439461861, 'subsample': 0.5392311924833902, 'colsample_bytree': 0.892146309820842}. Best is trial 14 with value: 2.080005809248427.\n",
            "[I 2025-10-07 08:50:32,182] Trial 16 finished with value: 2.7629266119072122 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.21147924834647894, 'subsample': 0.8720524565911548, 'colsample_bytree': 0.7753408523265203}. Best is trial 14 with value: 2.080005809248427.\n",
            "[I 2025-10-07 08:50:32,491] Trial 17 finished with value: 2.2651792676770515 and parameters: {'n_estimators': 57, 'max_depth': 2, 'learning_rate': 0.10617883891128144, 'subsample': 0.574853829354372, 'colsample_bytree': 0.9980084946806067}. Best is trial 14 with value: 2.080005809248427.\n",
            "[I 2025-10-07 08:50:34,119] Trial 18 finished with value: 2.5094210217082935 and parameters: {'n_estimators': 113, 'max_depth': 8, 'learning_rate': 0.09128402794796996, 'subsample': 0.7367194768156402, 'colsample_bytree': 0.910236126593346}. Best is trial 14 with value: 2.080005809248427.\n",
            "[I 2025-10-07 08:50:34,930] Trial 19 finished with value: 2.5422006755609656 and parameters: {'n_estimators': 79, 'max_depth': 5, 'learning_rate': 0.14920643617808257, 'subsample': 0.6059294247241275, 'colsample_bytree': 0.781815367238253}. Best is trial 14 with value: 2.080005809248427.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 11: {'n_estimators': 107, 'max_depth': 2, 'learning_rate': 0.11149130830015597, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.8138214726279749}\n",
            "\n",
            "--- LOOCV Fold 12/31 ---\n",
            "Fold 12 Epoch 1 Loss: 0.9292\n",
            "Fold 12 Epoch 2 Loss: 0.8884\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:50:44,286] A new study created in memory with name: no-name-6029ba61-a55f-432e-8715-5ec4a010c0bd\n",
            "[I 2025-10-07 08:50:45,699] Trial 0 finished with value: 2.2555845855749275 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.2555845855749275.\n",
            "[I 2025-10-07 08:50:46,120] Trial 1 finished with value: 2.2086731940729023 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.2086731940729023.\n",
            "[I 2025-10-07 08:50:46,671] Trial 2 finished with value: 2.4252193905642767 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 2.2086731940729023.\n",
            "[I 2025-10-07 08:50:47,424] Trial 3 finished with value: 2.2448350514790993 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 2.2086731940729023.\n",
            "[I 2025-10-07 08:50:48,711] Trial 4 finished with value: 2.3642654961361513 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 2.2086731940729023.\n",
            "[I 2025-10-07 08:50:49,920] Trial 5 finished with value: 2.2306126823729038 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 2.2086731940729023.\n",
            "[I 2025-10-07 08:50:51,552] Trial 6 finished with value: 2.635342487524481 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 1 with value: 2.2086731940729023.\n",
            "[I 2025-10-07 08:50:53,662] Trial 7 finished with value: 2.311480524565438 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 1 with value: 2.2086731940729023.\n",
            "[I 2025-10-07 08:50:54,594] Trial 8 finished with value: 2.6154861430436145 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 1 with value: 2.2086731940729023.\n",
            "[I 2025-10-07 08:50:57,118] Trial 9 finished with value: 2.253912864496382 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 1 with value: 2.2086731940729023.\n",
            "[I 2025-10-07 08:50:58,692] Trial 10 finished with value: 2.315428610545834 and parameters: {'n_estimators': 139, 'max_depth': 7, 'learning_rate': 0.27047297227177763, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9168158328299749}. Best is trial 1 with value: 2.2086731940729023.\n",
            "[I 2025-10-07 08:50:59,811] Trial 11 finished with value: 2.146693875575486 and parameters: {'n_estimators': 277, 'max_depth': 2, 'learning_rate': 0.09425671587813331, 'subsample': 0.8739084974067326, 'colsample_bytree': 0.8582590089757371}. Best is trial 11 with value: 2.146693875575486.\n",
            "[I 2025-10-07 08:51:00,879] Trial 12 finished with value: 2.119167455002441 and parameters: {'n_estimators': 299, 'max_depth': 2, 'learning_rate': 0.11038055977556087, 'subsample': 0.8683766692535109, 'colsample_bytree': 0.8443070778541023}. Best is trial 12 with value: 2.119167455002441.\n",
            "[I 2025-10-07 08:51:03,076] Trial 13 finished with value: 2.3470258555462955 and parameters: {'n_estimators': 293, 'max_depth': 6, 'learning_rate': 0.10192973190818733, 'subsample': 0.8907294133066953, 'colsample_bytree': 0.8229282203516206}. Best is trial 12 with value: 2.119167455002441.\n",
            "[I 2025-10-07 08:51:04,465] Trial 14 finished with value: 2.120650667477606 and parameters: {'n_estimators': 293, 'max_depth': 2, 'learning_rate': 0.03358501163441803, 'subsample': 0.8966936299338699, 'colsample_bytree': 0.8138214726279749}. Best is trial 12 with value: 2.119167455002441.\n",
            "[I 2025-10-07 08:51:11,194] Trial 15 finished with value: 2.390815918216236 and parameters: {'n_estimators': 298, 'max_depth': 8, 'learning_rate': 0.03208899494279826, 'subsample': 0.9234980616943328, 'colsample_bytree': 0.79007520675116}. Best is trial 12 with value: 2.119167455002441.\n",
            "[I 2025-10-07 08:51:14,400] Trial 16 finished with value: 2.684863212585417 and parameters: {'n_estimators': 254, 'max_depth': 5, 'learning_rate': 0.029279779088610076, 'subsample': 0.9973062122788221, 'colsample_bytree': 0.9219900508555974}. Best is trial 12 with value: 2.119167455002441.\n",
            "[I 2025-10-07 08:51:15,331] Trial 17 finished with value: 2.1027849178055122 and parameters: {'n_estimators': 226, 'max_depth': 2, 'learning_rate': 0.02068086816741835, 'subsample': 0.8577383398785322, 'colsample_bytree': 0.7696278579557545}. Best is trial 17 with value: 2.1027849178055122.\n",
            "[I 2025-10-07 08:51:16,884] Trial 18 finished with value: 2.255225143804637 and parameters: {'n_estimators': 163, 'max_depth': 8, 'learning_rate': 0.04104975015072629, 'subsample': 0.6471208002214496, 'colsample_bytree': 0.6786678239447604}. Best is trial 17 with value: 2.1027849178055122.\n",
            "[I 2025-10-07 08:51:19,204] Trial 19 finished with value: 2.3059181123239085 and parameters: {'n_estimators': 221, 'max_depth': 5, 'learning_rate': 0.01982441095178594, 'subsample': 0.8312585492417999, 'colsample_bytree': 0.7740066195800229}. Best is trial 17 with value: 2.1027849178055122.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 12: {'n_estimators': 226, 'max_depth': 2, 'learning_rate': 0.02068086816741835, 'subsample': 0.8577383398785322, 'colsample_bytree': 0.7696278579557545}\n",
            "\n",
            "--- LOOCV Fold 13/31 ---\n",
            "Fold 13 Epoch 1 Loss: 0.9413\n",
            "Fold 13 Epoch 2 Loss: 0.9276\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:51:28,533] A new study created in memory with name: no-name-3574cd18-d55f-4bee-b371-ba8b65b3b5c4\n",
            "[I 2025-10-07 08:51:29,857] Trial 0 finished with value: 2.1299770279200403 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:30,265] Trial 1 finished with value: 2.1890311953550574 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:30,791] Trial 2 finished with value: 2.3672009178370685 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:31,546] Trial 3 finished with value: 2.4345968963220606 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:32,734] Trial 4 finished with value: 2.2930239450631698 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:33,888] Trial 5 finished with value: 2.1956684323427433 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:35,584] Trial 6 finished with value: 2.585206281419245 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:38,908] Trial 7 finished with value: 2.2529626146465986 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:39,634] Trial 8 finished with value: 2.5290599704992447 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:41,124] Trial 9 finished with value: 2.214092364107799 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:42,200] Trial 10 finished with value: 2.152461899590833 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.10672112706919769, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5076838686640521}. Best is trial 0 with value: 2.1299770279200403.\n",
            "[I 2025-10-07 08:51:43,297] Trial 11 finished with value: 2.1124048405665845 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.11285320502212905, 'subsample': 0.5427925342039944, 'colsample_bytree': 0.5018684889065917}. Best is trial 11 with value: 2.1124048405665845.\n",
            "[I 2025-10-07 08:51:44,360] Trial 12 finished with value: 2.1344952139707525 and parameters: {'n_estimators': 144, 'max_depth': 8, 'learning_rate': 0.10081853902880024, 'subsample': 0.5259324752234724, 'colsample_bytree': 0.5082179758109964}. Best is trial 11 with value: 2.1124048405665845.\n",
            "[I 2025-10-07 08:51:45,270] Trial 13 finished with value: 2.058235443559834 and parameters: {'n_estimators': 154, 'max_depth': 8, 'learning_rate': 0.2966657606405238, 'subsample': 0.8936108460389663, 'colsample_bytree': 0.8229282203516206}. Best is trial 13 with value: 2.058235443559834.\n",
            "[I 2025-10-07 08:51:46,212] Trial 14 finished with value: 2.0557209267724095 and parameters: {'n_estimators': 171, 'max_depth': 8, 'learning_rate': 0.2967798066275567, 'subsample': 0.8947385331597324, 'colsample_bytree': 0.8350504632309295}. Best is trial 14 with value: 2.0557209267724095.\n",
            "[I 2025-10-07 08:51:47,354] Trial 15 finished with value: 2.224202541714039 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.29596284206078843, 'subsample': 0.9234980616943328, 'colsample_bytree': 0.8336228052393174}. Best is trial 14 with value: 2.0557209267724095.\n",
            "[I 2025-10-07 08:51:48,406] Trial 16 finished with value: 2.0533960482996645 and parameters: {'n_estimators': 164, 'max_depth': 8, 'learning_rate': 0.25865584850819207, 'subsample': 0.8975883235113203, 'colsample_bytree': 0.8494692628673504}. Best is trial 16 with value: 2.0533960482996645.\n",
            "[I 2025-10-07 08:51:50,650] Trial 17 finished with value: 2.3750041800907473 and parameters: {'n_estimators': 173, 'max_depth': 7, 'learning_rate': 0.19497960387429344, 'subsample': 0.8652192022364196, 'colsample_bytree': 0.9381034592744818}. Best is trial 16 with value: 2.0533960482996645.\n",
            "[I 2025-10-07 08:51:52,914] Trial 18 finished with value: 2.5216117543416425 and parameters: {'n_estimators': 120, 'max_depth': 6, 'learning_rate': 0.02851807846451206, 'subsample': 0.9943760100464112, 'colsample_bytree': 0.910236126593346}. Best is trial 16 with value: 2.0533960482996645.\n",
            "[I 2025-10-07 08:51:54,349] Trial 19 finished with value: 2.29483648048544 and parameters: {'n_estimators': 181, 'max_depth': 9, 'learning_rate': 0.1621856897300149, 'subsample': 0.9117717832596423, 'colsample_bytree': 0.7830779268368954}. Best is trial 16 with value: 2.0533960482996645.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 13: {'n_estimators': 164, 'max_depth': 8, 'learning_rate': 0.25865584850819207, 'subsample': 0.8975883235113203, 'colsample_bytree': 0.8494692628673504}\n",
            "\n",
            "--- LOOCV Fold 14/31 ---\n",
            "Fold 14 Epoch 1 Loss: 0.9392\n",
            "Fold 14 Epoch 2 Loss: 0.9189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:52:02,296] A new study created in memory with name: no-name-f15b6efd-0791-449d-8823-1fb0c17ec14a\n",
            "[I 2025-10-07 08:52:04,542] Trial 0 finished with value: 2.4030222105523777 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:05,208] Trial 1 finished with value: 2.4642197476451004 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:05,914] Trial 2 finished with value: 2.6352090295016093 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:06,646] Trial 3 finished with value: 2.6069276737393543 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:07,845] Trial 4 finished with value: 2.6520381348239193 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:08,987] Trial 5 finished with value: 2.468996189551939 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:10,642] Trial 6 finished with value: 2.904952170904459 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:12,731] Trial 7 finished with value: 2.720815674708576 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:13,387] Trial 8 finished with value: 2.6785912089708868 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:14,888] Trial 9 finished with value: 2.6411808763606865 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:16,069] Trial 10 finished with value: 2.4490902081408494 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.10672112706919769, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5076838686640521}. Best is trial 0 with value: 2.4030222105523777.\n",
            "[I 2025-10-07 08:52:17,791] Trial 11 finished with value: 2.3393153918763536 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.11285320502212905, 'subsample': 0.5427925342039944, 'colsample_bytree': 0.5018684889065917}. Best is trial 11 with value: 2.3393153918763536.\n",
            "[I 2025-10-07 08:52:19,427] Trial 12 finished with value: 2.3634196193292025 and parameters: {'n_estimators': 144, 'max_depth': 8, 'learning_rate': 0.10081853902880024, 'subsample': 0.5259324752234724, 'colsample_bytree': 0.5082179758109964}. Best is trial 11 with value: 2.3393153918763536.\n",
            "[I 2025-10-07 08:52:20,655] Trial 13 finished with value: 3.121939379508938 and parameters: {'n_estimators': 154, 'max_depth': 8, 'learning_rate': 0.29598873622155686, 'subsample': 0.503911956510792, 'colsample_bytree': 0.8229282203516206}. Best is trial 11 with value: 2.3393153918763536.\n",
            "[I 2025-10-07 08:52:21,560] Trial 14 finished with value: 2.6278693366582475 and parameters: {'n_estimators': 114, 'max_depth': 8, 'learning_rate': 0.10133215384123687, 'subsample': 0.5936677648398075, 'colsample_bytree': 0.5136575177203707}. Best is trial 11 with value: 2.3393153918763536.\n",
            "[I 2025-10-07 08:52:24,226] Trial 15 finished with value: 2.4819313895790702 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.03208899494279826, 'subsample': 0.5937663157166836, 'colsample_bytree': 0.6543715319553132}. Best is trial 11 with value: 2.3393153918763536.\n",
            "[I 2025-10-07 08:52:26,648] Trial 16 finished with value: 3.1007331845388335 and parameters: {'n_estimators': 174, 'max_depth': 7, 'learning_rate': 0.08280432737660769, 'subsample': 0.6518618401486819, 'colsample_bytree': 0.9827692441122055}. Best is trial 11 with value: 2.3393153918763536.\n",
            "[I 2025-10-07 08:52:27,784] Trial 17 finished with value: 2.5381073605896587 and parameters: {'n_estimators': 127, 'max_depth': 9, 'learning_rate': 0.03296312255364178, 'subsample': 0.5462802071188114, 'colsample_bytree': 0.7834630892431125}. Best is trial 11 with value: 2.3393153918763536.\n",
            "[I 2025-10-07 08:52:29,101] Trial 18 finished with value: 2.941185319917488 and parameters: {'n_estimators': 170, 'max_depth': 6, 'learning_rate': 0.17885497732428537, 'subsample': 0.8949828442656664, 'colsample_bytree': 0.910236126593346}. Best is trial 11 with value: 2.3393153918763536.\n",
            "[I 2025-10-07 08:52:30,713] Trial 19 finished with value: 2.676344668164418 and parameters: {'n_estimators': 116, 'max_depth': 9, 'learning_rate': 0.2702272241241035, 'subsample': 0.6391623280648224, 'colsample_bytree': 0.6894341987590544}. Best is trial 11 with value: 2.3393153918763536.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 14: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.11285320502212905, 'subsample': 0.5427925342039944, 'colsample_bytree': 0.5018684889065917}\n",
            "\n",
            "--- LOOCV Fold 15/31 ---\n",
            "Fold 15 Epoch 1 Loss: 0.9375\n",
            "Fold 15 Epoch 2 Loss: 0.8906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:52:39,046] A new study created in memory with name: no-name-9baa6e49-4b05-4fae-bbaa-0dd32b08e0b7\n",
            "[I 2025-10-07 08:52:40,424] Trial 0 finished with value: 2.400213361662383 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.400213361662383.\n",
            "[I 2025-10-07 08:52:40,848] Trial 1 finished with value: 2.087268965612753 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:41,359] Trial 2 finished with value: 2.333656581328286 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:42,081] Trial 3 finished with value: 2.4631874378292467 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:43,434] Trial 4 finished with value: 2.3687727658637354 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:45,286] Trial 5 finished with value: 2.312718635918674 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:47,429] Trial 6 finished with value: 2.9352709723634223 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:49,492] Trial 7 finished with value: 2.5411085883790525 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:50,228] Trial 8 finished with value: 2.5814517943925672 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:51,707] Trial 9 finished with value: 2.406686894821525 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:53,045] Trial 10 finished with value: 2.3063313702373605 and parameters: {'n_estimators': 139, 'max_depth': 7, 'learning_rate': 0.27047297227177763, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9168158328299749}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:54,347] Trial 11 finished with value: 2.7766551389203227 and parameters: {'n_estimators': 136, 'max_depth': 7, 'learning_rate': 0.28009047436880896, 'subsample': 0.5427925342039944, 'colsample_bytree': 0.8822343758608416}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:55,633] Trial 12 finished with value: 2.338206690423508 and parameters: {'n_estimators': 125, 'max_depth': 8, 'learning_rate': 0.2734262579715853, 'subsample': 0.5259599101527722, 'colsample_bytree': 0.8616315620551012}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:52:57,854] Trial 13 finished with value: 2.758620308539183 and parameters: {'n_estimators': 163, 'max_depth': 8, 'learning_rate': 0.1361058138823361, 'subsample': 0.8944520699149165, 'colsample_bytree': 0.8621196669592206}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:53:00,047] Trial 14 finished with value: 2.590270083434347 and parameters: {'n_estimators': 104, 'max_depth': 6, 'learning_rate': 0.1875387726457554, 'subsample': 0.6513023725975363, 'colsample_bytree': 0.9679715208755691}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:53:01,175] Trial 15 finished with value: 2.1964717074298363 and parameters: {'n_estimators': 292, 'max_depth': 2, 'learning_rate': 0.09510141055254434, 'subsample': 0.6101165452174964, 'colsample_bytree': 0.7878771413380138}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:53:02,354] Trial 16 finished with value: 2.225885532368113 and parameters: {'n_estimators': 300, 'max_depth': 2, 'learning_rate': 0.08719209000999652, 'subsample': 0.6107287974799808, 'colsample_bytree': 0.7984524585137823}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:53:03,590] Trial 17 finished with value: 2.129084530249976 and parameters: {'n_estimators': 300, 'max_depth': 2, 'learning_rate': 0.028069002597357614, 'subsample': 0.8791817744657436, 'colsample_bytree': 0.7985784745130113}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:53:06,333] Trial 18 finished with value: 2.657724178118363 and parameters: {'n_estimators': 245, 'max_depth': 5, 'learning_rate': 0.02938639447953787, 'subsample': 0.8852907125720524, 'colsample_bytree': 0.803504502439782}. Best is trial 1 with value: 2.087268965612753.\n",
            "[I 2025-10-07 08:53:06,964] Trial 19 finished with value: 2.0620678905879917 and parameters: {'n_estimators': 169, 'max_depth': 2, 'learning_rate': 0.030504805946356378, 'subsample': 0.9019885430838464, 'colsample_bytree': 0.6767990031486852}. Best is trial 19 with value: 2.0620678905879917.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 15: {'n_estimators': 169, 'max_depth': 2, 'learning_rate': 0.030504805946356378, 'subsample': 0.9019885430838464, 'colsample_bytree': 0.6767990031486852}\n",
            "\n",
            "--- LOOCV Fold 16/31 ---\n",
            "Fold 16 Epoch 1 Loss: 0.9256\n",
            "Fold 16 Epoch 2 Loss: 0.9592\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:53:16,355] A new study created in memory with name: no-name-8f8e4e25-8ab9-4fac-9889-e26072ec67c6\n",
            "[I 2025-10-07 08:53:17,744] Trial 0 finished with value: 2.2334592225350334 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:18,178] Trial 1 finished with value: 2.296646592680996 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:18,724] Trial 2 finished with value: 2.362985759450813 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:19,469] Trial 3 finished with value: 2.3817742206967734 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:20,659] Trial 4 finished with value: 2.42593396195772 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:21,804] Trial 5 finished with value: 2.347358468196307 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:23,427] Trial 6 finished with value: 2.6256432943240062 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:26,347] Trial 7 finished with value: 2.457713728176496 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:27,532] Trial 8 finished with value: 2.5409147675060657 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:29,015] Trial 9 finished with value: 2.4027555190311296 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:30,057] Trial 10 finished with value: 2.4972302410314264 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.10672112706919769, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5076838686640521}. Best is trial 0 with value: 2.2334592225350334.\n",
            "[I 2025-10-07 08:53:31,000] Trial 11 finished with value: 2.1355661431850064 and parameters: {'n_estimators': 136, 'max_depth': 6, 'learning_rate': 0.27188353688197003, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.8957660270445492}. Best is trial 11 with value: 2.1355661431850064.\n",
            "[I 2025-10-07 08:53:31,996] Trial 12 finished with value: 2.2100876958355706 and parameters: {'n_estimators': 138, 'max_depth': 8, 'learning_rate': 0.2735479091370399, 'subsample': 0.8676850494543448, 'colsample_bytree': 0.8754986202175145}. Best is trial 11 with value: 2.1355661431850064.\n",
            "[I 2025-10-07 08:53:32,916] Trial 13 finished with value: 2.3987657142621086 and parameters: {'n_estimators': 149, 'max_depth': 8, 'learning_rate': 0.2989102053028093, 'subsample': 0.8903098153397658, 'colsample_bytree': 0.88853698938351}. Best is trial 11 with value: 2.1355661431850064.\n",
            "[I 2025-10-07 08:53:33,766] Trial 14 finished with value: 2.7658106833856384 and parameters: {'n_estimators': 124, 'max_depth': 7, 'learning_rate': 0.2969323612040121, 'subsample': 0.8961689436930799, 'colsample_bytree': 0.8505812976968798}. Best is trial 11 with value: 2.1355661431850064.\n",
            "[I 2025-10-07 08:53:36,325] Trial 15 finished with value: 2.740450064559421 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.10249108373606142, 'subsample': 0.9198438358121845, 'colsample_bytree': 0.9519228483486102}. Best is trial 11 with value: 2.1355661431850064.\n",
            "[I 2025-10-07 08:53:37,332] Trial 16 finished with value: 2.757239934113767 and parameters: {'n_estimators': 174, 'max_depth': 6, 'learning_rate': 0.19170827738299262, 'subsample': 0.997430985975306, 'colsample_bytree': 0.8089267529497026}. Best is trial 11 with value: 2.1355661431850064.\n",
            "[I 2025-10-07 08:53:41,108] Trial 17 finished with value: 2.5848200140926534 and parameters: {'n_estimators': 172, 'max_depth': 8, 'learning_rate': 0.028904351098642492, 'subsample': 0.8608596682963604, 'colsample_bytree': 0.9266550436920955}. Best is trial 11 with value: 2.1355661431850064.\n",
            "[I 2025-10-07 08:53:42,503] Trial 18 finished with value: 2.471717593570651 and parameters: {'n_estimators': 126, 'max_depth': 7, 'learning_rate': 0.1546321288202552, 'subsample': 0.6471208002214496, 'colsample_bytree': 0.7869309611845361}. Best is trial 11 with value: 2.1355661431850064.\n",
            "[I 2025-10-07 08:53:43,388] Trial 19 finished with value: 2.166648339240575 and parameters: {'n_estimators': 110, 'max_depth': 5, 'learning_rate': 0.2702272241241035, 'subsample': 0.8330996495336215, 'colsample_bytree': 0.9049232735843799}. Best is trial 11 with value: 2.1355661431850064.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 16: {'n_estimators': 136, 'max_depth': 6, 'learning_rate': 0.27188353688197003, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.8957660270445492}\n",
            "\n",
            "--- LOOCV Fold 17/31 ---\n",
            "Fold 17 Epoch 1 Loss: 0.9079\n",
            "Fold 17 Epoch 2 Loss: 0.9479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:53:51,393] A new study created in memory with name: no-name-e478e015-4ecf-4fdc-8b7c-9477a74578ba\n",
            "[I 2025-10-07 08:53:53,659] Trial 0 finished with value: 2.3208417275751763 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.3208417275751763.\n",
            "[I 2025-10-07 08:53:54,351] Trial 1 finished with value: 2.349585027617584 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.3208417275751763.\n",
            "[I 2025-10-07 08:53:55,032] Trial 2 finished with value: 2.52636501214242 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.3208417275751763.\n",
            "[I 2025-10-07 08:53:55,752] Trial 3 finished with value: 2.280004342282501 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.280004342282501.\n",
            "[I 2025-10-07 08:53:56,973] Trial 4 finished with value: 2.3472818072194768 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 2.280004342282501.\n",
            "[I 2025-10-07 08:53:58,131] Trial 5 finished with value: 2.0518472776635597 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:53:59,691] Trial 6 finished with value: 2.299003863834024 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:01,702] Trial 7 finished with value: 2.2740132329772527 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:02,389] Trial 8 finished with value: 2.4984504043507587 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:03,831] Trial 9 finished with value: 2.140352082118735 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:06,594] Trial 10 finished with value: 2.5148760051911787 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.02964033652082882, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5072567334400258}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:09,208] Trial 11 finished with value: 2.1970043171364986 and parameters: {'n_estimators': 248, 'max_depth': 6, 'learning_rate': 0.06267435736339126, 'subsample': 0.8723304497253678, 'colsample_bytree': 0.5037892523442965}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:11,592] Trial 12 finished with value: 2.3136932595885233 and parameters: {'n_estimators': 246, 'max_depth': 5, 'learning_rate': 0.09936605342538195, 'subsample': 0.6585560646388384, 'colsample_bytree': 0.8385283305887051}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:12,152] Trial 13 finished with value: 2.2634123997377658 and parameters: {'n_estimators': 165, 'max_depth': 2, 'learning_rate': 0.029436497804366312, 'subsample': 0.8914524311747268, 'colsample_bytree': 0.554292090454271}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:15,419] Trial 14 finished with value: 2.0959104793542678 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.04252079435197325, 'subsample': 0.7643485527893804, 'colsample_bytree': 0.6731688441985715}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:16,602] Trial 15 finished with value: 2.458263988477419 and parameters: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.29596284206078843, 'subsample': 0.5937663157166837, 'colsample_bytree': 0.6782599578214803}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:21,587] Trial 16 finished with value: 2.2432002353013973 and parameters: {'n_estimators': 271, 'max_depth': 8, 'learning_rate': 0.0399880439895793, 'subsample': 0.7475227204153089, 'colsample_bytree': 0.8441147559750997}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:24,825] Trial 17 finished with value: 2.303071722575801 and parameters: {'n_estimators': 269, 'max_depth': 8, 'learning_rate': 0.019170308561956083, 'subsample': 0.9171981962581279, 'colsample_bytree': 0.7724161958998806}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:26,993] Trial 18 finished with value: 2.3609603998373307 and parameters: {'n_estimators': 229, 'max_depth': 9, 'learning_rate': 0.08875354400910485, 'subsample': 0.8445779114924311, 'colsample_bytree': 0.6703567034307713}. Best is trial 5 with value: 2.0518472776635597.\n",
            "[I 2025-10-07 08:54:28,866] Trial 19 finished with value: 2.2346809726658527 and parameters: {'n_estimators': 167, 'max_depth': 7, 'learning_rate': 0.04250407645125028, 'subsample': 0.7179832885458118, 'colsample_bytree': 0.7658662267651248}. Best is trial 5 with value: 2.0518472776635597.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 17: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}\n",
            "\n",
            "--- LOOCV Fold 18/31 ---\n",
            "Fold 18 Epoch 1 Loss: 0.9705\n",
            "Fold 18 Epoch 2 Loss: 0.9344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:54:38,092] A new study created in memory with name: no-name-dff61827-a68c-4310-8e7f-5d50a3f88854\n",
            "[I 2025-10-07 08:54:39,435] Trial 0 finished with value: 2.567153825415653 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.567153825415653.\n",
            "[I 2025-10-07 08:54:39,848] Trial 1 finished with value: 2.461388611071915 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:40,365] Trial 2 finished with value: 2.5191974708946967 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:41,105] Trial 3 finished with value: 2.5673313962884605 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:42,332] Trial 4 finished with value: 2.60215302178859 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:43,474] Trial 5 finished with value: 2.653961208201607 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:45,076] Trial 6 finished with value: 3.005195349111039 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:48,194] Trial 7 finished with value: 2.7616764981988657 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:49,289] Trial 8 finished with value: 2.7932044032476204 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:50,738] Trial 9 finished with value: 2.7330602343340997 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:52,114] Trial 10 finished with value: 3.170929022182436 and parameters: {'n_estimators': 139, 'max_depth': 7, 'learning_rate': 0.27047297227177763, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9168158328299749}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:52,837] Trial 11 finished with value: 3.0422315909895117 and parameters: {'n_estimators': 59, 'max_depth': 10, 'learning_rate': 0.28088381941259366, 'subsample': 0.5899164086425722, 'colsample_bytree': 0.8582590089757371}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:54,187] Trial 12 finished with value: 2.802322065449875 and parameters: {'n_estimators': 115, 'max_depth': 8, 'learning_rate': 0.13436119963045842, 'subsample': 0.6427754537375224, 'colsample_bytree': 0.8102888477597007}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:54,875] Trial 13 finished with value: 3.303093430659762 and parameters: {'n_estimators': 52, 'max_depth': 6, 'learning_rate': 0.1361058138823361, 'subsample': 0.8944520699149165, 'colsample_bytree': 0.7996547751838623}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:56,657] Trial 14 finished with value: 2.8793334161828086 and parameters: {'n_estimators': 157, 'max_depth': 8, 'learning_rate': 0.1737183744716791, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.9057260017887323}. Best is trial 1 with value: 2.461388611071915.\n",
            "[I 2025-10-07 08:54:57,694] Trial 15 finished with value: 2.37095877363084 and parameters: {'n_estimators': 292, 'max_depth': 2, 'learning_rate': 0.08962717626942573, 'subsample': 0.6274745095999714, 'colsample_bytree': 0.6924366913699725}. Best is trial 15 with value: 2.37095877363084.\n",
            "[I 2025-10-07 08:54:58,781] Trial 16 finished with value: 2.528466932267004 and parameters: {'n_estimators': 300, 'max_depth': 2, 'learning_rate': 0.08719209000999652, 'subsample': 0.7202649980833455, 'colsample_bytree': 0.6968404184730032}. Best is trial 15 with value: 2.37095877363084.\n",
            "[I 2025-10-07 08:55:00,613] Trial 17 finished with value: 2.5589395897274043 and parameters: {'n_estimators': 300, 'max_depth': 2, 'learning_rate': 0.028069002597357614, 'subsample': 0.8791817744657436, 'colsample_bytree': 0.7844162191142547}. Best is trial 15 with value: 2.37095877363084.\n",
            "[I 2025-10-07 08:55:03,996] Trial 18 finished with value: 2.8732285071022936 and parameters: {'n_estimators': 246, 'max_depth': 5, 'learning_rate': 0.09031256134732037, 'subsample': 0.6604822481437748, 'colsample_bytree': 0.8620313930247293}. Best is trial 15 with value: 2.37095877363084.\n",
            "[I 2025-10-07 08:55:04,633] Trial 19 finished with value: 2.415362569068224 and parameters: {'n_estimators': 172, 'max_depth': 2, 'learning_rate': 0.041799869026699674, 'subsample': 0.7514036985830645, 'colsample_bytree': 0.675760829066606}. Best is trial 15 with value: 2.37095877363084.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 18: {'n_estimators': 292, 'max_depth': 2, 'learning_rate': 0.08962717626942573, 'subsample': 0.6274745095999714, 'colsample_bytree': 0.6924366913699725}\n",
            "\n",
            "--- LOOCV Fold 19/31 ---\n",
            "Fold 19 Epoch 1 Loss: 0.9553\n",
            "Fold 19 Epoch 2 Loss: 0.9223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:55:12,431] A new study created in memory with name: no-name-d3623e1b-d63f-46bb-8ce0-f37397c20859\n",
            "[I 2025-10-07 08:55:14,707] Trial 0 finished with value: 2.757265070463046 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.757265070463046.\n",
            "[I 2025-10-07 08:55:15,389] Trial 1 finished with value: 2.649406317855912 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.649406317855912.\n",
            "[I 2025-10-07 08:55:16,146] Trial 2 finished with value: 2.5802004738202995 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.5802004738202995.\n",
            "[I 2025-10-07 08:55:16,884] Trial 3 finished with value: 2.5913766235939204 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 2.5802004738202995.\n",
            "[I 2025-10-07 08:55:18,097] Trial 4 finished with value: 2.5087242279938313 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:19,249] Trial 5 finished with value: 2.645030517051525 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:20,841] Trial 6 finished with value: 2.882904830190418 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:22,837] Trial 7 finished with value: 2.593851283096986 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:23,546] Trial 8 finished with value: 2.664946986844396 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:25,024] Trial 9 finished with value: 2.6851676668311635 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:28,994] Trial 10 finished with value: 2.594728025792752 and parameters: {'n_estimators': 289, 'max_depth': 7, 'learning_rate': 0.023969116039403743, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8200442512337782}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:30,303] Trial 11 finished with value: 3.09785244603476 and parameters: {'n_estimators': 158, 'max_depth': 10, 'learning_rate': 0.2757612719094118, 'subsample': 0.5996007697216827, 'colsample_bytree': 0.739627013585424}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:31,542] Trial 12 finished with value: 2.564348118540644 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.029882896955595446, 'subsample': 0.653615691352136, 'colsample_bytree': 0.6712579968688783}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:32,758] Trial 13 finished with value: 2.5335294255130725 and parameters: {'n_estimators': 134, 'max_depth': 8, 'learning_rate': 0.029105598116745866, 'subsample': 0.6707407277614729, 'colsample_bytree': 0.6937971020319804}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:34,922] Trial 14 finished with value: 2.646268403593153 and parameters: {'n_estimators': 191, 'max_depth': 8, 'learning_rate': 0.027722061261020493, 'subsample': 0.6887471426927588, 'colsample_bytree': 0.8138214726279749}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:36,107] Trial 15 finished with value: 2.6617554153846745 and parameters: {'n_estimators': 122, 'max_depth': 6, 'learning_rate': 0.020232867820031846, 'subsample': 0.5392311924833902, 'colsample_bytree': 0.936148316322498}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:37,725] Trial 16 finished with value: 2.6680682298629135 and parameters: {'n_estimators': 171, 'max_depth': 8, 'learning_rate': 0.03777128138812329, 'subsample': 0.6207774664901698, 'colsample_bytree': 0.700477984841022}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:40,655] Trial 17 finished with value: 2.6952665045214426 and parameters: {'n_estimators': 228, 'max_depth': 5, 'learning_rate': 0.039147136505991394, 'subsample': 0.9040330172235821, 'colsample_bytree': 0.7718136711123591}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:44,941] Trial 18 finished with value: 2.6838733336602187 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.01589978423522925, 'subsample': 0.7163079035763923, 'colsample_bytree': 0.8705765955899738}. Best is trial 4 with value: 2.5087242279938313.\n",
            "[I 2025-10-07 08:55:45,694] Trial 19 finished with value: 2.3809803364374176 and parameters: {'n_estimators': 188, 'max_depth': 2, 'learning_rate': 0.04035139517080454, 'subsample': 0.561069102686408, 'colsample_bytree': 0.7727918898540465}. Best is trial 19 with value: 2.3809803364374176.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 19: {'n_estimators': 188, 'max_depth': 2, 'learning_rate': 0.04035139517080454, 'subsample': 0.561069102686408, 'colsample_bytree': 0.7727918898540465}\n",
            "\n",
            "--- LOOCV Fold 20/31 ---\n",
            "Fold 20 Epoch 1 Loss: 0.8782\n",
            "Fold 20 Epoch 2 Loss: 0.8921\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:55:53,251] A new study created in memory with name: no-name-9731ac91-dfc6-4f7c-900f-e1ee07996a3f\n",
            "[I 2025-10-07 08:55:57,152] Trial 0 finished with value: 2.288003562539383 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:55:57,565] Trial 1 finished with value: 2.38484995133774 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:55:58,095] Trial 2 finished with value: 2.6970991899449634 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:55:58,827] Trial 3 finished with value: 2.4569832190550134 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:56:00,109] Trial 4 finished with value: 2.575989541400514 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:56:01,292] Trial 5 finished with value: 2.325500909650204 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:56:02,884] Trial 6 finished with value: 2.380762969923479 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:56:04,964] Trial 7 finished with value: 2.3541799060838016 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:56:05,717] Trial 8 finished with value: 2.4592812198389966 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:56:07,536] Trial 9 finished with value: 2.3492083112016635 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:56:09,222] Trial 10 finished with value: 2.48378274726999 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.10672112706919769, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5076838686640521}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:56:11,369] Trial 11 finished with value: 2.327076786451836 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.08757126827307511, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.5020730763700768}. Best is trial 0 with value: 2.288003562539383.\n",
            "[I 2025-10-07 08:56:12,668] Trial 12 finished with value: 2.2775479073182816 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.03074474394845574, 'subsample': 0.8806736357247846, 'colsample_bytree': 0.5380251134894303}. Best is trial 12 with value: 2.2775479073182816.\n",
            "[I 2025-10-07 08:56:14,625] Trial 13 finished with value: 2.382274588123352 and parameters: {'n_estimators': 148, 'max_depth': 8, 'learning_rate': 0.030179691732225438, 'subsample': 0.8979526911256507, 'colsample_bytree': 0.8229282203516206}. Best is trial 12 with value: 2.2775479073182816.\n",
            "[I 2025-10-07 08:56:16,222] Trial 14 finished with value: 2.1923954865433384 and parameters: {'n_estimators': 141, 'max_depth': 8, 'learning_rate': 0.028861072903516246, 'subsample': 0.9126144023612548, 'colsample_bytree': 0.6738167152482913}. Best is trial 14 with value: 2.1923954865433384.\n",
            "[I 2025-10-07 08:56:17,652] Trial 15 finished with value: 2.266547198092754 and parameters: {'n_estimators': 127, 'max_depth': 8, 'learning_rate': 0.03208899494279826, 'subsample': 0.9234980616943328, 'colsample_bytree': 0.6749876840078481}. Best is trial 14 with value: 2.1923954865433384.\n",
            "[I 2025-10-07 08:56:18,918] Trial 16 finished with value: 2.247051240041423 and parameters: {'n_estimators': 117, 'max_depth': 8, 'learning_rate': 0.0203622958010125, 'subsample': 0.9980771142314673, 'colsample_bytree': 0.670872921229478}. Best is trial 14 with value: 2.1923954865433384.\n",
            "[I 2025-10-07 08:56:21,842] Trial 17 finished with value: 2.380512426689563 and parameters: {'n_estimators': 176, 'max_depth': 7, 'learning_rate': 0.020435972718608832, 'subsample': 0.9975689038931951, 'colsample_bytree': 0.8224626619464634}. Best is trial 14 with value: 2.1923954865433384.\n",
            "[I 2025-10-07 08:56:23,875] Trial 18 finished with value: 2.3323161688846854 and parameters: {'n_estimators': 117, 'max_depth': 9, 'learning_rate': 0.016436623200632175, 'subsample': 0.9371661696363025, 'colsample_bytree': 0.7907003499878582}. Best is trial 14 with value: 2.1923954865433384.\n",
            "[I 2025-10-07 08:56:26,352] Trial 19 finished with value: 2.3929762416333986 and parameters: {'n_estimators': 174, 'max_depth': 7, 'learning_rate': 0.01993941610914833, 'subsample': 0.9725999806988521, 'colsample_bytree': 0.9072950643400716}. Best is trial 14 with value: 2.1923954865433384.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 20: {'n_estimators': 141, 'max_depth': 8, 'learning_rate': 0.028861072903516246, 'subsample': 0.9126144023612548, 'colsample_bytree': 0.6738167152482913}\n",
            "\n",
            "--- LOOCV Fold 21/31 ---\n",
            "Fold 21 Epoch 1 Loss: 0.9002\n",
            "Fold 21 Epoch 2 Loss: 0.9721\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:56:37,299] A new study created in memory with name: no-name-8ee4cec4-d64b-41da-b499-fce49a411109\n",
            "[I 2025-10-07 08:56:38,677] Trial 0 finished with value: 2.6837810976731036 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.6837810976731036.\n",
            "[I 2025-10-07 08:56:39,096] Trial 1 finished with value: 2.5312395124419966 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.5312395124419966.\n",
            "[I 2025-10-07 08:56:39,611] Trial 2 finished with value: 2.615988467805119 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 2.5312395124419966.\n",
            "[I 2025-10-07 08:56:40,333] Trial 3 finished with value: 2.747081079030136 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 2.5312395124419966.\n",
            "[I 2025-10-07 08:56:41,532] Trial 4 finished with value: 2.8308259012074073 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 2.5312395124419966.\n",
            "[I 2025-10-07 08:56:42,659] Trial 5 finished with value: 2.5150058878337367 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:44,204] Trial 6 finished with value: 2.664620522860949 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:46,168] Trial 7 finished with value: 2.68337034508079 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:46,872] Trial 8 finished with value: 2.6900355430280958 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:48,884] Trial 9 finished with value: 2.7926487314321036 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:51,608] Trial 10 finished with value: 2.546976913727998 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.02964033652082882, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5072567334400258}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:52,122] Trial 11 finished with value: 2.821828950576259 and parameters: {'n_estimators': 136, 'max_depth': 2, 'learning_rate': 0.27712482369073105, 'subsample': 0.8739084974067326, 'colsample_bytree': 0.8692027723625104}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:53,185] Trial 12 finished with value: 2.551267873517747 and parameters: {'n_estimators': 298, 'max_depth': 2, 'learning_rate': 0.10784779247007473, 'subsample': 0.8806736357247846, 'colsample_bytree': 0.8443070778541023}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:54,320] Trial 13 finished with value: 3.1611294152515295 and parameters: {'n_estimators': 145, 'max_depth': 6, 'learning_rate': 0.29397500854068426, 'subsample': 0.6376629900391765, 'colsample_bytree': 0.8362861806510449}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:57,861] Trial 14 finished with value: 2.8513245705957004 and parameters: {'n_estimators': 248, 'max_depth': 8, 'learning_rate': 0.0383477048262245, 'subsample': 0.7837958068143924, 'colsample_bytree': 0.9444452098985002}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:59,056] Trial 15 finished with value: 2.789843257842544 and parameters: {'n_estimators': 108, 'max_depth': 5, 'learning_rate': 0.09214174051661962, 'subsample': 0.9140258921869188, 'colsample_bytree': 0.7782373504720331}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:56:59,740] Trial 16 finished with value: 2.6417427992258276 and parameters: {'n_estimators': 177, 'max_depth': 2, 'learning_rate': 0.17889396860003975, 'subsample': 0.8317032143059913, 'colsample_bytree': 0.9047894591765572}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:57:01,514] Trial 17 finished with value: 2.711601474661189 and parameters: {'n_estimators': 246, 'max_depth': 3, 'learning_rate': 0.02068086816741835, 'subsample': 0.7261669761185063, 'colsample_bytree': 0.7844162191142547}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:57:04,055] Trial 18 finished with value: 2.723885709849729 and parameters: {'n_estimators': 166, 'max_depth': 5, 'learning_rate': 0.0832814627218555, 'subsample': 0.6654554892776469, 'colsample_bytree': 0.6868199329682921}. Best is trial 5 with value: 2.5150058878337367.\n",
            "[I 2025-10-07 08:57:04,912] Trial 19 finished with value: 2.2532609468252645 and parameters: {'n_estimators': 267, 'max_depth': 2, 'learning_rate': 0.14920643617808257, 'subsample': 0.56391216214132, 'colsample_bytree': 0.5075940447973836}. Best is trial 19 with value: 2.2532609468252645.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 21: {'n_estimators': 267, 'max_depth': 2, 'learning_rate': 0.14920643617808257, 'subsample': 0.56391216214132, 'colsample_bytree': 0.5075940447973836}\n",
            "\n",
            "--- LOOCV Fold 22/31 ---\n",
            "Fold 22 Epoch 1 Loss: 0.9135\n",
            "Fold 22 Epoch 2 Loss: 0.8861\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:57:12,541] A new study created in memory with name: no-name-7a606957-458a-43ea-bbeb-3b799ccf25c6\n",
            "[I 2025-10-07 08:57:13,869] Trial 0 finished with value: 2.3919236374549957 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.3919236374549957.\n",
            "[I 2025-10-07 08:57:14,276] Trial 1 finished with value: 2.4426159111822416 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.3919236374549957.\n",
            "[I 2025-10-07 08:57:15,023] Trial 2 finished with value: 2.516467315629285 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.3919236374549957.\n",
            "[I 2025-10-07 08:57:16,240] Trial 3 finished with value: 2.264471049821058 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:18,130] Trial 4 finished with value: 2.4413831615923542 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:19,264] Trial 5 finished with value: 2.384279125409715 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:20,840] Trial 6 finished with value: 2.6372110295404103 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:22,947] Trial 7 finished with value: 2.4543743444830914 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:23,666] Trial 8 finished with value: 2.5929490155188035 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:25,155] Trial 9 finished with value: 2.5060434305511956 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:26,479] Trial 10 finished with value: 2.6620503954641173 and parameters: {'n_estimators': 141, 'max_depth': 7, 'learning_rate': 0.030315725522749647, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8285618345363206}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:29,144] Trial 11 finished with value: 2.380883371458382 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.06463513151371238, 'subsample': 0.6758580632546128, 'colsample_bytree': 0.5020730763700768}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:31,997] Trial 12 finished with value: 2.3675931394633474 and parameters: {'n_estimators': 299, 'max_depth': 7, 'learning_rate': 0.08787850607501545, 'subsample': 0.6871730874891191, 'colsample_bytree': 0.509429036272865}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:33,383] Trial 13 finished with value: 2.347014310483838 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.09271853302861457, 'subsample': 0.6730069363658565, 'colsample_bytree': 0.6454936010745959}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:34,365] Trial 14 finished with value: 2.789661252133155 and parameters: {'n_estimators': 134, 'max_depth': 8, 'learning_rate': 0.2873672302988123, 'subsample': 0.5653256167561652, 'colsample_bytree': 0.6738167152482913}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:35,346] Trial 15 finished with value: 2.62984082965742 and parameters: {'n_estimators': 103, 'max_depth': 8, 'learning_rate': 0.03389312852746559, 'subsample': 0.6219374804317841, 'colsample_bytree': 0.7936744128415706}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:36,930] Trial 16 finished with value: 2.5447538480246146 and parameters: {'n_estimators': 168, 'max_depth': 5, 'learning_rate': 0.09792171490762987, 'subsample': 0.7304290955766622, 'colsample_bytree': 0.6688073286968252}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:38,532] Trial 17 finished with value: 2.5897482837593904 and parameters: {'n_estimators': 115, 'max_depth': 9, 'learning_rate': 0.04024740388095653, 'subsample': 0.8657177043361405, 'colsample_bytree': 0.9398069633632561}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:39,097] Trial 18 finished with value: 2.641656714947465 and parameters: {'n_estimators': 67, 'max_depth': 5, 'learning_rate': 0.019631797854441864, 'subsample': 0.7276826999536898, 'colsample_bytree': 0.6757675011837603}. Best is trial 3 with value: 2.264471049821058.\n",
            "[I 2025-10-07 08:57:41,199] Trial 19 finished with value: 2.5891919271620383 and parameters: {'n_estimators': 173, 'max_depth': 8, 'learning_rate': 0.08225479972309517, 'subsample': 0.8808313666551089, 'colsample_bytree': 0.760586251458076}. Best is trial 3 with value: 2.264471049821058.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 22: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}\n",
            "\n",
            "--- LOOCV Fold 23/31 ---\n",
            "Fold 23 Epoch 1 Loss: 0.8789\n",
            "Fold 23 Epoch 2 Loss: 0.8778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:57:50,454] A new study created in memory with name: no-name-933a99d2-c7e4-4614-afc3-5f5e59409b02\n",
            "[I 2025-10-07 08:57:51,846] Trial 0 finished with value: 2.291855895383205 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.291855895383205.\n",
            "[I 2025-10-07 08:57:52,245] Trial 1 finished with value: 2.263614813468331 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.263614813468331.\n",
            "[I 2025-10-07 08:57:52,766] Trial 2 finished with value: 2.348249471642667 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 2.263614813468331.\n",
            "[I 2025-10-07 08:57:53,501] Trial 3 finished with value: 2.2481889166729068 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.2481889166729068.\n",
            "[I 2025-10-07 08:57:54,729] Trial 4 finished with value: 2.3410496361967246 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 2.2481889166729068.\n",
            "[I 2025-10-07 08:57:56,339] Trial 5 finished with value: 2.219985629681822 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 2.219985629681822.\n",
            "[I 2025-10-07 08:57:58,866] Trial 6 finished with value: 2.184162547473105 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 6 with value: 2.184162547473105.\n",
            "[I 2025-10-07 08:58:00,907] Trial 7 finished with value: 2.177381513793559 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 7 with value: 2.177381513793559.\n",
            "[I 2025-10-07 08:58:01,644] Trial 8 finished with value: 2.4122864184539723 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 7 with value: 2.177381513793559.\n",
            "[I 2025-10-07 08:58:03,164] Trial 9 finished with value: 2.2074626673231057 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 7 with value: 2.177381513793559.\n",
            "[I 2025-10-07 08:58:05,905] Trial 10 finished with value: 2.4692366145521674 and parameters: {'n_estimators': 295, 'max_depth': 7, 'learning_rate': 0.023413669026819843, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8200442512337782}. Best is trial 7 with value: 2.177381513793559.\n",
            "[I 2025-10-07 08:58:10,341] Trial 11 finished with value: 2.2745133678912244 and parameters: {'n_estimators': 268, 'max_depth': 6, 'learning_rate': 0.010244149715931178, 'subsample': 0.9903324566798157, 'colsample_bytree': 0.990842956734007}. Best is trial 7 with value: 2.177381513793559.\n",
            "[I 2025-10-07 08:58:13,311] Trial 12 finished with value: 2.2194121786200904 and parameters: {'n_estimators': 178, 'max_depth': 5, 'learning_rate': 0.01924001519493744, 'subsample': 0.9108740628146075, 'colsample_bytree': 0.9970029045473664}. Best is trial 7 with value: 2.177381513793559.\n",
            "[I 2025-10-07 08:58:14,394] Trial 13 finished with value: 2.140778289748728 and parameters: {'n_estimators': 239, 'max_depth': 2, 'learning_rate': 0.017005609599943586, 'subsample': 0.8822868994698431, 'colsample_bytree': 0.8847647343909865}. Best is trial 13 with value: 2.140778289748728.\n",
            "[I 2025-10-07 08:58:15,514] Trial 14 finished with value: 2.148781906924607 and parameters: {'n_estimators': 248, 'max_depth': 2, 'learning_rate': 0.034935678448076865, 'subsample': 0.8767016211809998, 'colsample_bytree': 0.8742202544509611}. Best is trial 13 with value: 2.140778289748728.\n",
            "[I 2025-10-07 08:58:16,863] Trial 15 finished with value: 2.2218615131242876 and parameters: {'n_estimators': 298, 'max_depth': 2, 'learning_rate': 0.03473398931486679, 'subsample': 0.8530696715940759, 'colsample_bytree': 0.8865159860227801}. Best is trial 13 with value: 2.140778289748728.\n",
            "[I 2025-10-07 08:58:17,587] Trial 16 finished with value: 2.2062764029701727 and parameters: {'n_estimators': 152, 'max_depth': 2, 'learning_rate': 0.0394267635435513, 'subsample': 0.8972380433598665, 'colsample_bytree': 0.9047894591765572}. Best is trial 13 with value: 2.140778289748728.\n",
            "[I 2025-10-07 08:58:20,306] Trial 17 finished with value: 2.0978378810453187 and parameters: {'n_estimators': 227, 'max_depth': 8, 'learning_rate': 0.017947165791772544, 'subsample': 0.9040330172235821, 'colsample_bytree': 0.8011757943879945}. Best is trial 17 with value: 2.0978378810453187.\n",
            "[I 2025-10-07 08:58:23,833] Trial 18 finished with value: 2.0263879835227194 and parameters: {'n_estimators': 228, 'max_depth': 8, 'learning_rate': 0.021233594255386363, 'subsample': 0.9316548592000048, 'colsample_bytree': 0.7786203342572622}. Best is trial 18 with value: 2.0263879835227194.\n",
            "[I 2025-10-07 08:58:26,624] Trial 19 finished with value: 2.040023766337603 and parameters: {'n_estimators': 182, 'max_depth': 8, 'learning_rate': 0.022356097412002694, 'subsample': 0.9272692710409468, 'colsample_bytree': 0.7732020806697814}. Best is trial 18 with value: 2.0263879835227194.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 23: {'n_estimators': 228, 'max_depth': 8, 'learning_rate': 0.021233594255386363, 'subsample': 0.9316548592000048, 'colsample_bytree': 0.7786203342572622}\n",
            "\n",
            "--- LOOCV Fold 24/31 ---\n",
            "Fold 24 Epoch 1 Loss: 0.9208\n",
            "Fold 24 Epoch 2 Loss: 0.8905\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:58:34,732] A new study created in memory with name: no-name-0280f699-110d-4649-adae-898bea2c6a0f\n",
            "[I 2025-10-07 08:58:36,606] Trial 0 finished with value: 2.3291149287316735 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.3291149287316735.\n",
            "[I 2025-10-07 08:58:37,304] Trial 1 finished with value: 2.5667638097618166 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.3291149287316735.\n",
            "[I 2025-10-07 08:58:38,137] Trial 2 finished with value: 2.302950339320037 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.302950339320037.\n",
            "[I 2025-10-07 08:58:39,141] Trial 3 finished with value: 2.3159316306187105 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 2.302950339320037.\n",
            "[I 2025-10-07 08:58:40,366] Trial 4 finished with value: 2.304661623438022 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 2.302950339320037.\n",
            "[I 2025-10-07 08:58:41,515] Trial 5 finished with value: 2.2346719769025016 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 2.2346719769025016.\n",
            "[I 2025-10-07 08:58:43,107] Trial 6 finished with value: 2.438468924872946 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 2.2346719769025016.\n",
            "[I 2025-10-07 08:58:45,117] Trial 7 finished with value: 2.3294912878640686 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 5 with value: 2.2346719769025016.\n",
            "[I 2025-10-07 08:58:45,836] Trial 8 finished with value: 2.4910947275222393 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 5 with value: 2.2346719769025016.\n",
            "[I 2025-10-07 08:58:47,382] Trial 9 finished with value: 2.319095638861962 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 5 with value: 2.2346719769025016.\n",
            "[I 2025-10-07 08:58:49,650] Trial 10 finished with value: 2.4465905181880037 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.02964033652082882, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5072567334400258}. Best is trial 5 with value: 2.2346719769025016.\n",
            "[I 2025-10-07 08:58:50,936] Trial 11 finished with value: 2.1372271349208782 and parameters: {'n_estimators': 136, 'max_depth': 10, 'learning_rate': 0.2762122998112033, 'subsample': 0.5971603596691708, 'colsample_bytree': 0.5020730763700768}. Best is trial 11 with value: 2.1372271349208782.\n",
            "[I 2025-10-07 08:58:52,290] Trial 12 finished with value: 2.3875965178115734 and parameters: {'n_estimators': 138, 'max_depth': 8, 'learning_rate': 0.2564397559593849, 'subsample': 0.6081804339661661, 'colsample_bytree': 0.5082283557871199}. Best is trial 11 with value: 2.1372271349208782.\n",
            "[I 2025-10-07 08:58:54,289] Trial 13 finished with value: 2.4787661757866597 and parameters: {'n_estimators': 152, 'max_depth': 8, 'learning_rate': 0.09374720750315586, 'subsample': 0.8914524311747268, 'colsample_bytree': 0.8229282203516206}. Best is trial 11 with value: 2.1372271349208782.\n",
            "[I 2025-10-07 08:58:55,176] Trial 14 finished with value: 2.0762919690764994 and parameters: {'n_estimators': 258, 'max_depth': 6, 'learning_rate': 0.29478907845292945, 'subsample': 0.6460967579436883, 'colsample_bytree': 0.5072620627319698}. Best is trial 14 with value: 2.0762919690764994.\n",
            "[I 2025-10-07 08:58:56,353] Trial 15 finished with value: 2.555733289348416 and parameters: {'n_estimators': 298, 'max_depth': 6, 'learning_rate': 0.29596284206078843, 'subsample': 0.6259860524915188, 'colsample_bytree': 0.654526397080815}. Best is trial 14 with value: 2.0762919690764994.\n",
            "[I 2025-10-07 08:58:57,844] Trial 16 finished with value: 2.416387277020801 and parameters: {'n_estimators': 127, 'max_depth': 9, 'learning_rate': 0.11475419530425926, 'subsample': 0.5188347927302239, 'colsample_bytree': 0.9828912604212885}. Best is trial 14 with value: 2.0762919690764994.\n",
            "[I 2025-10-07 08:58:59,317] Trial 17 finished with value: 2.4092095389845136 and parameters: {'n_estimators': 172, 'max_depth': 6, 'learning_rate': 0.18925901806428228, 'subsample': 0.560685560272977, 'colsample_bytree': 0.7838158961797974}. Best is trial 14 with value: 2.0762919690764994.\n",
            "[I 2025-10-07 08:59:01,951] Trial 18 finished with value: 2.5949853212309297 and parameters: {'n_estimators': 247, 'max_depth': 5, 'learning_rate': 0.09267721647387621, 'subsample': 0.6740295158339392, 'colsample_bytree': 0.910236126593346}. Best is trial 14 with value: 2.0762919690764994.\n",
            "[I 2025-10-07 08:59:03,552] Trial 19 finished with value: 3.0039833300272387 and parameters: {'n_estimators': 180, 'max_depth': 8, 'learning_rate': 0.2702272241241035, 'subsample': 0.6527137739308639, 'colsample_bytree': 0.689644952988969}. Best is trial 14 with value: 2.0762919690764994.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 24: {'n_estimators': 258, 'max_depth': 6, 'learning_rate': 0.29478907845292945, 'subsample': 0.6460967579436883, 'colsample_bytree': 0.5072620627319698}\n",
            "\n",
            "--- LOOCV Fold 25/31 ---\n",
            "Fold 25 Epoch 1 Loss: 0.9610\n",
            "Fold 25 Epoch 2 Loss: 0.9358\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:59:11,947] A new study created in memory with name: no-name-1b3cea95-f597-4d6c-98de-aa0d9c026845\n",
            "[I 2025-10-07 08:59:13,334] Trial 0 finished with value: 2.4794536704352605 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.4794536704352605.\n",
            "[I 2025-10-07 08:59:13,741] Trial 1 finished with value: 2.654182161717078 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.4794536704352605.\n",
            "[I 2025-10-07 08:59:14,239] Trial 2 finished with value: 2.48353671990874 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.4794536704352605.\n",
            "[I 2025-10-07 08:59:14,998] Trial 3 finished with value: 2.439853842320889 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.439853842320889.\n",
            "[I 2025-10-07 08:59:16,444] Trial 4 finished with value: 2.5766543438411746 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 2.439853842320889.\n",
            "[I 2025-10-07 08:59:18,352] Trial 5 finished with value: 2.387899299403627 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:20,304] Trial 6 finished with value: 2.6827948102511883 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:22,359] Trial 7 finished with value: 2.52255063858292 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:23,073] Trial 8 finished with value: 2.5792499879162185 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:24,579] Trial 9 finished with value: 2.4717676851522943 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:26,555] Trial 10 finished with value: 2.4755542367387253 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.02964033652082882, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5072567334400258}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:27,666] Trial 11 finished with value: 2.4952375043012593 and parameters: {'n_estimators': 136, 'max_depth': 6, 'learning_rate': 0.06463513151371238, 'subsample': 0.6758580632546128, 'colsample_bytree': 0.5020730763700768}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:29,050] Trial 12 finished with value: 2.497579930080598 and parameters: {'n_estimators': 142, 'max_depth': 5, 'learning_rate': 0.08787850607501545, 'subsample': 0.8806736357247846, 'colsample_bytree': 0.6712579968688783}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:30,718] Trial 13 finished with value: 2.4324015769765897 and parameters: {'n_estimators': 261, 'max_depth': 2, 'learning_rate': 0.03032430179630666, 'subsample': 0.7005754115147793, 'colsample_bytree': 0.8229282203516206}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:32,691] Trial 14 finished with value: 2.4592504197122946 and parameters: {'n_estimators': 292, 'max_depth': 2, 'learning_rate': 0.03069135508355042, 'subsample': 0.6209371149754793, 'colsample_bytree': 0.8350504632309295}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:34,012] Trial 15 finished with value: 2.730766662061704 and parameters: {'n_estimators': 246, 'max_depth': 8, 'learning_rate': 0.29596284206078843, 'subsample': 0.6057060850536673, 'colsample_bytree': 0.8304278626461432}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:35,195] Trial 16 finished with value: 2.388023618829114 and parameters: {'n_estimators': 253, 'max_depth': 2, 'learning_rate': 0.020352303298579975, 'subsample': 0.8688325692371892, 'colsample_bytree': 0.9219900508555974}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:37,023] Trial 17 finished with value: 2.666777270692792 and parameters: {'n_estimators': 228, 'max_depth': 3, 'learning_rate': 0.019170308561956083, 'subsample': 0.9033301420805847, 'colsample_bytree': 0.9980084946806067}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:39,298] Trial 18 finished with value: 2.7409817491197215 and parameters: {'n_estimators': 178, 'max_depth': 5, 'learning_rate': 0.04104975015072629, 'subsample': 0.8949828442656664, 'colsample_bytree': 0.910236126593346}. Best is trial 5 with value: 2.387899299403627.\n",
            "[I 2025-10-07 08:59:40,535] Trial 19 finished with value: 2.3255715887892987 and parameters: {'n_estimators': 271, 'max_depth': 2, 'learning_rate': 0.019772552781473295, 'subsample': 0.8339264427206704, 'colsample_bytree': 0.9126321526111343}. Best is trial 19 with value: 2.3255715887892987.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 25: {'n_estimators': 271, 'max_depth': 2, 'learning_rate': 0.019772552781473295, 'subsample': 0.8339264427206704, 'colsample_bytree': 0.9126321526111343}\n",
            "\n",
            "--- LOOCV Fold 26/31 ---\n",
            "Fold 26 Epoch 1 Loss: 0.9426\n",
            "Fold 26 Epoch 2 Loss: 0.8836\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:59:49,806] A new study created in memory with name: no-name-2c37c7ce-5020-4013-9d9e-963c820ebe36\n",
            "[I 2025-10-07 08:59:51,197] Trial 0 finished with value: 2.3698163699828676 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.3698163699828676.\n",
            "[I 2025-10-07 08:59:51,636] Trial 1 finished with value: 2.476516995089382 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.3698163699828676.\n",
            "[I 2025-10-07 08:59:52,142] Trial 2 finished with value: 2.5574139993205356 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.3698163699828676.\n",
            "[I 2025-10-07 08:59:52,884] Trial 3 finished with value: 2.311049610547733 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.311049610547733.\n",
            "[I 2025-10-07 08:59:54,142] Trial 4 finished with value: 2.2558454634608505 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 4 with value: 2.2558454634608505.\n",
            "[I 2025-10-07 08:59:55,301] Trial 5 finished with value: 2.2874620659696876 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 4 with value: 2.2558454634608505.\n",
            "[I 2025-10-07 08:59:57,264] Trial 6 finished with value: 2.386596788441829 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 4 with value: 2.2558454634608505.\n",
            "[I 2025-10-07 09:00:00,399] Trial 7 finished with value: 2.239621846886659 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 7 with value: 2.239621846886659.\n",
            "[I 2025-10-07 09:00:01,124] Trial 8 finished with value: 2.515796067809075 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 7 with value: 2.239621846886659.\n",
            "[I 2025-10-07 09:00:02,780] Trial 9 finished with value: 2.2690346729092252 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 7 with value: 2.239621846886659.\n",
            "[I 2025-10-07 09:00:05,560] Trial 10 finished with value: 2.3919212273487895 and parameters: {'n_estimators': 295, 'max_depth': 7, 'learning_rate': 0.023413669026819843, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8200442512337782}. Best is trial 7 with value: 2.239621846886659.\n",
            "[I 2025-10-07 09:00:08,336] Trial 11 finished with value: 2.4064405020633095 and parameters: {'n_estimators': 268, 'max_depth': 6, 'learning_rate': 0.02793917800027492, 'subsample': 0.6673975881116181, 'colsample_bytree': 0.7311187398797386}. Best is trial 7 with value: 2.239621846886659.\n",
            "[I 2025-10-07 09:00:10,383] Trial 12 finished with value: 2.2731884097970587 and parameters: {'n_estimators': 178, 'max_depth': 5, 'learning_rate': 0.02378376643079244, 'subsample': 0.9106760230554231, 'colsample_bytree': 0.7363570033289744}. Best is trial 7 with value: 2.239621846886659.\n",
            "[I 2025-10-07 09:00:12,138] Trial 13 finished with value: 2.1395516076795857 and parameters: {'n_estimators': 239, 'max_depth': 2, 'learning_rate': 0.017338764664337526, 'subsample': 0.8738214287354175, 'colsample_bytree': 0.8229282203516206}. Best is trial 13 with value: 2.1395516076795857.\n",
            "[I 2025-10-07 09:00:13,699] Trial 14 finished with value: 2.187296889354379 and parameters: {'n_estimators': 248, 'max_depth': 2, 'learning_rate': 0.014125222884129636, 'subsample': 0.8767016211809998, 'colsample_bytree': 0.8505812976968798}. Best is trial 13 with value: 2.1395516076795857.\n",
            "[I 2025-10-07 09:00:15,012] Trial 15 finished with value: 2.0634617460379028 and parameters: {'n_estimators': 298, 'max_depth': 2, 'learning_rate': 0.01653328692799589, 'subsample': 0.9234980616943328, 'colsample_bytree': 0.8714127232837274}. Best is trial 15 with value: 2.0634617460379028.\n",
            "[I 2025-10-07 09:00:16,431] Trial 16 finished with value: 2.1247100605499347 and parameters: {'n_estimators': 300, 'max_depth': 2, 'learning_rate': 0.034760669643952545, 'subsample': 0.9974520669192535, 'colsample_bytree': 0.9355732454459806}. Best is trial 15 with value: 2.0634617460379028.\n",
            "[I 2025-10-07 09:00:20,830] Trial 17 finished with value: 2.375116989544936 and parameters: {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.03809179361482492, 'subsample': 0.9975592275711639, 'colsample_bytree': 0.9532809340971593}. Best is trial 15 with value: 2.0634617460379028.\n",
            "[I 2025-10-07 09:00:22,984] Trial 18 finished with value: 2.401052175014922 and parameters: {'n_estimators': 142, 'max_depth': 8, 'learning_rate': 0.08595489414339409, 'subsample': 0.9328926313279808, 'colsample_bytree': 0.910236126593346}. Best is trial 15 with value: 2.0634617460379028.\n",
            "[I 2025-10-07 09:00:27,880] Trial 19 finished with value: 2.307923084986028 and parameters: {'n_estimators': 279, 'max_depth': 5, 'learning_rate': 0.0385309230060162, 'subsample': 0.9058547676252653, 'colsample_bytree': 0.9100063793746583}. Best is trial 15 with value: 2.0634617460379028.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 26: {'n_estimators': 298, 'max_depth': 2, 'learning_rate': 0.01653328692799589, 'subsample': 0.9234980616943328, 'colsample_bytree': 0.8714127232837274}\n",
            "\n",
            "--- LOOCV Fold 27/31 ---\n",
            "Fold 27 Epoch 1 Loss: 0.9343\n",
            "Fold 27 Epoch 2 Loss: 0.9175\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 09:00:35,748] A new study created in memory with name: no-name-f09b0be0-ea0d-413e-aa03-7b8e0976e35e\n",
            "[I 2025-10-07 09:00:37,356] Trial 0 finished with value: 2.5558655650418998 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:38,022] Trial 1 finished with value: 2.651746026471709 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:38,861] Trial 2 finished with value: 2.684755212918787 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:40,076] Trial 3 finished with value: 2.793423238293629 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:41,332] Trial 4 finished with value: 2.8645772189710494 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:42,462] Trial 5 finished with value: 2.592158917858694 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:44,071] Trial 6 finished with value: 3.0868325639715666 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:46,117] Trial 7 finished with value: 2.8836220578227802 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:46,871] Trial 8 finished with value: 2.7763931653623937 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:48,350] Trial 9 finished with value: 2.7937186056673275 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:49,354] Trial 10 finished with value: 2.6594895198129946 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.10672112706919769, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5076838686640521}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:51,653] Trial 11 finished with value: 2.564850701698474 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.08757126827307511, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.5020730763700768}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:54,108] Trial 12 finished with value: 2.6133424129722203 and parameters: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.10081853902880024, 'subsample': 0.8676850494543448, 'colsample_bytree': 0.5082283557871199}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:55,040] Trial 13 finished with value: 2.7900549995192763 and parameters: {'n_estimators': 147, 'max_depth': 7, 'learning_rate': 0.29486920256900473, 'subsample': 0.8936108460389663, 'colsample_bytree': 0.8229282203516206}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:56,512] Trial 14 finished with value: 2.8164423768398876 and parameters: {'n_estimators': 141, 'max_depth': 6, 'learning_rate': 0.10064440408188902, 'subsample': 0.9085121811914739, 'colsample_bytree': 0.6738167152482913}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:00:59,139] Trial 15 finished with value: 2.8122657763476395 and parameters: {'n_estimators': 298, 'max_depth': 8, 'learning_rate': 0.03208899494279826, 'subsample': 0.6274745095999714, 'colsample_bytree': 0.5758436719988205}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:01:00,690] Trial 16 finished with value: 2.954983046689051 and parameters: {'n_estimators': 159, 'max_depth': 9, 'learning_rate': 0.1597843901181593, 'subsample': 0.8352899108914831, 'colsample_bytree': 0.8335124824629957}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:01:02,281] Trial 17 finished with value: 2.6925692891747497 and parameters: {'n_estimators': 179, 'max_depth': 6, 'learning_rate': 0.034827497346874084, 'subsample': 0.731057379200564, 'colsample_bytree': 0.5646659115326091}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:01:03,535] Trial 18 finished with value: 2.8584797879713157 and parameters: {'n_estimators': 122, 'max_depth': 5, 'learning_rate': 0.08867463581022517, 'subsample': 0.9281326369763928, 'colsample_bytree': 0.671071029561349}. Best is trial 0 with value: 2.5558655650418998.\n",
            "[I 2025-10-07 09:01:05,743] Trial 19 finished with value: 2.630122305519242 and parameters: {'n_estimators': 264, 'max_depth': 8, 'learning_rate': 0.2702272241241035, 'subsample': 0.6693712831714181, 'colsample_bytree': 0.7624863444539982}. Best is trial 0 with value: 2.5558655650418998.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 27: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}\n",
            "\n",
            "--- LOOCV Fold 28/31 ---\n",
            "Fold 28 Epoch 1 Loss: 0.9592\n",
            "Fold 28 Epoch 2 Loss: 0.9261\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 09:01:14,277] A new study created in memory with name: no-name-a6808e45-1961-41ec-a30f-c03f17182e4e\n",
            "[I 2025-10-07 09:01:15,669] Trial 0 finished with value: 2.674931362918586 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.674931362918586.\n",
            "[I 2025-10-07 09:01:16,077] Trial 1 finished with value: 2.8122035338942024 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.674931362918586.\n",
            "[I 2025-10-07 09:01:16,581] Trial 2 finished with value: 2.654273199516597 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.654273199516597.\n",
            "[I 2025-10-07 09:01:17,366] Trial 3 finished with value: 2.556134802693397 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.556134802693397.\n",
            "[I 2025-10-07 09:01:19,425] Trial 4 finished with value: 2.6293894706593504 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 2.556134802693397.\n",
            "[I 2025-10-07 09:01:21,017] Trial 5 finished with value: 2.5434330861348893 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 2.5434330861348893.\n",
            "[I 2025-10-07 09:01:22,596] Trial 6 finished with value: 2.949225977817253 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 2.5434330861348893.\n",
            "[I 2025-10-07 09:01:24,711] Trial 7 finished with value: 2.6097986245713294 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 5 with value: 2.5434330861348893.\n",
            "[I 2025-10-07 09:01:25,430] Trial 8 finished with value: 2.633466547648463 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 5 with value: 2.5434330861348893.\n",
            "[I 2025-10-07 09:01:26,937] Trial 9 finished with value: 2.6123528589540994 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 5 with value: 2.5434330861348893.\n",
            "[I 2025-10-07 09:01:28,941] Trial 10 finished with value: 2.5084635252077994 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.02964033652082882, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5072567334400258}. Best is trial 10 with value: 2.5084635252077994.\n",
            "[I 2025-10-07 09:01:31,094] Trial 11 finished with value: 2.4998409478024377 and parameters: {'n_estimators': 284, 'max_depth': 7, 'learning_rate': 0.030362637356519326, 'subsample': 0.5427925342039944, 'colsample_bytree': 0.5018633118857413}. Best is trial 11 with value: 2.4998409478024377.\n",
            "[I 2025-10-07 09:01:34,361] Trial 12 finished with value: 2.558616581699868 and parameters: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.026438783878681333, 'subsample': 0.5168705837603152, 'colsample_bytree': 0.5082177134093606}. Best is trial 11 with value: 2.4998409478024377.\n",
            "[I 2025-10-07 09:01:37,092] Trial 13 finished with value: 2.673988492155979 and parameters: {'n_estimators': 292, 'max_depth': 7, 'learning_rate': 0.027933677427691955, 'subsample': 0.5021577654211222, 'colsample_bytree': 0.8229282203516206}. Best is trial 11 with value: 2.4998409478024377.\n",
            "[I 2025-10-07 09:01:39,056] Trial 14 finished with value: 2.452763569243887 and parameters: {'n_estimators': 260, 'max_depth': 8, 'learning_rate': 0.03370568122494104, 'subsample': 0.6015036159824763, 'colsample_bytree': 0.5072526255450793}. Best is trial 14 with value: 2.452763569243887.\n",
            "[I 2025-10-07 09:01:41,298] Trial 15 finished with value: 2.6992101498145806 and parameters: {'n_estimators': 249, 'max_depth': 8, 'learning_rate': 0.09894162917596783, 'subsample': 0.6075647002138126, 'colsample_bytree': 0.6545279268509865}. Best is trial 14 with value: 2.452763569243887.\n",
            "[I 2025-10-07 09:01:42,899] Trial 16 finished with value: 2.7254215203200447 and parameters: {'n_estimators': 152, 'max_depth': 6, 'learning_rate': 0.018023794395708836, 'subsample': 0.594659837816319, 'colsample_bytree': 0.9828924556573516}. Best is trial 14 with value: 2.452763569243887.\n",
            "[I 2025-10-07 09:01:47,174] Trial 17 finished with value: 2.6501547343708545 and parameters: {'n_estimators': 267, 'max_depth': 9, 'learning_rate': 0.04210510098972302, 'subsample': 0.6601954070763639, 'colsample_bytree': 0.7838193728974229}. Best is trial 14 with value: 2.452763569243887.\n",
            "[I 2025-10-07 09:01:49,844] Trial 18 finished with value: 2.6883083790316844 and parameters: {'n_estimators': 217, 'max_depth': 7, 'learning_rate': 0.04104975015072629, 'subsample': 0.5638276900241361, 'colsample_bytree': 0.910236126593346}. Best is trial 14 with value: 2.452763569243887.\n",
            "[I 2025-10-07 09:01:52,232] Trial 19 finished with value: 2.6069912016185937 and parameters: {'n_estimators': 269, 'max_depth': 5, 'learning_rate': 0.01859973641941343, 'subsample': 0.6682159968593403, 'colsample_bytree': 0.6896470331233833}. Best is trial 14 with value: 2.452763569243887.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 28: {'n_estimators': 260, 'max_depth': 8, 'learning_rate': 0.03370568122494104, 'subsample': 0.6015036159824763, 'colsample_bytree': 0.5072526255450793}\n",
            "\n",
            "--- LOOCV Fold 29/31 ---\n",
            "Fold 29 Epoch 1 Loss: 0.9027\n",
            "Fold 29 Epoch 2 Loss: 0.9132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 09:02:03,446] A new study created in memory with name: no-name-f86cfbf9-4a5b-4ec3-9ff1-d06af7ed8cdd\n",
            "[I 2025-10-07 09:02:04,814] Trial 0 finished with value: 2.323000441541385 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.323000441541385.\n",
            "[I 2025-10-07 09:02:05,217] Trial 1 finished with value: 2.2027894991533765 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.2027894991533765.\n",
            "[I 2025-10-07 09:02:05,704] Trial 2 finished with value: 2.076074902973279 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.076074902973279.\n",
            "[I 2025-10-07 09:02:06,446] Trial 3 finished with value: 2.0029742709853595 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.0029742709853595.\n",
            "[I 2025-10-07 09:02:07,688] Trial 4 finished with value: 1.9546099283539966 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:08,818] Trial 5 finished with value: 2.065021859283199 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:10,414] Trial 6 finished with value: 2.772035656582441 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:12,485] Trial 7 finished with value: 2.267089575089143 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:13,583] Trial 8 finished with value: 2.661910157686584 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:16,053] Trial 9 finished with value: 2.1659743765602544 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:18,682] Trial 10 finished with value: 2.080243005983828 and parameters: {'n_estimators': 289, 'max_depth': 7, 'learning_rate': 0.023969116039403743, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8200442512337782}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:19,958] Trial 11 finished with value: 1.9573114378717975 and parameters: {'n_estimators': 148, 'max_depth': 6, 'learning_rate': 0.030241001410559513, 'subsample': 0.6658536276465391, 'colsample_bytree': 0.6898021217101117}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:21,270] Trial 12 finished with value: 1.9872569363419532 and parameters: {'n_estimators': 147, 'max_depth': 8, 'learning_rate': 0.026438783878681333, 'subsample': 0.6461192327509931, 'colsample_bytree': 0.7363570033289744}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:22,859] Trial 13 finished with value: 1.9707195609941022 and parameters: {'n_estimators': 157, 'max_depth': 6, 'learning_rate': 0.029105598116745866, 'subsample': 0.6707407277614729, 'colsample_bytree': 0.8229282203516206}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:24,394] Trial 14 finished with value: 2.0667302455371845 and parameters: {'n_estimators': 189, 'max_depth': 8, 'learning_rate': 0.03142197199990161, 'subsample': 0.5490728968753553, 'colsample_bytree': 0.6842432263357744}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:25,706] Trial 15 finished with value: 2.1992669930427593 and parameters: {'n_estimators': 133, 'max_depth': 5, 'learning_rate': 0.09894162917596783, 'subsample': 0.5937663157166837, 'colsample_bytree': 0.7951421650899351}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:27,296] Trial 16 finished with value: 2.0807683182963337 and parameters: {'n_estimators': 233, 'max_depth': 2, 'learning_rate': 0.019613277466224713, 'subsample': 0.7051687523609489, 'colsample_bytree': 0.9047894591765573}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:30,304] Trial 17 finished with value: 2.3206562430711997 and parameters: {'n_estimators': 173, 'max_depth': 8, 'learning_rate': 0.039147136505991394, 'subsample': 0.9040330172235821, 'colsample_bytree': 0.759099737178248}. Best is trial 4 with value: 1.9546099283539966.\n",
            "[I 2025-10-07 09:02:33,081] Trial 18 finished with value: 1.9358056685374394 and parameters: {'n_estimators': 296, 'max_depth': 7, 'learning_rate': 0.04104975015072629, 'subsample': 0.6396280552677315, 'colsample_bytree': 0.6604250893099085}. Best is trial 18 with value: 1.9358056685374394.\n",
            "[I 2025-10-07 09:02:35,550] Trial 19 finished with value: 2.0919777749979263 and parameters: {'n_estimators': 293, 'max_depth': 9, 'learning_rate': 0.04388356984488711, 'subsample': 0.7242275368080187, 'colsample_bytree': 0.5151045586314384}. Best is trial 18 with value: 1.9358056685374394.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 29: {'n_estimators': 296, 'max_depth': 7, 'learning_rate': 0.04104975015072629, 'subsample': 0.6396280552677315, 'colsample_bytree': 0.6604250893099085}\n",
            "\n",
            "--- LOOCV Fold 30/31 ---\n",
            "Fold 30 Epoch 1 Loss: 0.9709\n",
            "Fold 30 Epoch 2 Loss: 0.8871\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 09:02:45,440] A new study created in memory with name: no-name-e4bda934-7323-4c20-93a3-f009c74a8759\n",
            "[I 2025-10-07 09:02:46,877] Trial 0 finished with value: 2.46457677449506 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.46457677449506.\n",
            "[I 2025-10-07 09:02:47,276] Trial 1 finished with value: 2.6941577949359834 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.46457677449506.\n",
            "[I 2025-10-07 09:02:47,794] Trial 2 finished with value: 2.9875961556449733 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.46457677449506.\n",
            "[I 2025-10-07 09:02:48,537] Trial 3 finished with value: 2.5585587589407335 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 2.46457677449506.\n",
            "[I 2025-10-07 09:02:49,749] Trial 4 finished with value: 2.6325700926668354 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 2.46457677449506.\n",
            "[I 2025-10-07 09:02:50,908] Trial 5 finished with value: 2.4547738302804722 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:02:52,533] Trial 6 finished with value: 2.9595069443318587 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:02:55,203] Trial 7 finished with value: 2.7879953018778085 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:02:56,386] Trial 8 finished with value: 2.9175788796223365 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:02:58,154] Trial 9 finished with value: 2.637382180748275 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:03:00,153] Trial 10 finished with value: 2.6711326028432896 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.02964033652082882, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5072567334400258}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:03:01,432] Trial 11 finished with value: 2.7461159774324413 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.11285320502212905, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.5020730763700768}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:03:02,763] Trial 12 finished with value: 2.775508890589794 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.10081853902880024, 'subsample': 0.8806736357247846, 'colsample_bytree': 0.5380251134894303}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:03:03,913] Trial 13 finished with value: 2.458515192700898 and parameters: {'n_estimators': 159, 'max_depth': 6, 'learning_rate': 0.2882069963494139, 'subsample': 0.6376629900391765, 'colsample_bytree': 0.8229282203516206}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:03:05,218] Trial 14 finished with value: 2.6996991578674097 and parameters: {'n_estimators': 258, 'max_depth': 6, 'learning_rate': 0.29478408729031286, 'subsample': 0.6372424866412404, 'colsample_bytree': 0.8350504632309295}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:03:06,962] Trial 15 finished with value: 2.702260288271703 and parameters: {'n_estimators': 171, 'max_depth': 8, 'learning_rate': 0.03208899494279826, 'subsample': 0.5526789661035063, 'colsample_bytree': 0.8304278626461432}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:03:11,234] Trial 16 finished with value: 2.6323884243957836 and parameters: {'n_estimators': 250, 'max_depth': 5, 'learning_rate': 0.0788182723865694, 'subsample': 0.6695070722352982, 'colsample_bytree': 0.9219900508555974}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:03:12,002] Trial 17 finished with value: 3.1257182379182464 and parameters: {'n_estimators': 295, 'max_depth': 2, 'learning_rate': 0.26663748121620445, 'subsample': 0.731057379200564, 'colsample_bytree': 0.7794028774114414}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:03:13,484] Trial 18 finished with value: 2.7640422228132873 and parameters: {'n_estimators': 174, 'max_depth': 8, 'learning_rate': 0.02001514763574974, 'subsample': 0.582778140692689, 'colsample_bytree': 0.6850752557404476}. Best is trial 5 with value: 2.4547738302804722.\n",
            "[I 2025-10-07 09:03:15,844] Trial 19 finished with value: 2.5800860784026085 and parameters: {'n_estimators': 228, 'max_depth': 5, 'learning_rate': 0.04158533584804761, 'subsample': 0.6869888528707923, 'colsample_bytree': 0.7860760029137474}. Best is trial 5 with value: 2.4547738302804722.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 30: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}\n",
            "\n",
            "--- LOOCV Fold 31/31 ---\n",
            "Fold 31 Epoch 1 Loss: 0.9191\n",
            "Fold 31 Epoch 2 Loss: 0.9215\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 09:03:25,169] A new study created in memory with name: no-name-fd766291-6b9b-42ab-b55f-c7520f80ec64\n",
            "[I 2025-10-07 09:03:26,524] Trial 0 finished with value: 2.6446328914417254 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.6446328914417254.\n",
            "[I 2025-10-07 09:03:26,923] Trial 1 finished with value: 2.542174061798968 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.542174061798968.\n",
            "[I 2025-10-07 09:03:27,449] Trial 2 finished with value: 2.7041695111418855 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 2.542174061798968.\n",
            "[I 2025-10-07 09:03:28,227] Trial 3 finished with value: 2.6727669899848174 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 2.542174061798968.\n",
            "[I 2025-10-07 09:03:29,425] Trial 4 finished with value: 2.8120097418781436 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 2.542174061798968.\n",
            "[I 2025-10-07 09:03:30,540] Trial 5 finished with value: 2.664158920114834 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 2.542174061798968.\n",
            "[I 2025-10-07 09:03:32,152] Trial 6 finished with value: 3.5519802674619187 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 1 with value: 2.542174061798968.\n",
            "[I 2025-10-07 09:03:34,182] Trial 7 finished with value: 3.140956377757835 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 1 with value: 2.542174061798968.\n",
            "[I 2025-10-07 09:03:35,082] Trial 8 finished with value: 3.1331783462950353 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 1 with value: 2.542174061798968.\n",
            "[I 2025-10-07 09:03:37,468] Trial 9 finished with value: 2.7080169014926305 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 1 with value: 2.542174061798968.\n",
            "[I 2025-10-07 09:03:38,999] Trial 10 finished with value: 2.4832790321058553 and parameters: {'n_estimators': 139, 'max_depth': 7, 'learning_rate': 0.27047297227177763, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9168158328299749}. Best is trial 10 with value: 2.4832790321058553.\n",
            "[I 2025-10-07 09:03:40,274] Trial 11 finished with value: 2.322064425076671 and parameters: {'n_estimators': 136, 'max_depth': 7, 'learning_rate': 0.28009047436880896, 'subsample': 0.5427925342039944, 'colsample_bytree': 0.8822343758608416}. Best is trial 11 with value: 2.322064425076671.\n",
            "[I 2025-10-07 09:03:41,590] Trial 12 finished with value: 2.4764738834106295 and parameters: {'n_estimators': 138, 'max_depth': 8, 'learning_rate': 0.27422797079747635, 'subsample': 0.5168705837603152, 'colsample_bytree': 0.8995879107630846}. Best is trial 11 with value: 2.322064425076671.\n",
            "[I 2025-10-07 09:03:42,776] Trial 13 finished with value: 2.8943317778494406 and parameters: {'n_estimators': 149, 'max_depth': 8, 'learning_rate': 0.2981013621711842, 'subsample': 0.503911956510792, 'colsample_bytree': 0.8283254019450633}. Best is trial 11 with value: 2.322064425076671.\n",
            "[I 2025-10-07 09:03:44,177] Trial 14 finished with value: 2.532252388594193 and parameters: {'n_estimators': 124, 'max_depth': 8, 'learning_rate': 0.10435488871673998, 'subsample': 0.5898911475235388, 'colsample_bytree': 0.8138214726279749}. Best is trial 11 with value: 2.322064425076671.\n",
            "[I 2025-10-07 09:03:47,386] Trial 15 finished with value: 2.678014414583779 and parameters: {'n_estimators': 292, 'max_depth': 6, 'learning_rate': 0.10814777637672683, 'subsample': 0.5937663157166836, 'colsample_bytree': 0.9947460358092146}. Best is trial 11 with value: 2.322064425076671.\n",
            "[I 2025-10-07 09:03:49,800] Trial 16 finished with value: 2.444841567588982 and parameters: {'n_estimators': 174, 'max_depth': 8, 'learning_rate': 0.18957628748432268, 'subsample': 0.6468683889346044, 'colsample_bytree': 0.9047894591765572}. Best is trial 11 with value: 2.322064425076671.\n",
            "[I 2025-10-07 09:03:52,258] Trial 17 finished with value: 2.332819114737064 and parameters: {'n_estimators': 172, 'max_depth': 9, 'learning_rate': 0.17671030684230263, 'subsample': 0.6601954070763639, 'colsample_bytree': 0.9240177029489687}. Best is trial 11 with value: 2.322064425076671.\n",
            "[I 2025-10-07 09:03:53,971] Trial 18 finished with value: 2.7346121807228645 and parameters: {'n_estimators': 179, 'max_depth': 9, 'learning_rate': 0.03506564903637163, 'subsample': 0.5638276900241361, 'colsample_bytree': 0.7871579224782475}. Best is trial 11 with value: 2.322064425076671.\n",
            "[I 2025-10-07 09:03:55,445] Trial 19 finished with value: 2.4581174552418856 and parameters: {'n_estimators': 110, 'max_depth': 7, 'learning_rate': 0.13726220264427577, 'subsample': 0.6630517979763797, 'colsample_bytree': 0.9507561466348154}. Best is trial 11 with value: 2.322064425076671.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 31: {'n_estimators': 136, 'max_depth': 7, 'learning_rate': 0.28009047436880896, 'subsample': 0.5427925342039944, 'colsample_bytree': 0.8822343758608416}\n",
            "\n",
            "=== LOOCV Evaluation ===\n",
            "RMSE: 2.9816 | MAE: 1.7792\n",
            "Loaded RMSE: 2.981595647515881\n",
            "Loaded MAE: 1.7792028188705444\n"
          ]
        }
      ],
      "source": [
        "loaded_results = run_ssl_pipeline_loocv(\n",
        "    labeled_df=df,\n",
        "    load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_combined\"\n",
        ")\n",
        "\n",
        "print(\"Loaded RMSE:\", loaded_results[\"rmse\"])\n",
        "print(\"Loaded MAE:\", loaded_results[\"mae\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18vdyaA_6hvW"
      },
      "source": [
        "### Metrics\n",
        "\n",
        "=== LOOCV Evaluation ===\n",
        "RMSE: 2.9816 | MAE: 1.7792\n",
        "Loaded RMSE: 2.981595647515881\n",
        "Loaded MAE: 1.7792028188705444\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWdLnP3a7-Yp"
      },
      "source": [
        "#### K Folder Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU91eERm7-LB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def run_ssl_pipeline_kfold_with_r2(labeled_df,\n",
        "                                   unlabelled_dir=None,\n",
        "                                   ssl_epochs=20,\n",
        "                                   ssl_batch=8,\n",
        "                                   fine_tune_backbone=True,\n",
        "                                   use_metadata=False,\n",
        "                                   optuna_trials=20,\n",
        "                                   run_base_dir=\"models\",\n",
        "                                   load_run_dir=None,\n",
        "                                   n_splits=5):\n",
        "\n",
        "    run_dir = load_run_dir if load_run_dir else make_run_dir(run_base_dir)\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    print(f\"Run directory: {run_dir}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Load or pretrain SSL backbone\n",
        "    # -------------------\n",
        "    backbone = get_ssl_backbone(pretrained=False)\n",
        "    proj_head = ProjectionHead().to(device)\n",
        "\n",
        "    if load_run_dir and os.path.exists(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\")):\n",
        "        backbone.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"), map_location=device))\n",
        "        proj_head.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"), map_location=device))\n",
        "        backbone.to(device).eval()\n",
        "        proj_head.to(device).eval()\n",
        "        print(\"Loaded pretrained backbone and projection head from saved run.\")\n",
        "    else:\n",
        "        print(\"=== SSL Pretraining ===\")\n",
        "        if unlabelled_dir:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(unlabelled_dir, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=True)\n",
        "        else:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(labeled_df, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=False)\n",
        "\n",
        "    # -------------------\n",
        "    # K-Fold CV\n",
        "    # -------------------\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(labeled_df)):\n",
        "        print(f\"\\n--- K-Fold {fold_idx+1}/{n_splits} ---\")\n",
        "        train_df, test_df = labeled_df.iloc[train_idx], labeled_df.iloc[test_idx]\n",
        "\n",
        "        # Fine-tune backbone if required\n",
        "        if fine_tune_backbone:\n",
        "            dataset = HbImageDataset(train_df, transform=ssl_transform, n_views=2)\n",
        "            loader = DataLoader(dataset, batch_size=ssl_batch, shuffle=True, num_workers=2, pin_memory=True)\n",
        "            backbone.train()\n",
        "            proj_head.train()\n",
        "            optimizer = torch.optim.Adam(list(backbone.parameters()) + list(proj_head.parameters()), lr=1e-4)\n",
        "            for epoch in range(10):\n",
        "                total_loss = 0.0\n",
        "                for views, _ in loader:\n",
        "                    v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "                    feats1, feats2 = backbone(v1).view(v1.size(0), -1), backbone(v2).view(v2.size(0), -1)\n",
        "                    z1, z2 = proj_head(feats1), proj_head(feats2)\n",
        "                    loss = nt_xent_loss(z1, z2)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "                print(f\"Fold {fold_idx+1} Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "        # Extract embeddings\n",
        "        X_train, y_train = extract_embeddings(train_df, backbone, val_transform)\n",
        "        X_test, y_test = extract_embeddings(test_df, backbone, val_transform)\n",
        "\n",
        "        if use_metadata:\n",
        "            X_train = combine_metadata(X_train, train_df)\n",
        "            X_test = combine_metadata(X_test, test_df)\n",
        "\n",
        "        # Optuna hyperparameter tuning\n",
        "        def objective(trial):\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\",50,300),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\",2,10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\",0.01,0.3,log=True),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\",0.5,1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.5,1.0),\n",
        "                \"random_state\": RANDOM_SEED,\n",
        "                \"verbosity\": 0,\n",
        "                \"n_jobs\": 1,\n",
        "                \"tree_method\": \"hist\"\n",
        "            }\n",
        "            kf_inner = KFold(n_splits=min(5, len(train_df)), shuffle=True, random_state=RANDOM_SEED)\n",
        "            maes = []\n",
        "            for tr_idx, val_idx in kf_inner.split(X_train):\n",
        "                model = xgb.XGBRegressor(**params)\n",
        "                model.fit(X_train[tr_idx], y_train[tr_idx])\n",
        "                y_pred = model.predict(X_train[val_idx])\n",
        "                maes.append(mean_absolute_error(y_train[val_idx], y_pred))\n",
        "            return np.mean(maes)\n",
        "\n",
        "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "        study.optimize(objective, n_trials=optuna_trials, show_progress_bar=False)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best params fold {fold_idx+1}: {best_params}\")\n",
        "\n",
        "        # Train final model\n",
        "        final_model = xgb.XGBRegressor(**best_params, random_state=RANDOM_SEED, n_jobs=-1, tree_method=\"hist\")\n",
        "        final_model.fit(X_train, y_train)\n",
        "        y_pred = final_model.predict(X_test)\n",
        "\n",
        "        # Compute metrics including R\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        print(f\"[Fold {fold_idx+1}] RMSE: {rmse:.4f}, MAE: {mae:.4f}, R: {r2:.4f}\")\n",
        "\n",
        "        fold_metrics.append({\"fold\": fold_idx+1, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n",
        "\n",
        "    # -------------------\n",
        "    # Summary metrics\n",
        "    # -------------------\n",
        "    metrics_df = pd.DataFrame(fold_metrics)\n",
        "    metrics_df.loc[\"Mean\"] = metrics_df.mean()\n",
        "    print(\"\\n=== Fold Metrics Summary ===\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    # Save metrics and models\n",
        "    metrics_df.to_csv(os.path.join(run_dir, \"kfold_metrics.csv\"), index=False)\n",
        "    torch.save(backbone.state_dict(), os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"))\n",
        "    torch.save(proj_head.state_dict(), os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"))\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(metrics_df['fold'][:-1], metrics_df['RMSE'][:-1], marker='o', label='RMSE')\n",
        "    plt.plot(metrics_df['fold'][:-1], metrics_df['MAE'][:-1], marker='s', label='MAE')\n",
        "    plt.plot(metrics_df['fold'][:-1], metrics_df['R2'][:-1], marker='^', label='R')\n",
        "    plt.axhline(metrics_df.loc[\"Mean\", 'RMSE'], color='blue', linestyle='--', alpha=0.5)\n",
        "    plt.axhline(metrics_df.loc[\"Mean\", 'MAE'], color='orange', linestyle='--', alpha=0.5)\n",
        "    plt.axhline(metrics_df.loc[\"Mean\", 'R2'], color='green', linestyle='--', alpha=0.5)\n",
        "    plt.xlabel(\"Fold\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.title(\"K-Fold Regression Metrics (RMSE, MAE, R)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(run_dir, \"kfold_metrics_plot.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        \"backbone\": backbone,\n",
        "        \"proj_head\": proj_head,\n",
        "        \"metrics_df\": metrics_df,\n",
        "        \"run_dir\": run_dir\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "collapsed": true,
        "id": "jyNkov1I8HPC",
        "outputId": "9add5cc2-af4a-4e2b-f384-e372bf1fd4e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run directory: /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_combined\n",
            "Loaded pretrained backbone and projection head from saved run.\n",
            "\n",
            "--- K-Fold 1/5 ---\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "HbImageDataset.__init__() got an unexpected keyword argument 'n_views'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1733936693.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loaded_results = run_ssl_pipeline_kfold(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlabeled_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mload_run_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_combined\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3033067333.py\u001b[0m in \u001b[0;36mrun_ssl_pipeline_kfold\u001b[0;34m(labeled_df, unlabelled_dir, ssl_epochs, ssl_batch, fine_tune_backbone, use_metadata, optuna_trials, run_base_dir, load_run_dir, n_splits)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# -------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfine_tune_backbone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHbImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mssl_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_views\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mssl_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: HbImageDataset.__init__() got an unexpected keyword argument 'n_views'"
          ]
        }
      ],
      "source": [
        "loaded_results = run_ssl_pipeline_kfold(\n",
        "    labeled_df=df,\n",
        "    load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_combined\"\n",
        ")\n",
        "\n",
        "print(\"Loaded RMSE:\", loaded_results[\"rmse\"])\n",
        "print(\"Loaded MAE:\", loaded_results[\"mae\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD62fbA49GOI"
      },
      "source": [
        "#### Metrics: MAE 1.9079\n",
        "\n",
        "=== K-Fold Evaluation ===\n",
        "RMSE: 2.7354 | MAE: 1.9079\n",
        "Loaded RMSE: 2.7354124008652914\n",
        "Loaded MAE: 1.9079385995864868"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRPiGTmfCDdY",
        "outputId": "2392a378-6c9f-43e5-988f-c2185539fa0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run directory: /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\n",
            "Loaded pretrained backbone and projection head from saved run.\n",
            "\n",
            "--- K-Fold 1/5 ---\n",
            "Fold 1 Epoch 1 Loss: 1.0188\n",
            "Fold 1 Epoch 2 Loss: 1.0468\n",
            "Fold 1 Epoch 3 Loss: 1.0131\n",
            "Fold 1 Epoch 4 Loss: 0.9837\n",
            "Fold 1 Epoch 5 Loss: 1.0434\n",
            "Fold 1 Epoch 6 Loss: 0.9461\n",
            "Fold 1 Epoch 7 Loss: 1.0483\n",
            "Fold 1 Epoch 8 Loss: 1.0291\n",
            "Fold 1 Epoch 9 Loss: 0.9727\n",
            "Fold 1 Epoch 10 Loss: 0.9708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 09:48:58,309] A new study created in memory with name: no-name-d9cf9eb4-d63a-4338-888b-14e53cb26c51\n",
            "[I 2025-10-07 09:48:59,393] Trial 0 finished with value: 1.776565706729889 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.776565706729889.\n",
            "[I 2025-10-07 09:48:59,742] Trial 1 finished with value: 2.074971556663513 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.776565706729889.\n",
            "[I 2025-10-07 09:49:00,141] Trial 2 finished with value: 1.875977087020874 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 1.776565706729889.\n",
            "[I 2025-10-07 09:49:01,031] Trial 3 finished with value: 1.8244225144386292 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 1.776565706729889.\n",
            "[I 2025-10-07 09:49:02,709] Trial 4 finished with value: 1.8014218091964722 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 1.776565706729889.\n",
            "[I 2025-10-07 09:49:04,137] Trial 5 finished with value: 1.762640380859375 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 1.762640380859375.\n",
            "[I 2025-10-07 09:49:05,504] Trial 6 finished with value: 1.6879327774047852 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 6 with value: 1.6879327774047852.\n",
            "[I 2025-10-07 09:49:07,242] Trial 7 finished with value: 1.65127831697464 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 7 with value: 1.65127831697464.\n",
            "[I 2025-10-07 09:49:07,866] Trial 8 finished with value: 1.7794917345046997 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 7 with value: 1.65127831697464.\n",
            "[I 2025-10-07 09:49:09,124] Trial 9 finished with value: 1.9002431392669679 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 7 with value: 1.65127831697464.\n",
            "[I 2025-10-07 09:49:11,323] Trial 10 finished with value: 2.029728889465332 and parameters: {'n_estimators': 295, 'max_depth': 7, 'learning_rate': 0.023413669026819843, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8200442512337782}. Best is trial 7 with value: 1.65127831697464.\n",
            "[I 2025-10-07 09:49:14,807] Trial 11 finished with value: 1.7632657051086427 and parameters: {'n_estimators': 268, 'max_depth': 6, 'learning_rate': 0.010244149715931178, 'subsample': 0.9903324566798157, 'colsample_bytree': 0.990842956734007}. Best is trial 7 with value: 1.65127831697464.\n",
            "[I 2025-10-07 09:49:17,901] Trial 12 finished with value: 1.6779598712921142 and parameters: {'n_estimators': 178, 'max_depth': 5, 'learning_rate': 0.01924001519493744, 'subsample': 0.9108740628146075, 'colsample_bytree': 0.9970029045473664}. Best is trial 7 with value: 1.65127831697464.\n",
            "[I 2025-10-07 09:49:19,293] Trial 13 finished with value: 1.630586290359497 and parameters: {'n_estimators': 153, 'max_depth': 5, 'learning_rate': 0.024248476575294508, 'subsample': 0.8897860844614501, 'colsample_bytree': 0.7535821814794282}. Best is trial 13 with value: 1.630586290359497.\n",
            "[I 2025-10-07 09:49:20,669] Trial 14 finished with value: 1.7222598791122437 and parameters: {'n_estimators': 135, 'max_depth': 8, 'learning_rate': 0.03485051954811126, 'subsample': 0.8767016211809998, 'colsample_bytree': 0.7608816703564613}. Best is trial 13 with value: 1.630586290359497.\n",
            "[I 2025-10-07 09:49:21,793] Trial 15 finished with value: 1.6632333755493165 and parameters: {'n_estimators': 137, 'max_depth': 5, 'learning_rate': 0.017189913467487754, 'subsample': 0.8455336024587532, 'colsample_bytree': 0.7263937946209092}. Best is trial 13 with value: 1.630586290359497.\n",
            "[I 2025-10-07 09:49:24,656] Trial 16 finished with value: 1.7654817819595336 and parameters: {'n_estimators': 233, 'max_depth': 8, 'learning_rate': 0.03450821954895383, 'subsample': 0.9001670874972677, 'colsample_bytree': 0.8989974345395086}. Best is trial 13 with value: 1.630586290359497.\n",
            "[I 2025-10-07 09:49:25,842] Trial 17 finished with value: 1.8706122159957885 and parameters: {'n_estimators': 165, 'max_depth': 5, 'learning_rate': 0.015758467636682115, 'subsample': 0.6647152497699358, 'colsample_bytree': 0.6803710106053958}. Best is trial 13 with value: 1.630586290359497.\n",
            "[I 2025-10-07 09:49:26,799] Trial 18 finished with value: 1.8521159410476684 and parameters: {'n_estimators': 297, 'max_depth': 2, 'learning_rate': 0.10012424397696543, 'subsample': 0.8481497780299396, 'colsample_bytree': 0.7902239148546872}. Best is trial 13 with value: 1.630586290359497.\n",
            "[I 2025-10-07 09:49:31,379] Trial 19 finished with value: 1.7719008922576904 and parameters: {'n_estimators': 261, 'max_depth': 7, 'learning_rate': 0.03506184744756866, 'subsample': 0.9077489586353636, 'colsample_bytree': 0.8948340529727098}. Best is trial 13 with value: 1.630586290359497.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 1: {'n_estimators': 153, 'max_depth': 5, 'learning_rate': 0.024248476575294508, 'subsample': 0.8897860844614501, 'colsample_bytree': 0.7535821814794282}\n",
            "\n",
            "--- K-Fold 2/5 ---\n",
            "Fold 2 Epoch 1 Loss: 0.7555\n",
            "Fold 2 Epoch 2 Loss: 0.7269\n",
            "Fold 2 Epoch 3 Loss: 0.7598\n",
            "Fold 2 Epoch 4 Loss: 0.8377\n",
            "Fold 2 Epoch 5 Loss: 0.7641\n",
            "Fold 2 Epoch 6 Loss: 0.7397\n",
            "Fold 2 Epoch 7 Loss: 0.7665\n",
            "Fold 2 Epoch 8 Loss: 0.7106\n",
            "Fold 2 Epoch 9 Loss: 0.8058\n",
            "Fold 2 Epoch 10 Loss: 0.7118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 09:49:58,204] A new study created in memory with name: no-name-af882f47-525a-49f0-837a-104fdbb42e99\n",
            "[I 2025-10-07 09:49:59,819] Trial 0 finished with value: 1.9384898900985719 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.9384898900985719.\n",
            "[I 2025-10-07 09:50:00,198] Trial 1 finished with value: 1.7907150268554688 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.7907150268554688.\n",
            "[I 2025-10-07 09:50:00,659] Trial 2 finished with value: 1.769911003112793 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.769911003112793.\n",
            "[I 2025-10-07 09:50:01,315] Trial 3 finished with value: 1.9445127964019775 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.769911003112793.\n",
            "[I 2025-10-07 09:50:02,386] Trial 4 finished with value: 1.8252233505249023 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.769911003112793.\n",
            "[I 2025-10-07 09:50:03,382] Trial 5 finished with value: 1.9267129182815552 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.769911003112793.\n",
            "[I 2025-10-07 09:50:04,797] Trial 6 finished with value: 1.6994181871414185 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 6 with value: 1.6994181871414185.\n",
            "[I 2025-10-07 09:50:07,126] Trial 7 finished with value: 1.7027616262435914 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 6 with value: 1.6994181871414185.\n",
            "[I 2025-10-07 09:50:07,705] Trial 8 finished with value: 1.9209668636322021 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 6 with value: 1.6994181871414185.\n",
            "[I 2025-10-07 09:50:09,006] Trial 9 finished with value: 1.892789626121521 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 6 with value: 1.6994181871414185.\n",
            "[I 2025-10-07 09:50:14,252] Trial 10 finished with value: 1.6625646829605103 and parameters: {'n_estimators': 289, 'max_depth': 7, 'learning_rate': 0.02367326211211725, 'subsample': 0.9775724145208066, 'colsample_bytree': 0.991948117710163}. Best is trial 10 with value: 1.6625646829605103.\n",
            "[I 2025-10-07 09:50:17,711] Trial 11 finished with value: 1.647692656517029 and parameters: {'n_estimators': 278, 'max_depth': 7, 'learning_rate': 0.022731789983718213, 'subsample': 0.9851129432910448, 'colsample_bytree': 0.9988392431163264}. Best is trial 11 with value: 1.647692656517029.\n",
            "[I 2025-10-07 09:50:21,515] Trial 12 finished with value: 1.5009472727775575 and parameters: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.026438783878681333, 'subsample': 0.9095948184003396, 'colsample_bytree': 0.9970015683128081}. Best is trial 12 with value: 1.5009472727775575.\n",
            "[I 2025-10-07 09:50:25,975] Trial 13 finished with value: 1.5749244451522828 and parameters: {'n_estimators': 293, 'max_depth': 8, 'learning_rate': 0.027675619830887233, 'subsample': 0.893052454632663, 'colsample_bytree': 0.8972372304620013}. Best is trial 12 with value: 1.5009472727775575.\n",
            "[I 2025-10-07 09:50:29,720] Trial 14 finished with value: 1.5300115108489991 and parameters: {'n_estimators': 297, 'max_depth': 8, 'learning_rate': 0.038542057915351174, 'subsample': 0.8885483648944142, 'colsample_bytree': 0.8859620061900356}. Best is trial 12 with value: 1.5009472727775575.\n",
            "[I 2025-10-07 09:50:31,340] Trial 15 finished with value: 1.7692992448806764 and parameters: {'n_estimators': 153, 'max_depth': 9, 'learning_rate': 0.09894162917596783, 'subsample': 0.8704152241935851, 'colsample_bytree': 0.8711100865904403}. Best is trial 12 with value: 1.5009472727775575.\n",
            "[I 2025-10-07 09:50:34,034] Trial 16 finished with value: 1.5487596035003661 and parameters: {'n_estimators': 253, 'max_depth': 6, 'learning_rate': 0.03715372111562421, 'subsample': 0.911675272562542, 'colsample_bytree': 0.8019462153840969}. Best is trial 12 with value: 1.5009472727775575.\n",
            "[I 2025-10-07 09:50:36,811] Trial 17 finished with value: 1.8934078693389893 and parameters: {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.03850253962820081, 'subsample': 0.5694064187762937, 'colsample_bytree': 0.9299637852708278}. Best is trial 12 with value: 1.5009472727775575.\n",
            "[I 2025-10-07 09:50:40,560] Trial 18 finished with value: 1.8219726085662842 and parameters: {'n_estimators': 236, 'max_depth': 8, 'learning_rate': 0.016759587004154426, 'subsample': 0.6471208002214496, 'colsample_bytree': 0.8023966919176156}. Best is trial 12 with value: 1.5009472727775575.\n",
            "[I 2025-10-07 09:50:43,283] Trial 19 finished with value: 1.8962692975997926 and parameters: {'n_estimators': 170, 'max_depth': 9, 'learning_rate': 0.2702272241241035, 'subsample': 0.5054945946218856, 'colsample_bytree': 0.9401278138490896}. Best is trial 12 with value: 1.5009472727775575.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 2: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.026438783878681333, 'subsample': 0.9095948184003396, 'colsample_bytree': 0.9970015683128081}\n",
            "\n",
            "--- K-Fold 3/5 ---\n",
            "Fold 3 Epoch 1 Loss: 0.7727\n",
            "Fold 3 Epoch 2 Loss: 0.7530\n",
            "Fold 3 Epoch 3 Loss: 0.7313\n",
            "Fold 3 Epoch 4 Loss: 0.7757\n",
            "Fold 3 Epoch 5 Loss: 0.7187\n",
            "Fold 3 Epoch 6 Loss: 0.7083\n",
            "Fold 3 Epoch 7 Loss: 0.7242\n",
            "Fold 3 Epoch 8 Loss: 0.7408\n",
            "Fold 3 Epoch 9 Loss: 0.7184\n",
            "Fold 3 Epoch 10 Loss: 0.7431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 09:51:16,280] A new study created in memory with name: no-name-7708bc66-5128-4633-bdf0-88b99c6a9045\n",
            "[I 2025-10-07 09:51:17,636] Trial 0 finished with value: 2.648856997489929 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.648856997489929.\n",
            "[I 2025-10-07 09:51:18,227] Trial 1 finished with value: 2.958119106292725 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.648856997489929.\n",
            "[I 2025-10-07 09:51:18,897] Trial 2 finished with value: 2.78582706451416 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.648856997489929.\n",
            "[I 2025-10-07 09:51:19,998] Trial 3 finished with value: 2.7400585412979126 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 2.648856997489929.\n",
            "[I 2025-10-07 09:51:23,838] Trial 4 finished with value: 2.8135964632034303 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 2.648856997489929.\n",
            "[I 2025-10-07 09:51:25,408] Trial 5 finished with value: 2.6998428106307983 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 2.648856997489929.\n",
            "[I 2025-10-07 09:51:26,812] Trial 6 finished with value: 2.9837947130203246 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 0 with value: 2.648856997489929.\n",
            "[I 2025-10-07 09:51:28,593] Trial 7 finished with value: 2.732273077964783 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 0 with value: 2.648856997489929.\n",
            "[I 2025-10-07 09:51:29,732] Trial 8 finished with value: 2.5645708322525023 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 8 with value: 2.5645708322525023.\n",
            "[I 2025-10-07 09:51:31,490] Trial 9 finished with value: 2.716684889793396 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 8 with value: 2.5645708322525023.\n",
            "[I 2025-10-07 09:51:32,997] Trial 10 finished with value: 2.8956226348876952 and parameters: {'n_estimators': 144, 'max_depth': 7, 'learning_rate': 0.023955920588449954, 'subsample': 0.9774843668999692, 'colsample_bytree': 0.8259332753890892}. Best is trial 8 with value: 2.5645708322525023.\n",
            "[I 2025-10-07 09:51:34,153] Trial 11 finished with value: 2.4224066138267517 and parameters: {'n_estimators': 144, 'max_depth': 7, 'learning_rate': 0.11285320502212905, 'subsample': 0.8918148105745084, 'colsample_bytree': 0.5050500683408422}. Best is trial 11 with value: 2.4224066138267517.\n",
            "[I 2025-10-07 09:51:35,908] Trial 12 finished with value: 2.4807389855384825 and parameters: {'n_estimators': 144, 'max_depth': 7, 'learning_rate': 0.10020899840797187, 'subsample': 0.9153473865115568, 'colsample_bytree': 0.5083816959804007}. Best is trial 11 with value: 2.4224066138267517.\n",
            "[I 2025-10-07 09:51:36,803] Trial 13 finished with value: 2.455449032783508 and parameters: {'n_estimators': 154, 'max_depth': 7, 'learning_rate': 0.295947421254701, 'subsample': 0.8975580199866934, 'colsample_bytree': 0.503283785719303}. Best is trial 11 with value: 2.4224066138267517.\n",
            "[I 2025-10-07 09:51:37,666] Trial 14 finished with value: 2.3735864400863647 and parameters: {'n_estimators': 171, 'max_depth': 8, 'learning_rate': 0.2967813207727778, 'subsample': 0.8883155118091521, 'colsample_bytree': 0.5073169022416317}. Best is trial 14 with value: 2.3735864400863647.\n",
            "[I 2025-10-07 09:51:38,514] Trial 15 finished with value: 2.5424909710884096 and parameters: {'n_estimators': 292, 'max_depth': 9, 'learning_rate': 0.29596284206078843, 'subsample': 0.8593416070738904, 'colsample_bytree': 0.6545175079139021}. Best is trial 14 with value: 2.3735864400863647.\n",
            "[I 2025-10-07 09:51:40,224] Trial 16 finished with value: 2.967742419242859 and parameters: {'n_estimators': 191, 'max_depth': 8, 'learning_rate': 0.10346003517453034, 'subsample': 0.6207774664901698, 'colsample_bytree': 0.8441147559750997}. Best is trial 14 with value: 2.3735864400863647.\n",
            "[I 2025-10-07 09:51:41,282] Trial 17 finished with value: 2.6763867855072023 and parameters: {'n_estimators': 109, 'max_depth': 6, 'learning_rate': 0.19560165165260576, 'subsample': 0.8922228568305098, 'colsample_bytree': 0.9980084946806067}. Best is trial 14 with value: 2.3735864400863647.\n",
            "[I 2025-10-07 09:51:42,325] Trial 18 finished with value: 2.5279454231262206 and parameters: {'n_estimators': 163, 'max_depth': 8, 'learning_rate': 0.14389543309550662, 'subsample': 0.6924839515611932, 'colsample_bytree': 0.5562918749229789}. Best is trial 14 with value: 2.3735864400863647.\n",
            "[I 2025-10-07 09:51:43,122] Trial 19 finished with value: 2.8587636947631836 and parameters: {'n_estimators': 116, 'max_depth': 5, 'learning_rate': 0.08553293763713823, 'subsample': 0.5054945946218856, 'colsample_bytree': 0.6862849409465537}. Best is trial 14 with value: 2.3735864400863647.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 3: {'n_estimators': 171, 'max_depth': 8, 'learning_rate': 0.2967813207727778, 'subsample': 0.8883155118091521, 'colsample_bytree': 0.5073169022416317}\n",
            "\n",
            "--- K-Fold 4/5 ---\n",
            "Fold 4 Epoch 1 Loss: 0.7393\n",
            "Fold 4 Epoch 2 Loss: 0.7962\n",
            "Fold 4 Epoch 3 Loss: 0.7054\n",
            "Fold 4 Epoch 4 Loss: 0.7818\n",
            "Fold 4 Epoch 5 Loss: 0.7749\n",
            "Fold 4 Epoch 6 Loss: 0.7311\n",
            "Fold 4 Epoch 7 Loss: 0.7483\n",
            "Fold 4 Epoch 8 Loss: 0.7564\n",
            "Fold 4 Epoch 9 Loss: 0.7366\n",
            "Fold 4 Epoch 10 Loss: 0.7428\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 09:52:13,676] A new study created in memory with name: no-name-e259ec0a-e387-4ead-b7ce-d508dfe3ed07\n",
            "[I 2025-10-07 09:52:14,998] Trial 0 finished with value: 1.7402315258979797 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:15,731] Trial 1 finished with value: 1.8883888244628906 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:16,452] Trial 2 finished with value: 1.8327743530273437 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:17,449] Trial 3 finished with value: 1.822501301765442 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:19,589] Trial 4 finished with value: 1.7970555543899536 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:20,964] Trial 5 finished with value: 1.7708197951316833 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:22,495] Trial 6 finished with value: 2.012029457092285 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:24,218] Trial 7 finished with value: 1.8660241603851317 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:24,784] Trial 8 finished with value: 1.9630683422088624 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:26,039] Trial 9 finished with value: 1.9068235635757447 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:26,860] Trial 10 finished with value: 1.7977259635925293 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.10672112706919769, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5076838686640521}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:28,177] Trial 11 finished with value: 1.9127590656280518 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.08757126827307511, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.5020730763700768}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:29,114] Trial 12 finished with value: 1.7790935516357422 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.03074474394845574, 'subsample': 0.8806736357247846, 'colsample_bytree': 0.5380251134894303}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:30,483] Trial 13 finished with value: 1.965190303325653 and parameters: {'n_estimators': 159, 'max_depth': 8, 'learning_rate': 0.2882069963494139, 'subsample': 0.6376629900391765, 'colsample_bytree': 0.8229282203516206}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:32,983] Trial 14 finished with value: 1.8561978340148926 and parameters: {'n_estimators': 246, 'max_depth': 8, 'learning_rate': 0.1062272193666865, 'subsample': 0.7480206560848571, 'colsample_bytree': 0.6738167152482913}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:35,067] Trial 15 finished with value: 1.894387125968933 and parameters: {'n_estimators': 298, 'max_depth': 5, 'learning_rate': 0.03376610234655322, 'subsample': 0.9140258921869188, 'colsample_bytree': 0.5778918965965928}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:37,021] Trial 16 finished with value: 1.8778950452804566 and parameters: {'n_estimators': 191, 'max_depth': 9, 'learning_rate': 0.04562044888767241, 'subsample': 0.8104537952868079, 'colsample_bytree': 0.8441147559750997}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:38,126] Trial 17 finished with value: 1.8052446365356445 and parameters: {'n_estimators': 127, 'max_depth': 6, 'learning_rate': 0.07994074546676172, 'subsample': 0.6738168463735492, 'colsample_bytree': 0.7701341133322941}. Best is trial 0 with value: 1.7402315258979797.\n",
            "[I 2025-10-07 09:52:38,725] Trial 18 finished with value: 1.6947714805603027 and parameters: {'n_estimators': 239, 'max_depth': 2, 'learning_rate': 0.1521927957546102, 'subsample': 0.5827781406926889, 'colsample_bytree': 0.5525315087165706}. Best is trial 18 with value: 1.6947714805603027.\n",
            "[I 2025-10-07 09:52:40,155] Trial 19 finished with value: 1.8127273082733155 and parameters: {'n_estimators': 227, 'max_depth': 7, 'learning_rate': 0.14920643617808257, 'subsample': 0.5354310417586021, 'colsample_bytree': 0.675760829066606}. Best is trial 18 with value: 1.6947714805603027.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 4: {'n_estimators': 239, 'max_depth': 2, 'learning_rate': 0.1521927957546102, 'subsample': 0.5827781406926889, 'colsample_bytree': 0.5525315087165706}\n",
            "\n",
            "--- K-Fold 5/5 ---\n",
            "Fold 5 Epoch 1 Loss: 0.8067\n",
            "Fold 5 Epoch 2 Loss: 0.8001\n",
            "Fold 5 Epoch 3 Loss: 0.7219\n",
            "Fold 5 Epoch 4 Loss: 0.7291\n",
            "Fold 5 Epoch 5 Loss: 0.7094\n",
            "Fold 5 Epoch 6 Loss: 0.7640\n",
            "Fold 5 Epoch 7 Loss: 0.7189\n",
            "Fold 5 Epoch 8 Loss: 0.7386\n",
            "Fold 5 Epoch 9 Loss: 0.7584\n",
            "Fold 5 Epoch 10 Loss: 0.7397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 09:53:05,831] A new study created in memory with name: no-name-c6cd733b-e051-46ac-a6b9-efbc5a3dda76\n",
            "[I 2025-10-07 09:53:06,831] Trial 0 finished with value: 1.3315021634101867 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.3315021634101867.\n",
            "[I 2025-10-07 09:53:07,199] Trial 1 finished with value: 1.7060800790786743 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.3315021634101867.\n",
            "[I 2025-10-07 09:53:07,592] Trial 2 finished with value: 1.4735140562057496 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 1.3315021634101867.\n",
            "[I 2025-10-07 09:53:08,196] Trial 3 finished with value: 1.4088106274604797 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 1.3315021634101867.\n",
            "[I 2025-10-07 09:53:09,244] Trial 4 finished with value: 1.4305185198783874 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 1.3315021634101867.\n",
            "[I 2025-10-07 09:53:10,226] Trial 5 finished with value: 1.36066472530365 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 1.3315021634101867.\n",
            "[I 2025-10-07 09:53:12,118] Trial 6 finished with value: 1.7265264987945557 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 0 with value: 1.3315021634101867.\n",
            "[I 2025-10-07 09:53:15,103] Trial 7 finished with value: 1.460090959072113 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 0 with value: 1.3315021634101867.\n",
            "[I 2025-10-07 09:53:15,744] Trial 8 finished with value: 1.7535218477249146 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 0 with value: 1.3315021634101867.\n",
            "[I 2025-10-07 09:53:17,087] Trial 9 finished with value: 1.316898274421692 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 9 with value: 1.316898274421692.\n",
            "[I 2025-10-07 09:53:19,240] Trial 10 finished with value: 1.56051504611969 and parameters: {'n_estimators': 291, 'max_depth': 7, 'learning_rate': 0.03001215871999436, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8259332753890892}. Best is trial 9 with value: 1.316898274421692.\n",
            "[I 2025-10-07 09:53:20,185] Trial 11 finished with value: 1.3839579582214356 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.11285320502212905, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.5037892523442965}. Best is trial 9 with value: 1.316898274421692.\n",
            "[I 2025-10-07 09:53:21,173] Trial 12 finished with value: 1.2980119228363036 and parameters: {'n_estimators': 148, 'max_depth': 7, 'learning_rate': 0.09936605342538195, 'subsample': 0.6585560646388384, 'colsample_bytree': 0.5788506755138203}. Best is trial 12 with value: 1.2980119228363036.\n",
            "[I 2025-10-07 09:53:21,892] Trial 13 finished with value: 1.677946889400482 and parameters: {'n_estimators': 157, 'max_depth': 8, 'learning_rate': 0.29588894479990485, 'subsample': 0.6334955364726846, 'colsample_bytree': 0.6781276119443599}. Best is trial 12 with value: 1.2980119228363036.\n",
            "[I 2025-10-07 09:53:23,373] Trial 14 finished with value: 1.4768369674682618 and parameters: {'n_estimators': 189, 'max_depth': 5, 'learning_rate': 0.07937357644840533, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.8138214726279749}. Best is trial 12 with value: 1.2980119228363036.\n",
            "[I 2025-10-07 09:53:25,419] Trial 15 finished with value: 1.4422671318054199 and parameters: {'n_estimators': 239, 'max_depth': 8, 'learning_rate': 0.033439619925994035, 'subsample': 0.6587620735727939, 'colsample_bytree': 0.5777944591365302}. Best is trial 12 with value: 1.2980119228363036.\n",
            "[I 2025-10-07 09:53:26,788] Trial 16 finished with value: 1.327511441707611 and parameters: {'n_estimators': 126, 'max_depth': 6, 'learning_rate': 0.04185444933164421, 'subsample': 0.7452057396632572, 'colsample_bytree': 0.5527985007094495}. Best is trial 12 with value: 1.2980119228363036.\n",
            "[I 2025-10-07 09:53:28,446] Trial 17 finished with value: 1.5631909132003785 and parameters: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.02068086816741835, 'subsample': 0.5926298696920421, 'colsample_bytree': 0.676697874250107}. Best is trial 12 with value: 1.2980119228363036.\n",
            "[I 2025-10-07 09:53:30,499] Trial 18 finished with value: 1.5777446627616882 and parameters: {'n_estimators': 296, 'max_depth': 5, 'learning_rate': 0.08357671924952854, 'subsample': 0.8949828442656664, 'colsample_bytree': 0.910236126593346}. Best is trial 12 with value: 1.2980119228363036.\n",
            "[I 2025-10-07 09:53:32,055] Trial 19 finished with value: 1.535011112689972 and parameters: {'n_estimators': 175, 'max_depth': 7, 'learning_rate': 0.0868562784612828, 'subsample': 0.7275134533930888, 'colsample_bytree': 0.7714380910044382}. Best is trial 12 with value: 1.2980119228363036.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 5: {'n_estimators': 148, 'max_depth': 7, 'learning_rate': 0.09936605342538195, 'subsample': 0.6585560646388384, 'colsample_bytree': 0.5788506755138203}\n",
            "\n",
            "=== K-Fold Evaluation ===\n",
            "RMSE: 2.8710 | MAE: 1.9697\n",
            "Loaded RMSE: 2.870966860503849\n",
            "Loaded MAE: 1.9697116613388062\n"
          ]
        }
      ],
      "source": [
        "loaded_results = run_ssl_pipeline_kfold(\n",
        "    labeled_df=df,\n",
        "    load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\"\n",
        ")\n",
        "\n",
        "print(\"Loaded RMSE:\", loaded_results[\"rmse\"])\n",
        "print(\"Loaded MAE:\", loaded_results[\"mae\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjOI2XXfC4D0"
      },
      "source": [
        "#### Metrics: MAE 1.978\n",
        "\n",
        "=== K-Fold Evaluation ===\n",
        "RMSE: 2.7948 | MAE: 1.9784\n",
        "Loaded RMSE: 2.794774636069688\n",
        "Loaded MAE: 1.9784057140350342"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ganAph2heYH3"
      },
      "source": [
        "### R2 Kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNum3Z9zeYqf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def run_ssl_pipeline_kfold_with_r2(labeled_df,\n",
        "                                   unlabelled_dir=None,\n",
        "                                   ssl_epochs=20,\n",
        "                                   ssl_batch=8,\n",
        "                                   fine_tune_backbone=True,\n",
        "                                   fine_tune_epochs=10,\n",
        "                                   use_metadata=False,\n",
        "                                   optuna_trials=20,\n",
        "                                   run_base_dir=\"models\",\n",
        "                                   load_run_dir=None,\n",
        "                                   n_splits=5):\n",
        "\n",
        "    run_dir = load_run_dir if load_run_dir else make_run_dir(run_base_dir)\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    print(f\"Run directory: {run_dir}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Load or pretrain SSL backbone\n",
        "    # -------------------\n",
        "    backbone = get_ssl_backbone(pretrained=False)\n",
        "    proj_head = ProjectionHead().to(device)\n",
        "\n",
        "    if load_run_dir and os.path.exists(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\")):\n",
        "        backbone.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"), map_location=device))\n",
        "        proj_head.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"), map_location=device))\n",
        "        backbone.to(device).eval()\n",
        "        proj_head.to(device).eval()\n",
        "        print(\"Loaded pretrained backbone and projection head from saved run.\")\n",
        "    else:\n",
        "        print(\"=== SSL Pretraining ===\")\n",
        "        if unlabelled_dir:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(unlabelled_dir, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=True)\n",
        "        else:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(labeled_df, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=False)\n",
        "\n",
        "    # -------------------\n",
        "    # K-Fold CV\n",
        "    # -------------------\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(labeled_df)):\n",
        "        print(f\"\\n--- K-Fold {fold_idx+1}/{n_splits} ---\")\n",
        "        train_df, test_df = labeled_df.iloc[train_idx], labeled_df.iloc[test_idx]\n",
        "\n",
        "        # Fine-tune backbone if required\n",
        "        if fine_tune_backbone:\n",
        "            dataset = HbImageDataset(train_df, transform=ssl_transform, n_views=2)\n",
        "            loader = DataLoader(dataset, batch_size=ssl_batch, shuffle=True, num_workers=2, pin_memory=True)\n",
        "            backbone.train()\n",
        "            proj_head.train()\n",
        "            optimizer = torch.optim.Adam(list(backbone.parameters()) + list(proj_head.parameters()), lr=1e-4)\n",
        "            for epoch in range(fine_tune_epochs):\n",
        "                total_loss = 0.0\n",
        "                for views, _ in loader:\n",
        "                    v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "                    feats1, feats2 = backbone(v1).view(v1.size(0), -1), backbone(v2).view(v2.size(0), -1)\n",
        "                    z1, z2 = proj_head(feats1), proj_head(feats2)\n",
        "                    loss = nt_xent_loss(z1, z2)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "                print(f\"Fold {fold_idx+1} Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "        # Extract embeddings\n",
        "        X_train, y_train = extract_embeddings(train_df, backbone, val_transform)\n",
        "        X_test, y_test = extract_embeddings(test_df, backbone, val_transform)\n",
        "\n",
        "        # Optuna hyperparameter tuning\n",
        "        def objective(trial):\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\",50,300),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\",2,10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\",0.01,0.3,log=True),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\",0.5,1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.5,1.0),\n",
        "                \"random_state\": RANDOM_SEED,\n",
        "                \"verbosity\": 0,\n",
        "                \"n_jobs\": 1,\n",
        "                \"tree_method\": \"hist\"\n",
        "            }\n",
        "            kf_inner = KFold(n_splits=min(5, len(train_df)), shuffle=True, random_state=RANDOM_SEED)\n",
        "            maes = []\n",
        "            for tr_idx, val_idx in kf_inner.split(X_train):\n",
        "                model = xgb.XGBRegressor(**params)\n",
        "                model.fit(X_train[tr_idx], y_train[tr_idx])\n",
        "                y_pred = model.predict(X_train[val_idx])\n",
        "                maes.append(mean_absolute_error(y_train[val_idx], y_pred))\n",
        "            return np.mean(maes)\n",
        "\n",
        "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "        study.optimize(objective, n_trials=optuna_trials, show_progress_bar=False)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best params fold {fold_idx+1}: {best_params}\")\n",
        "\n",
        "        # Train final model\n",
        "        final_model = xgb.XGBRegressor(**best_params, random_state=RANDOM_SEED, n_jobs=-1, tree_method=\"hist\")\n",
        "        final_model.fit(X_train, y_train)\n",
        "        y_pred = final_model.predict(X_test)\n",
        "\n",
        "        # Compute metrics including R\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        print(f\"[Fold {fold_idx+1}] RMSE: {rmse:.4f}, MAE: {mae:.4f}, R: {r2:.4f}\")\n",
        "\n",
        "        fold_metrics.append({\"fold\": fold_idx+1, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n",
        "\n",
        "    # -------------------\n",
        "    # Summary metrics\n",
        "    # -------------------\n",
        "    metrics_df = pd.DataFrame(fold_metrics)\n",
        "    metrics_df.loc[\"Mean\"] = metrics_df.mean()\n",
        "    print(\"\\n=== Fold Metrics Summary ===\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    # Save metrics and models\n",
        "    metrics_df.to_csv(os.path.join(run_dir, \"kfold_metrics.csv\"), index=False)\n",
        "    torch.save(backbone.state_dict(), os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"))\n",
        "    torch.save(proj_head.state_dict(), os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"))\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(metrics_df['fold'][:-1], metrics_df['RMSE'][:-1], marker='o', label='RMSE')\n",
        "    plt.plot(metrics_df['fold'][:-1], metrics_df['MAE'][:-1], marker='s', label='MAE')\n",
        "    plt.plot(metrics_df['fold'][:-1], metrics_df['R2'][:-1], marker='^', label='R')\n",
        "    plt.axhline(metrics_df.loc[\"Mean\", 'RMSE'], color='blue', linestyle='--', alpha=0.5)\n",
        "    plt.axhline(metrics_df.loc[\"Mean\", 'MAE'], color='orange', linestyle='--', alpha=0.5)\n",
        "    plt.axhline(metrics_df.loc[\"Mean\", 'R2'], color='green', linestyle='--', alpha=0.5)\n",
        "    plt.xlabel(\"Fold\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.title(\"K-Fold Regression Metrics (RMSE, MAE, R)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(run_dir, \"kfold_metrics_plot.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        \"backbone\": backbone,\n",
        "        \"proj_head\": proj_head,\n",
        "        \"metrics_df\": metrics_df,\n",
        "        \"run_dir\": run_dir\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-OEqHKlCuL"
      },
      "source": [
        "#### Current Best (SSL + Finetuning) High R2 > 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S3NtZuf0i29k",
        "outputId": "4cc32288-9464-4a71-baa0-5dadf493076e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run directory: /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\n",
            "Loaded pretrained backbone and projection head from saved run.\n",
            "\n",
            "--- K-Fold 1/5 ---\n",
            "Fold 1 Epoch 1 Loss: 1.7436\n",
            "Fold 1 Epoch 2 Loss: 1.9377\n",
            "Fold 1 Epoch 3 Loss: 2.1040\n",
            "Fold 1 Epoch 4 Loss: 2.1732\n",
            "Fold 1 Epoch 5 Loss: 1.8421\n",
            "Fold 1 Epoch 6 Loss: 1.8999\n",
            "Fold 1 Epoch 7 Loss: 1.8951\n",
            "Fold 1 Epoch 8 Loss: 1.8689\n",
            "Fold 1 Epoch 9 Loss: 1.8060\n",
            "Fold 1 Epoch 10 Loss: 1.9578\n",
            "Fold 1 Epoch 11 Loss: 1.7840\n",
            "Fold 1 Epoch 12 Loss: 1.8210\n",
            "Fold 1 Epoch 13 Loss: 2.0893\n",
            "Fold 1 Epoch 14 Loss: 2.1568\n",
            "Fold 1 Epoch 15 Loss: 1.9301\n",
            "Fold 1 Epoch 16 Loss: 2.1927\n",
            "Fold 1 Epoch 17 Loss: 1.6963\n",
            "Fold 1 Epoch 18 Loss: 2.0176\n",
            "Fold 1 Epoch 19 Loss: 1.8706\n",
            "Fold 1 Epoch 20 Loss: 1.8268\n",
            "Fold 1 Epoch 21 Loss: 1.7679\n",
            "Fold 1 Epoch 22 Loss: 1.8760\n",
            "Fold 1 Epoch 23 Loss: 1.5489\n",
            "Fold 1 Epoch 24 Loss: 1.8597\n",
            "Fold 1 Epoch 25 Loss: 1.8951\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 12:04:17,685] A new study created in memory with name: no-name-af432900-392d-41f1-b202-fbf6f8c82395\n",
            "[I 2025-10-07 12:04:18,760] Trial 0 finished with value: 1.705259418487549 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.705259418487549.\n",
            "[I 2025-10-07 12:04:19,086] Trial 1 finished with value: 1.9320958137512207 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.705259418487549.\n",
            "[I 2025-10-07 12:04:19,479] Trial 2 finished with value: 2.000939130783081 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 1.705259418487549.\n",
            "[I 2025-10-07 12:04:20,047] Trial 3 finished with value: 1.6770681142807007 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 1.6770681142807007.\n",
            "[I 2025-10-07 12:04:21,031] Trial 4 finished with value: 1.9044593572616577 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 1.6770681142807007.\n",
            "[I 2025-10-07 12:04:21,985] Trial 5 finished with value: 1.8472979068756104 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 3 with value: 1.6770681142807007.\n",
            "[I 2025-10-07 12:04:24,061] Trial 6 finished with value: 1.3368527591228485 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:26,434] Trial 7 finished with value: 1.6032768726348876 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:26,981] Trial 8 finished with value: 1.607345747947693 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:28,178] Trial 9 finished with value: 1.7718917846679687 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:31,819] Trial 10 finished with value: 1.3821647047996521 and parameters: {'n_estimators': 289, 'max_depth': 7, 'learning_rate': 0.02367326211211725, 'subsample': 0.9775724145208066, 'colsample_bytree': 0.991948117710163}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:36,709] Trial 11 finished with value: 1.446961623430252 and parameters: {'n_estimators': 278, 'max_depth': 7, 'learning_rate': 0.022731789983718213, 'subsample': 0.9851129432910448, 'colsample_bytree': 0.9988392431163264}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:41,763] Trial 12 finished with value: 1.4973576307296752 and parameters: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.02376337959095378, 'subsample': 0.9064708648566141, 'colsample_bytree': 0.9970029045473664}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:43,377] Trial 13 finished with value: 1.4618464708328247 and parameters: {'n_estimators': 161, 'max_depth': 8, 'learning_rate': 0.017005609599943586, 'subsample': 0.9112320253004816, 'colsample_bytree': 0.8775185785252101}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:44,943] Trial 14 finished with value: 2.161573576927185 and parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.03423835804110461, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.8859620061900356}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:46,186] Trial 15 finished with value: 1.435259461402893 and parameters: {'n_estimators': 134, 'max_depth': 5, 'learning_rate': 0.01091446449704049, 'subsample': 0.9948158984245835, 'colsample_bytree': 0.9390182871697861}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:48,791] Trial 16 finished with value: 1.4423927187919616 and parameters: {'n_estimators': 245, 'max_depth': 8, 'learning_rate': 0.03869025123624818, 'subsample': 0.8961058234725818, 'colsample_bytree': 0.797727788634879}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:49,985] Trial 17 finished with value: 1.7889298915863037 and parameters: {'n_estimators': 300, 'max_depth': 2, 'learning_rate': 0.016450757660184966, 'subsample': 0.8600075789089481, 'colsample_bytree': 0.9348202392619969}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:52,606] Trial 18 finished with value: 1.4998973250389098 and parameters: {'n_estimators': 180, 'max_depth': 7, 'learning_rate': 0.09816121393578726, 'subsample': 0.9470259666420766, 'colsample_bytree': 0.8117125374670062}. Best is trial 6 with value: 1.3368527591228485.\n",
            "[I 2025-10-07 12:04:53,981] Trial 19 finished with value: 1.907213854789734 and parameters: {'n_estimators': 265, 'max_depth': 5, 'learning_rate': 0.2702272241241035, 'subsample': 0.6869888528707923, 'colsample_bytree': 0.9396878847451278}. Best is trial 6 with value: 1.3368527591228485.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 1: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}\n",
            "[Fold 1] RMSE: 1.6598, MAE: 1.0721, R: 0.6515\n",
            "\n",
            "--- K-Fold 2/5 ---\n",
            "Fold 2 Epoch 1 Loss: 1.3323\n",
            "Fold 2 Epoch 2 Loss: 1.4725\n",
            "Fold 2 Epoch 3 Loss: 1.2966\n",
            "Fold 2 Epoch 4 Loss: 1.4716\n",
            "Fold 2 Epoch 5 Loss: 1.4495\n",
            "Fold 2 Epoch 6 Loss: 1.3590\n",
            "Fold 2 Epoch 7 Loss: 1.2919\n",
            "Fold 2 Epoch 8 Loss: 1.2126\n",
            "Fold 2 Epoch 9 Loss: 1.3848\n",
            "Fold 2 Epoch 10 Loss: 1.3565\n",
            "Fold 2 Epoch 11 Loss: 1.3810\n",
            "Fold 2 Epoch 12 Loss: 1.1610\n",
            "Fold 2 Epoch 13 Loss: 1.0989\n",
            "Fold 2 Epoch 14 Loss: 1.3616\n",
            "Fold 2 Epoch 15 Loss: 1.3426\n",
            "Fold 2 Epoch 16 Loss: 1.1646\n",
            "Fold 2 Epoch 17 Loss: 1.1218\n",
            "Fold 2 Epoch 18 Loss: 1.4559\n",
            "Fold 2 Epoch 19 Loss: 1.2467\n",
            "Fold 2 Epoch 20 Loss: 1.1818\n",
            "Fold 2 Epoch 21 Loss: 1.2721\n",
            "Fold 2 Epoch 22 Loss: 1.4293\n",
            "Fold 2 Epoch 23 Loss: 1.4157\n",
            "Fold 2 Epoch 24 Loss: 1.1802\n",
            "Fold 2 Epoch 25 Loss: 1.1343\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 12:06:00,161] A new study created in memory with name: no-name-25306abd-5e85-4187-a665-819df17ba72e\n",
            "[I 2025-10-07 12:06:01,902] Trial 0 finished with value: 1.7566500186920166 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.7566500186920166.\n",
            "[I 2025-10-07 12:06:02,465] Trial 1 finished with value: 1.9514246225357055 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.7566500186920166.\n",
            "[I 2025-10-07 12:06:03,022] Trial 2 finished with value: 1.6932018995285034 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:03,599] Trial 3 finished with value: 1.8018436193466187 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:04,626] Trial 4 finished with value: 1.9132163047790527 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:05,573] Trial 5 finished with value: 1.8650894165039062 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:06,829] Trial 6 finished with value: 2.0605970859527587 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:08,512] Trial 7 finished with value: 1.9383918523788453 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:09,007] Trial 8 finished with value: 1.9432401657104492 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:10,237] Trial 9 finished with value: 1.8113396406173705 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:11,217] Trial 10 finished with value: 2.4288330554962156 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:12,075] Trial 11 finished with value: 1.8924609661102294 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.12040035550822742, 'subsample': 0.5899164086425722, 'colsample_bytree': 0.5037653191432865}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:12,473] Trial 12 finished with value: 1.7998998165130615 and parameters: {'n_estimators': 53, 'max_depth': 8, 'learning_rate': 0.13001429961399444, 'subsample': 0.6417615206842633, 'colsample_bytree': 0.5788506755138203}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:14,060] Trial 13 finished with value: 1.7344013571739196 and parameters: {'n_estimators': 135, 'max_depth': 8, 'learning_rate': 0.11678558089474768, 'subsample': 0.8936108460389663, 'colsample_bytree': 0.6715143647973427}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:14,984] Trial 14 finished with value: 2.0461023807525636 and parameters: {'n_estimators': 111, 'max_depth': 8, 'learning_rate': 0.291266617275455, 'subsample': 0.8947385331597324, 'colsample_bytree': 0.6738167152482913}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:17,705] Trial 15 finished with value: 2.0160589933395388 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.08962717626942573, 'subsample': 0.5392311924833902, 'colsample_bytree': 0.815562518756932}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:18,123] Trial 16 finished with value: 1.828784990310669 and parameters: {'n_estimators': 51, 'max_depth': 7, 'learning_rate': 0.03629684946538655, 'subsample': 0.8945591171626043, 'colsample_bytree': 0.6997505004217871}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:19,154] Trial 17 finished with value: 2.0364668369293213 and parameters: {'n_estimators': 122, 'max_depth': 9, 'learning_rate': 0.1668922544725929, 'subsample': 0.6356032890768908, 'colsample_bytree': 0.7761845970290123}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:20,719] Trial 18 finished with value: 2.0029885292053224 and parameters: {'n_estimators': 173, 'max_depth': 6, 'learning_rate': 0.09447807611743418, 'subsample': 0.5827781406926889, 'colsample_bytree': 0.910236126593346}. Best is trial 2 with value: 1.6932018995285034.\n",
            "[I 2025-10-07 12:06:22,048] Trial 19 finished with value: 1.8106297731399537 and parameters: {'n_estimators': 170, 'max_depth': 9, 'learning_rate': 0.07748616070784038, 'subsample': 0.7304037511564767, 'colsample_bytree': 0.6280311653510122}. Best is trial 2 with value: 1.6932018995285034.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 2: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}\n",
            "[Fold 2] RMSE: 1.3619, MAE: 1.0797, R: 0.5529\n",
            "\n",
            "--- K-Fold 3/5 ---\n",
            "Fold 3 Epoch 1 Loss: 1.2273\n",
            "Fold 3 Epoch 2 Loss: 1.3413\n",
            "Fold 3 Epoch 3 Loss: 1.2920\n",
            "Fold 3 Epoch 4 Loss: 1.3030\n",
            "Fold 3 Epoch 5 Loss: 1.2384\n",
            "Fold 3 Epoch 6 Loss: 1.1677\n",
            "Fold 3 Epoch 7 Loss: 1.1632\n",
            "Fold 3 Epoch 8 Loss: 1.1622\n",
            "Fold 3 Epoch 9 Loss: 1.1562\n",
            "Fold 3 Epoch 10 Loss: 1.1945\n",
            "Fold 3 Epoch 11 Loss: 1.3315\n",
            "Fold 3 Epoch 12 Loss: 1.2406\n",
            "Fold 3 Epoch 13 Loss: 1.1839\n",
            "Fold 3 Epoch 14 Loss: 1.0732\n",
            "Fold 3 Epoch 15 Loss: 1.0282\n",
            "Fold 3 Epoch 16 Loss: 1.1237\n",
            "Fold 3 Epoch 17 Loss: 1.2375\n",
            "Fold 3 Epoch 18 Loss: 1.2289\n",
            "Fold 3 Epoch 19 Loss: 1.3227\n",
            "Fold 3 Epoch 20 Loss: 1.0959\n",
            "Fold 3 Epoch 21 Loss: 1.1631\n",
            "Fold 3 Epoch 22 Loss: 1.3535\n",
            "Fold 3 Epoch 23 Loss: 1.1950\n",
            "Fold 3 Epoch 24 Loss: 1.0456\n",
            "Fold 3 Epoch 25 Loss: 1.1367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 12:07:31,052] A new study created in memory with name: no-name-accbc2f4-e8d2-4969-bc2d-fadce9626e48\n",
            "[I 2025-10-07 12:07:32,090] Trial 0 finished with value: 1.7423351526260376 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.7423351526260376.\n",
            "[I 2025-10-07 12:07:32,424] Trial 1 finished with value: 2.1031939506530763 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.7423351526260376.\n",
            "[I 2025-10-07 12:07:32,834] Trial 2 finished with value: 2.057113242149353 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 1.7423351526260376.\n",
            "[I 2025-10-07 12:07:33,443] Trial 3 finished with value: 1.7191841840744018 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 1.7191841840744018.\n",
            "[I 2025-10-07 12:07:34,462] Trial 4 finished with value: 1.9130093574523925 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 1.7191841840744018.\n",
            "[I 2025-10-07 12:07:35,413] Trial 5 finished with value: 1.7340064525604248 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 3 with value: 1.7191841840744018.\n",
            "[I 2025-10-07 12:07:36,807] Trial 6 finished with value: 1.7517093658447265 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 3 with value: 1.7191841840744018.\n",
            "[I 2025-10-07 12:07:39,662] Trial 7 finished with value: 1.840550994873047 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 3 with value: 1.7191841840744018.\n",
            "[I 2025-10-07 12:07:40,460] Trial 8 finished with value: 2.0044724702835084 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 3 with value: 1.7191841840744018.\n",
            "[I 2025-10-07 12:07:41,654] Trial 9 finished with value: 1.827299952507019 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 3 with value: 1.7191841840744018.\n",
            "[I 2025-10-07 12:07:42,657] Trial 10 finished with value: 1.8676088333129883 and parameters: {'n_estimators': 141, 'max_depth': 7, 'learning_rate': 0.030315725522749647, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8285618345363206}. Best is trial 3 with value: 1.7191841840744018.\n",
            "[I 2025-10-07 12:07:44,304] Trial 11 finished with value: 1.731068778038025 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.06463513151371238, 'subsample': 0.6758580632546128, 'colsample_bytree': 0.5020730763700768}. Best is trial 3 with value: 1.7191841840744018.\n",
            "[I 2025-10-07 12:07:45,851] Trial 12 finished with value: 1.6026379346847535 and parameters: {'n_estimators': 299, 'max_depth': 7, 'learning_rate': 0.08787850607501545, 'subsample': 0.6871730874891191, 'colsample_bytree': 0.509429036272865}. Best is trial 12 with value: 1.6026379346847535.\n",
            "[I 2025-10-07 12:07:46,928] Trial 13 finished with value: 1.8302698373794555 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.09271853302861457, 'subsample': 0.6730069363658565, 'colsample_bytree': 0.6454936010745959}. Best is trial 12 with value: 1.6026379346847535.\n",
            "[I 2025-10-07 12:07:47,633] Trial 14 finished with value: 2.120265555381775 and parameters: {'n_estimators': 116, 'max_depth': 5, 'learning_rate': 0.2861636630570305, 'subsample': 0.5653256167561652, 'colsample_bytree': 0.6738167152482913}. Best is trial 12 with value: 1.6026379346847535.\n",
            "[I 2025-10-07 12:07:50,550] Trial 15 finished with value: 1.873235535621643 and parameters: {'n_estimators': 298, 'max_depth': 8, 'learning_rate': 0.03208899494279826, 'subsample': 0.7184573645895903, 'colsample_bytree': 0.8304278626461433}. Best is trial 12 with value: 1.6026379346847535.\n",
            "[I 2025-10-07 12:07:52,376] Trial 16 finished with value: 1.8917981386184692 and parameters: {'n_estimators': 174, 'max_depth': 8, 'learning_rate': 0.09792171490762987, 'subsample': 0.6142690860932788, 'colsample_bytree': 0.5501823082481293}. Best is trial 12 with value: 1.6026379346847535.\n",
            "[I 2025-10-07 12:07:54,477] Trial 17 finished with value: 1.7241249322891234 and parameters: {'n_estimators': 165, 'max_depth': 6, 'learning_rate': 0.03979334151581204, 'subsample': 0.8772424455984447, 'colsample_bytree': 0.7830181258743638}. Best is trial 12 with value: 1.6026379346847535.\n",
            "[I 2025-10-07 12:07:55,235] Trial 18 finished with value: 1.880804705619812 and parameters: {'n_estimators': 107, 'max_depth': 5, 'learning_rate': 0.019631797854441864, 'subsample': 0.7276826999536898, 'colsample_bytree': 0.6874356927360459}. Best is trial 12 with value: 1.6026379346847535.\n",
            "[I 2025-10-07 12:07:57,465] Trial 19 finished with value: 1.9320272207260132 and parameters: {'n_estimators': 240, 'max_depth': 9, 'learning_rate': 0.07879221623268368, 'subsample': 0.8808313666551089, 'colsample_bytree': 0.9072950643400716}. Best is trial 12 with value: 1.6026379346847535.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 3: {'n_estimators': 299, 'max_depth': 7, 'learning_rate': 0.08787850607501545, 'subsample': 0.6871730874891191, 'colsample_bytree': 0.509429036272865}\n",
            "[Fold 3] RMSE: 0.9821, MAE: 0.5247, R: 0.8628\n",
            "\n",
            "--- K-Fold 4/5 ---\n",
            "Fold 4 Epoch 1 Loss: 1.1673\n",
            "Fold 4 Epoch 2 Loss: 1.1872\n",
            "Fold 4 Epoch 3 Loss: 1.0005\n",
            "Fold 4 Epoch 4 Loss: 1.1346\n",
            "Fold 4 Epoch 5 Loss: 1.1570\n",
            "Fold 4 Epoch 6 Loss: 1.1259\n",
            "Fold 4 Epoch 7 Loss: 1.1001\n",
            "Fold 4 Epoch 8 Loss: 1.0498\n",
            "Fold 4 Epoch 9 Loss: 1.1840\n",
            "Fold 4 Epoch 10 Loss: 1.1744\n",
            "Fold 4 Epoch 11 Loss: 1.1251\n",
            "Fold 4 Epoch 12 Loss: 1.2022\n",
            "Fold 4 Epoch 13 Loss: 1.0075\n",
            "Fold 4 Epoch 14 Loss: 1.1518\n",
            "Fold 4 Epoch 15 Loss: 1.2023\n",
            "Fold 4 Epoch 16 Loss: 1.0238\n",
            "Fold 4 Epoch 17 Loss: 1.0295\n",
            "Fold 4 Epoch 18 Loss: 1.1954\n",
            "Fold 4 Epoch 19 Loss: 1.1278\n",
            "Fold 4 Epoch 20 Loss: 1.0936\n",
            "Fold 4 Epoch 21 Loss: 1.0825\n",
            "Fold 4 Epoch 22 Loss: 1.0348\n",
            "Fold 4 Epoch 23 Loss: 0.9575\n",
            "Fold 4 Epoch 24 Loss: 1.2814\n",
            "Fold 4 Epoch 25 Loss: 1.1488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 12:09:10,988] A new study created in memory with name: no-name-49adcd7d-81c1-4511-b6fb-7761953fc9e1\n",
            "[I 2025-10-07 12:09:12,046] Trial 0 finished with value: 1.7044961214065553 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.7044961214065553.\n",
            "[I 2025-10-07 12:09:12,390] Trial 1 finished with value: 1.6737548470497132 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.6737548470497132.\n",
            "[I 2025-10-07 12:09:12,773] Trial 2 finished with value: 1.9776325941085815 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 1.6737548470497132.\n",
            "[I 2025-10-07 12:09:13,354] Trial 3 finished with value: 1.6396090865135193 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 1.6396090865135193.\n",
            "[I 2025-10-07 12:09:14,410] Trial 4 finished with value: 1.5690837860107423 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 4 with value: 1.5690837860107423.\n",
            "[I 2025-10-07 12:09:16,017] Trial 5 finished with value: 1.5506445169448853 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 1.5506445169448853.\n",
            "[I 2025-10-07 12:09:18,226] Trial 6 finished with value: 1.6333621263504028 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 1.5506445169448853.\n",
            "[I 2025-10-07 12:09:20,476] Trial 7 finished with value: 1.5468757390975951 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 7 with value: 1.5468757390975951.\n",
            "[I 2025-10-07 12:09:21,053] Trial 8 finished with value: 1.9155937552452087 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 7 with value: 1.5468757390975951.\n",
            "[I 2025-10-07 12:09:22,291] Trial 9 finished with value: 1.6013031482696534 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 7 with value: 1.5468757390975951.\n",
            "[I 2025-10-07 12:09:24,423] Trial 10 finished with value: 1.8034382820129395 and parameters: {'n_estimators': 295, 'max_depth': 7, 'learning_rate': 0.023413669026819843, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8200442512337782}. Best is trial 7 with value: 1.5468757390975951.\n",
            "[I 2025-10-07 12:09:26,281] Trial 11 finished with value: 1.588922095298767 and parameters: {'n_estimators': 273, 'max_depth': 6, 'learning_rate': 0.030241001410559513, 'subsample': 0.8751050092487717, 'colsample_bytree': 0.5065883662316701}. Best is trial 7 with value: 1.5468757390975951.\n",
            "[I 2025-10-07 12:09:28,083] Trial 12 finished with value: 1.5534969806671142 and parameters: {'n_estimators': 246, 'max_depth': 5, 'learning_rate': 0.08527855281875678, 'subsample': 0.8651265717543035, 'colsample_bytree': 0.7363570033289744}. Best is trial 7 with value: 1.5468757390975951.\n",
            "[I 2025-10-07 12:09:29,131] Trial 13 finished with value: 1.5292926371097564 and parameters: {'n_estimators': 249, 'max_depth': 2, 'learning_rate': 0.04227361237988428, 'subsample': 0.8738214287354175, 'colsample_bytree': 0.5078395227619208}. Best is trial 13 with value: 1.5292926371097564.\n",
            "[I 2025-10-07 12:09:30,298] Trial 14 finished with value: 1.5357429087162018 and parameters: {'n_estimators': 170, 'max_depth': 2, 'learning_rate': 0.01939840148712275, 'subsample': 0.8947385331597324, 'colsample_bytree': 0.9483273769974196}. Best is trial 13 with value: 1.5292926371097564.\n",
            "[I 2025-10-07 12:09:31,335] Trial 15 finished with value: 1.5589881896972657 and parameters: {'n_estimators': 151, 'max_depth': 2, 'learning_rate': 0.020395525436415603, 'subsample': 0.9234980616943328, 'colsample_bytree': 0.974119988488152}. Best is trial 13 with value: 1.5292926371097564.\n",
            "[I 2025-10-07 12:09:31,984] Trial 16 finished with value: 1.494716227054596 and parameters: {'n_estimators': 159, 'max_depth': 2, 'learning_rate': 0.03759815678966938, 'subsample': 0.9974520669192535, 'colsample_bytree': 0.8989974345395086}. Best is trial 16 with value: 1.494716227054596.\n",
            "[I 2025-10-07 12:09:33,271] Trial 17 finished with value: 1.5731019139289857 and parameters: {'n_estimators': 125, 'max_depth': 8, 'learning_rate': 0.03451108212021234, 'subsample': 0.9975592275711639, 'colsample_bytree': 0.885561851069681}. Best is trial 16 with value: 1.494716227054596.\n",
            "[I 2025-10-07 12:09:36,339] Trial 18 finished with value: 1.6154810667037964 and parameters: {'n_estimators': 183, 'max_depth': 8, 'learning_rate': 0.04039735816417211, 'subsample': 0.9286364425461193, 'colsample_bytree': 0.8004745502036947}. Best is trial 16 with value: 1.494716227054596.\n",
            "[I 2025-10-07 12:09:37,358] Trial 19 finished with value: 1.6075962424278258 and parameters: {'n_estimators': 221, 'max_depth': 5, 'learning_rate': 0.2702272241241035, 'subsample': 0.9166469343732542, 'colsample_bytree': 0.9106018662180388}. Best is trial 16 with value: 1.494716227054596.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 4: {'n_estimators': 159, 'max_depth': 2, 'learning_rate': 0.03759815678966938, 'subsample': 0.9974520669192535, 'colsample_bytree': 0.8989974345395086}\n",
            "[Fold 4] RMSE: 1.3100, MAE: 1.0577, R: 0.7828\n",
            "\n",
            "--- K-Fold 5/5 ---\n",
            "Fold 5 Epoch 1 Loss: 1.1446\n",
            "Fold 5 Epoch 2 Loss: 1.1504\n",
            "Fold 5 Epoch 3 Loss: 1.0943\n",
            "Fold 5 Epoch 4 Loss: 1.1534\n",
            "Fold 5 Epoch 5 Loss: 1.1841\n",
            "Fold 5 Epoch 6 Loss: 1.0242\n",
            "Fold 5 Epoch 7 Loss: 0.9957\n",
            "Fold 5 Epoch 8 Loss: 1.1227\n",
            "Fold 5 Epoch 9 Loss: 0.9849\n",
            "Fold 5 Epoch 10 Loss: 0.9875\n",
            "Fold 5 Epoch 11 Loss: 1.0630\n",
            "Fold 5 Epoch 12 Loss: 0.9743\n",
            "Fold 5 Epoch 13 Loss: 1.0539\n",
            "Fold 5 Epoch 14 Loss: 1.0296\n",
            "Fold 5 Epoch 15 Loss: 1.1134\n",
            "Fold 5 Epoch 16 Loss: 0.9720\n",
            "Fold 5 Epoch 17 Loss: 0.9454\n",
            "Fold 5 Epoch 18 Loss: 1.0685\n",
            "Fold 5 Epoch 19 Loss: 0.9922\n",
            "Fold 5 Epoch 20 Loss: 0.9857\n",
            "Fold 5 Epoch 21 Loss: 0.9939\n",
            "Fold 5 Epoch 22 Loss: 0.9963\n",
            "Fold 5 Epoch 23 Loss: 1.0094\n",
            "Fold 5 Epoch 24 Loss: 1.0147\n",
            "Fold 5 Epoch 25 Loss: 1.0491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 12:10:46,158] A new study created in memory with name: no-name-eff8bc53-b6b9-4052-8480-52517860f490\n",
            "[I 2025-10-07 12:10:47,171] Trial 0 finished with value: 2.0845030546188354 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.0845030546188354.\n",
            "[I 2025-10-07 12:10:47,523] Trial 1 finished with value: 2.0461567401885987 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.0461567401885987.\n",
            "[I 2025-10-07 12:10:47,903] Trial 2 finished with value: 1.8341010451316833 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.8341010451316833.\n",
            "[I 2025-10-07 12:10:48,513] Trial 3 finished with value: 1.949990487098694 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.8341010451316833.\n",
            "[I 2025-10-07 12:10:49,526] Trial 4 finished with value: 1.9053124785423279 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.8341010451316833.\n",
            "[I 2025-10-07 12:10:50,451] Trial 5 finished with value: 1.9824583411216736 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.8341010451316833.\n",
            "[I 2025-10-07 12:10:52,271] Trial 6 finished with value: 1.8981231451034546 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 1.8341010451316833.\n",
            "[I 2025-10-07 12:10:54,939] Trial 7 finished with value: 1.934395432472229 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 1.8341010451316833.\n",
            "[I 2025-10-07 12:10:55,559] Trial 8 finished with value: 1.9679493725299835 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 1.8341010451316833.\n",
            "[I 2025-10-07 12:10:56,738] Trial 9 finished with value: 1.8932819604873656 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 1.8341010451316833.\n",
            "[I 2025-10-07 12:10:57,736] Trial 10 finished with value: 2.202276921272278 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 2 with value: 1.8341010451316833.\n",
            "[I 2025-10-07 12:10:59,242] Trial 11 finished with value: 1.8919120490550996 and parameters: {'n_estimators': 277, 'max_depth': 8, 'learning_rate': 0.09301556331715216, 'subsample': 0.5971603596691708, 'colsample_bytree': 0.5037892523442965}. Best is trial 2 with value: 1.8341010451316833.\n",
            "[I 2025-10-07 12:11:00,586] Trial 12 finished with value: 1.7018365383148193 and parameters: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.12594007031355467, 'subsample': 0.5783170319382445, 'colsample_bytree': 0.5083162241661173}. Best is trial 12 with value: 1.7018365383148193.\n",
            "[I 2025-10-07 12:11:01,401] Trial 13 finished with value: 1.8616889476776124 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.1361058138823361, 'subsample': 0.6079180203564428, 'colsample_bytree': 0.501817089415866}. Best is trial 12 with value: 1.7018365383148193.\n",
            "[I 2025-10-07 12:11:02,450] Trial 14 finished with value: 2.1352511048316956 and parameters: {'n_estimators': 281, 'max_depth': 8, 'learning_rate': 0.29219551756458056, 'subsample': 0.5259781923099083, 'colsample_bytree': 0.6738167152482913}. Best is trial 12 with value: 1.7018365383148193.\n",
            "[I 2025-10-07 12:11:02,833] Trial 15 finished with value: 1.8276178121566773 and parameters: {'n_estimators': 58, 'max_depth': 9, 'learning_rate': 0.09214174051661962, 'subsample': 0.6274745095999714, 'colsample_bytree': 0.5778918965965928}. Best is trial 12 with value: 1.7018365383148193.\n",
            "[I 2025-10-07 12:11:04,789] Trial 16 finished with value: 1.818587052822113 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.029758042310518568, 'subsample': 0.66316755774775, 'colsample_bytree': 0.5626015947022875}. Best is trial 12 with value: 1.7018365383148193.\n",
            "[I 2025-10-07 12:11:08,782] Trial 17 finished with value: 1.8801779985427856 and parameters: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.028069002597357614, 'subsample': 0.6828326837264873, 'colsample_bytree': 0.8690491710631163}. Best is trial 12 with value: 1.7018365383148193.\n",
            "[I 2025-10-07 12:11:10,585] Trial 18 finished with value: 1.85572190284729 and parameters: {'n_estimators': 245, 'max_depth': 7, 'learning_rate': 0.03212437033285374, 'subsample': 0.5515037717121494, 'colsample_bytree': 0.7848002461661141}. Best is trial 12 with value: 1.7018365383148193.\n",
            "[I 2025-10-07 12:11:12,318] Trial 19 finished with value: 1.7777797520160674 and parameters: {'n_estimators': 297, 'max_depth': 7, 'learning_rate': 0.018880962848698477, 'subsample': 0.6652888386273534, 'colsample_bytree': 0.5498122661645771}. Best is trial 12 with value: 1.7018365383148193.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 5: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.12594007031355467, 'subsample': 0.5783170319382445, 'colsample_bytree': 0.5083162241661173}\n",
            "[Fold 5] RMSE: 3.9304, MAE: 2.4142, R: 0.0055\n",
            "\n",
            "=== Fold Metrics Summary ===\n",
            "      fold      RMSE       MAE        R2\n",
            "0      1.0  1.659839  1.072101  0.651509\n",
            "1      2.0  1.361875  1.079659  0.552874\n",
            "2      3.0  0.982149  0.524712  0.862835\n",
            "3      4.0  1.310026  1.057672  0.782825\n",
            "4      5.0  3.930360  2.414192  0.005458\n",
            "Mean   3.0  1.848850  1.229667  0.571100\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "loaded_results = run_ssl_pipeline_kfold_with_r2(\n",
        "    labeled_df=df,\n",
        "    load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\",\n",
        "    fine_tune_epochs = 25\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYPYHQUEt1-x"
      },
      "source": [
        "#### SSL MultiTask Only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jRiEht5st2cM",
        "outputId": "4ab94081-354b-4ee1-c6ce-8112ee1bedb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run directory: /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_multi\n",
            "Loaded pretrained backbone and projection head from saved run.\n",
            "\n",
            "--- K-Fold 1/5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 12:50:45,527] A new study created in memory with name: no-name-fd7e0c4d-7b8f-4190-bc87-9db59ca736b7\n",
            "[I 2025-10-07 12:50:46,575] Trial 0 finished with value: 1.4701033353805542 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.4701033353805542.\n",
            "[I 2025-10-07 12:50:46,960] Trial 1 finished with value: 1.4971279621124267 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.4701033353805542.\n",
            "[I 2025-10-07 12:50:47,365] Trial 2 finished with value: 1.4460347414016723 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:48,315] Trial 3 finished with value: 1.6173266649246216 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:49,933] Trial 4 finished with value: 1.6505090236663817 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:51,427] Trial 5 finished with value: 1.5188835859298706 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:52,775] Trial 6 finished with value: 1.634413242340088 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:54,445] Trial 7 finished with value: 1.6663888692855835 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:54,976] Trial 8 finished with value: 1.837188196182251 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:56,242] Trial 9 finished with value: 1.6526788234710694 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:57,250] Trial 10 finished with value: 1.6527145385742188 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:58,124] Trial 11 finished with value: 1.5876589298248291 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.12040035550822742, 'subsample': 0.5899164086425722, 'colsample_bytree': 0.5037653191432865}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:58,523] Trial 12 finished with value: 1.5519406080245972 and parameters: {'n_estimators': 53, 'max_depth': 8, 'learning_rate': 0.13001429961399444, 'subsample': 0.6417615206842633, 'colsample_bytree': 0.5788506755138203}. Best is trial 2 with value: 1.4460347414016723.\n",
            "[I 2025-10-07 12:50:59,628] Trial 13 finished with value: 1.3709022045135497 and parameters: {'n_estimators': 135, 'max_depth': 8, 'learning_rate': 0.11678558089474768, 'subsample': 0.8936108460389663, 'colsample_bytree': 0.6715143647973427}. Best is trial 13 with value: 1.3709022045135497.\n",
            "[I 2025-10-07 12:51:00,173] Trial 14 finished with value: 1.4067579507827759 and parameters: {'n_estimators': 111, 'max_depth': 8, 'learning_rate': 0.291266617275455, 'subsample': 0.8947385331597324, 'colsample_bytree': 0.6738167152482913}. Best is trial 13 with value: 1.3709022045135497.\n",
            "[I 2025-10-07 12:51:00,950] Trial 15 finished with value: 1.406356406211853 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.29596284206078843, 'subsample': 0.9234980616943328, 'colsample_bytree': 0.6827976655392337}. Best is trial 13 with value: 1.3709022045135497.\n",
            "[I 2025-10-07 12:51:04,077] Trial 16 finished with value: 1.6533766269683838 and parameters: {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.08719209000999652, 'subsample': 0.8975883235113203, 'colsample_bytree': 0.7787340845988605}. Best is trial 13 with value: 1.3709022045135497.\n",
            "[I 2025-10-07 12:51:07,933] Trial 17 finished with value: 1.5423847913742066 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.03025754636751209, 'subsample': 0.9040330172235821, 'colsample_bytree': 0.8957021983287922}. Best is trial 13 with value: 1.3709022045135497.\n",
            "[I 2025-10-07 12:51:08,656] Trial 18 finished with value: 1.562148642539978 and parameters: {'n_estimators': 178, 'max_depth': 6, 'learning_rate': 0.20088555632439606, 'subsample': 0.9955071842863534, 'colsample_bytree': 0.7050848480849484}. Best is trial 13 with value: 1.3709022045135497.\n",
            "[I 2025-10-07 12:51:10,577] Trial 19 finished with value: 1.633702325820923 and parameters: {'n_estimators': 265, 'max_depth': 9, 'learning_rate': 0.08954950751000706, 'subsample': 0.8695850876863044, 'colsample_bytree': 0.7822178954607859}. Best is trial 13 with value: 1.3709022045135497.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 1: {'n_estimators': 135, 'max_depth': 8, 'learning_rate': 0.11678558089474768, 'subsample': 0.8936108460389663, 'colsample_bytree': 0.6715143647973427}\n",
            "[Fold 1] RMSE: 3.8148, MAE: 2.7552, R: -0.8408\n",
            "\n",
            "--- K-Fold 2/5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 12:51:13,130] A new study created in memory with name: no-name-1563d4e3-8196-47d6-9030-0801be7d1ecf\n",
            "[I 2025-10-07 12:51:14,200] Trial 0 finished with value: 2.1902722120285034 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.1902722120285034.\n",
            "[I 2025-10-07 12:51:14,559] Trial 1 finished with value: 2.195744752883911 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.1902722120285034.\n",
            "[I 2025-10-07 12:51:14,977] Trial 2 finished with value: 2.1142508029937743 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.1142508029937743.\n",
            "[I 2025-10-07 12:51:15,925] Trial 3 finished with value: 2.0713748216629027 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.0713748216629027.\n",
            "[I 2025-10-07 12:51:17,568] Trial 4 finished with value: 2.0452786922454833 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 4 with value: 2.0452786922454833.\n",
            "[I 2025-10-07 12:51:19,051] Trial 5 finished with value: 1.996230459213257 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 1.996230459213257.\n",
            "[I 2025-10-07 12:51:20,443] Trial 6 finished with value: 2.2262870609760284 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 1.996230459213257.\n",
            "[I 2025-10-07 12:51:22,070] Trial 7 finished with value: 2.002614450454712 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 5 with value: 1.996230459213257.\n",
            "[I 2025-10-07 12:51:22,641] Trial 8 finished with value: 2.1291483640670776 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 5 with value: 1.996230459213257.\n",
            "[I 2025-10-07 12:51:23,837] Trial 9 finished with value: 2.136815977096558 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 5 with value: 1.996230459213257.\n",
            "[I 2025-10-07 12:51:25,404] Trial 10 finished with value: 2.0529899835586547 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.02964033652082882, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.5072567334400258}. Best is trial 5 with value: 1.996230459213257.\n",
            "[I 2025-10-07 12:51:27,962] Trial 11 finished with value: 2.0459393739700316 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.030241001410559513, 'subsample': 0.8751050092487717, 'colsample_bytree': 0.7758575504056592}. Best is trial 5 with value: 1.996230459213257.\n",
            "[I 2025-10-07 12:51:30,231] Trial 12 finished with value: 2.184788393974304 and parameters: {'n_estimators': 246, 'max_depth': 5, 'learning_rate': 0.08527855281875678, 'subsample': 0.8651265717543035, 'colsample_bytree': 0.7363570033289744}. Best is trial 5 with value: 1.996230459213257.\n",
            "[I 2025-10-07 12:51:32,583] Trial 13 finished with value: 2.0876817226409914 and parameters: {'n_estimators': 249, 'max_depth': 2, 'learning_rate': 0.020979549542327804, 'subsample': 0.8738214287354175, 'colsample_bytree': 0.9023571111374418}. Best is trial 5 with value: 1.996230459213257.\n",
            "[I 2025-10-07 12:51:34,488] Trial 14 finished with value: 2.0145817518234255 and parameters: {'n_estimators': 158, 'max_depth': 8, 'learning_rate': 0.04924185908787293, 'subsample': 0.6372424866412404, 'colsample_bytree': 0.5306510341863633}. Best is trial 5 with value: 1.996230459213257.\n",
            "[I 2025-10-07 12:51:35,141] Trial 15 finished with value: 1.9872940540313722 and parameters: {'n_estimators': 253, 'max_depth': 4, 'learning_rate': 0.29596284206078843, 'subsample': 0.9234980616943328, 'colsample_bytree': 0.6608574886555885}. Best is trial 15 with value: 1.9872940540313722.\n",
            "[I 2025-10-07 12:51:35,999] Trial 16 finished with value: 2.103871989250183 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.22814032285040356, 'subsample': 0.9332805919923572, 'colsample_bytree': 0.6561174679610582}. Best is trial 15 with value: 1.9872940540313722.\n",
            "[I 2025-10-07 12:51:36,559] Trial 17 finished with value: 2.0484763860702513 and parameters: {'n_estimators': 227, 'max_depth': 3, 'learning_rate': 0.26663748121620445, 'subsample': 0.9957893593351752, 'colsample_bytree': 0.6803710106053958}. Best is trial 15 with value: 1.9872940540313722.\n",
            "[I 2025-10-07 12:51:38,061] Trial 18 finished with value: 2.1786056995391845 and parameters: {'n_estimators': 178, 'max_depth': 5, 'learning_rate': 0.11572776890757983, 'subsample': 0.7317366662416382, 'colsample_bytree': 0.7985248393876554}. Best is trial 15 with value: 1.9872940540313722.\n",
            "[I 2025-10-07 12:51:38,631] Trial 19 finished with value: 2.299046492576599 and parameters: {'n_estimators': 271, 'max_depth': 2, 'learning_rate': 0.13436560448953488, 'subsample': 0.9212357456243315, 'colsample_bytree': 0.5630014917270565}. Best is trial 15 with value: 1.9872940540313722.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 2: {'n_estimators': 253, 'max_depth': 4, 'learning_rate': 0.29596284206078843, 'subsample': 0.9234980616943328, 'colsample_bytree': 0.6608574886555885}\n",
            "[Fold 2] RMSE: 2.0388, MAE: 1.3231, R: -0.0021\n",
            "\n",
            "--- K-Fold 3/5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 12:51:40,961] A new study created in memory with name: no-name-9c3f95f0-dea4-4d0f-a31a-087be2ccfc60\n",
            "[I 2025-10-07 12:51:42,075] Trial 0 finished with value: 1.9337733507156372 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.9337733507156372.\n",
            "[I 2025-10-07 12:51:42,436] Trial 1 finished with value: 1.929962146282196 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:42,851] Trial 2 finished with value: 2.114901876449585 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:43,797] Trial 3 finished with value: 2.075519347190857 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:45,496] Trial 4 finished with value: 2.087763261795044 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:46,929] Trial 5 finished with value: 1.9794597029685974 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:48,420] Trial 6 finished with value: 2.3895811080932616 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:50,155] Trial 7 finished with value: 2.051478850841522 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:50,720] Trial 8 finished with value: 2.2739287853240966 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:51,970] Trial 9 finished with value: 1.9787000179290772 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:53,135] Trial 10 finished with value: 2.608278250694275 and parameters: {'n_estimators': 139, 'max_depth': 7, 'learning_rate': 0.27047297227177763, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9168158328299749}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:54,538] Trial 11 finished with value: 2.1021482825279234 and parameters: {'n_estimators': 138, 'max_depth': 10, 'learning_rate': 0.1286674737306269, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.8416909650214576}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:55,866] Trial 12 finished with value: 2.124895977973938 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.13116982721498638, 'subsample': 0.8837864985996776, 'colsample_bytree': 0.8576737848754852}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:56,743] Trial 13 finished with value: 2.0479438066482545 and parameters: {'n_estimators': 110, 'max_depth': 6, 'learning_rate': 0.29838444194171054, 'subsample': 0.6412396842185336, 'colsample_bytree': 0.8229282203516206}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:51:59,366] Trial 14 finished with value: 2.0121572971343995 and parameters: {'n_estimators': 172, 'max_depth': 8, 'learning_rate': 0.1062272193666865, 'subsample': 0.7917316769460038, 'colsample_bytree': 0.7698100534013989}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:52:00,441] Trial 15 finished with value: 1.9940475821495056 and parameters: {'n_estimators': 292, 'max_depth': 2, 'learning_rate': 0.08962717626942573, 'subsample': 0.9167270621070531, 'colsample_bytree': 0.5015731892181307}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:52:01,061] Trial 16 finished with value: 2.1516726136207582 and parameters: {'n_estimators': 50, 'max_depth': 9, 'learning_rate': 0.178465268046373, 'subsample': 0.8317032143059913, 'colsample_bytree': 0.9219900508555974}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:52:02,419] Trial 17 finished with value: 1.9748562574386597 and parameters: {'n_estimators': 167, 'max_depth': 6, 'learning_rate': 0.034827497346874084, 'subsample': 0.731057379200564, 'colsample_bytree': 0.6803710106053958}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:52:03,366] Trial 18 finished with value: 2.2690988063812254 and parameters: {'n_estimators': 121, 'max_depth': 5, 'learning_rate': 0.20284401171141872, 'subsample': 0.5827781406926889, 'colsample_bytree': 0.78705435593608}. Best is trial 1 with value: 1.929962146282196.\n",
            "[I 2025-10-07 12:52:03,721] Trial 19 finished with value: 2.124632549285889 and parameters: {'n_estimators': 82, 'max_depth': 2, 'learning_rate': 0.08716454446567826, 'subsample': 0.6869888528707923, 'colsample_bytree': 0.9072950643400716}. Best is trial 1 with value: 1.929962146282196.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 3: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}\n",
            "[Fold 3] RMSE: 1.2160, MAE: 0.8217, R: 0.7897\n",
            "\n",
            "--- K-Fold 4/5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 12:52:06,050] A new study created in memory with name: no-name-3589d897-702a-431a-8fc7-4f35ea4b608e\n",
            "[I 2025-10-07 12:52:07,136] Trial 0 finished with value: 1.865040910243988 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.865040910243988.\n",
            "[I 2025-10-07 12:52:07,485] Trial 1 finished with value: 1.8159724950790406 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.8159724950790406.\n",
            "[I 2025-10-07 12:52:07,903] Trial 2 finished with value: 1.8949223160743713 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 1.8159724950790406.\n",
            "[I 2025-10-07 12:52:08,483] Trial 3 finished with value: 1.8738661766052247 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 1.8159724950790406.\n",
            "[I 2025-10-07 12:52:09,493] Trial 4 finished with value: 1.767871117591858 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 4 with value: 1.767871117591858.\n",
            "[I 2025-10-07 12:52:10,554] Trial 5 finished with value: 1.760578215122223 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 5 with value: 1.760578215122223.\n",
            "[I 2025-10-07 12:52:12,887] Trial 6 finished with value: 1.99468936920166 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 5 with value: 1.760578215122223.\n",
            "[I 2025-10-07 12:52:15,001] Trial 7 finished with value: 1.737834906578064 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 7 with value: 1.737834906578064.\n",
            "[I 2025-10-07 12:52:15,577] Trial 8 finished with value: 2.0261118412017822 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 7 with value: 1.737834906578064.\n",
            "[I 2025-10-07 12:52:16,795] Trial 9 finished with value: 1.860806930065155 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 7 with value: 1.737834906578064.\n",
            "[I 2025-10-07 12:52:18,990] Trial 10 finished with value: 1.9190417289733888 and parameters: {'n_estimators': 295, 'max_depth': 7, 'learning_rate': 0.023413669026819843, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8200442512337782}. Best is trial 7 with value: 1.737834906578064.\n",
            "[I 2025-10-07 12:52:20,860] Trial 11 finished with value: 1.6550440073013306 and parameters: {'n_estimators': 273, 'max_depth': 6, 'learning_rate': 0.030241001410559513, 'subsample': 0.8751050092487717, 'colsample_bytree': 0.5065883662316701}. Best is trial 11 with value: 1.6550440073013306.\n",
            "[I 2025-10-07 12:52:23,620] Trial 12 finished with value: 1.8030891418457031 and parameters: {'n_estimators': 299, 'max_depth': 7, 'learning_rate': 0.025228964120157658, 'subsample': 0.8968427783065984, 'colsample_bytree': 0.7320276236813734}. Best is trial 11 with value: 1.6550440073013306.\n",
            "[I 2025-10-07 12:52:25,832] Trial 13 finished with value: 1.7115851640701294 and parameters: {'n_estimators': 260, 'max_depth': 5, 'learning_rate': 0.017874942019846893, 'subsample': 0.8741879020349066, 'colsample_bytree': 0.502954166654256}. Best is trial 11 with value: 1.6550440073013306.\n",
            "[I 2025-10-07 12:52:28,301] Trial 14 finished with value: 1.708729898929596 and parameters: {'n_estimators': 260, 'max_depth': 6, 'learning_rate': 0.03423200862435618, 'subsample': 0.8947385331597324, 'colsample_bytree': 0.507301909215697}. Best is trial 11 with value: 1.6550440073013306.\n",
            "[I 2025-10-07 12:52:29,382] Trial 15 finished with value: 1.683490014076233 and parameters: {'n_estimators': 156, 'max_depth': 8, 'learning_rate': 0.03208899494279826, 'subsample': 0.9234980616943328, 'colsample_bytree': 0.5008073776934018}. Best is trial 11 with value: 1.6550440073013306.\n",
            "[I 2025-10-07 12:52:30,678] Trial 16 finished with value: 1.7823699235916137 and parameters: {'n_estimators': 152, 'max_depth': 8, 'learning_rate': 0.04129058543404059, 'subsample': 0.997485475833634, 'colsample_bytree': 0.6643979885912391}. Best is trial 11 with value: 1.6550440073013306.\n",
            "[I 2025-10-07 12:52:32,328] Trial 17 finished with value: 1.7214268684387206 and parameters: {'n_estimators': 138, 'max_depth': 8, 'learning_rate': 0.04196921103920749, 'subsample': 0.9334415664289303, 'colsample_bytree': 0.9980084946806067}. Best is trial 11 with value: 1.6550440073013306.\n",
            "[I 2025-10-07 12:52:33,673] Trial 18 finished with value: 1.797831130027771 and parameters: {'n_estimators': 174, 'max_depth': 9, 'learning_rate': 0.08031330579472766, 'subsample': 0.8445779114924311, 'colsample_bytree': 0.5543780080628663}. Best is trial 11 with value: 1.6550440073013306.\n",
            "[I 2025-10-07 12:52:34,391] Trial 19 finished with value: 1.8537391424179077 and parameters: {'n_estimators': 121, 'max_depth': 8, 'learning_rate': 0.2702272241241035, 'subsample': 0.9238552199363875, 'colsample_bytree': 0.8202139292383122}. Best is trial 11 with value: 1.6550440073013306.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 4: {'n_estimators': 273, 'max_depth': 6, 'learning_rate': 0.030241001410559513, 'subsample': 0.8751050092487717, 'colsample_bytree': 0.5065883662316701}\n",
            "[Fold 4] RMSE: 3.1157, MAE: 2.6016, R: -0.2285\n",
            "\n",
            "--- K-Fold 5/5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 12:52:37,187] A new study created in memory with name: no-name-d5ae60c3-c002-462f-b6d1-edc2083d5fb4\n",
            "[I 2025-10-07 12:52:38,794] Trial 0 finished with value: 1.9399768590927124 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.9399768590927124.\n",
            "[I 2025-10-07 12:52:39,908] Trial 1 finished with value: 2.027738428115845 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.9399768590927124.\n",
            "[I 2025-10-07 12:52:41,221] Trial 2 finished with value: 1.8075245380401612 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.8075245380401612.\n",
            "[I 2025-10-07 12:52:41,916] Trial 3 finished with value: 1.8983506917953492 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.8075245380401612.\n",
            "[I 2025-10-07 12:52:42,961] Trial 4 finished with value: 1.9202093124389648 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.8075245380401612.\n",
            "[I 2025-10-07 12:52:43,916] Trial 5 finished with value: 1.9198862314224243 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.8075245380401612.\n",
            "[I 2025-10-07 12:52:45,299] Trial 6 finished with value: 1.9338036894798278 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 1.8075245380401612.\n",
            "[I 2025-10-07 12:52:46,968] Trial 7 finished with value: 1.9584956169128418 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 1.8075245380401612.\n",
            "[I 2025-10-07 12:52:47,583] Trial 8 finished with value: 1.8835987210273744 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 1.8075245380401612.\n",
            "[I 2025-10-07 12:52:48,858] Trial 9 finished with value: 1.8530445575714112 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 1.8075245380401612.\n",
            "[I 2025-10-07 12:52:49,933] Trial 10 finished with value: 1.7320837497711181 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 10 with value: 1.7320837497711181.\n",
            "[I 2025-10-07 12:52:51,041] Trial 11 finished with value: 1.8232603788375854 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.28009047436880896, 'subsample': 0.5199086519616609, 'colsample_bytree': 0.8692027723625104}. Best is trial 10 with value: 1.7320837497711181.\n",
            "[I 2025-10-07 12:52:51,537] Trial 12 finished with value: 1.9295009136199952 and parameters: {'n_estimators': 58, 'max_depth': 8, 'learning_rate': 0.14196217019146598, 'subsample': 0.5149340702175954, 'colsample_bytree': 0.8370887965872971}. Best is trial 10 with value: 1.7320837497711181.\n",
            "[I 2025-10-07 12:52:52,985] Trial 13 finished with value: 1.9449363708496095 and parameters: {'n_estimators': 132, 'max_depth': 8, 'learning_rate': 0.29889801029971286, 'subsample': 0.6095659107531833, 'colsample_bytree': 0.7760131200377596}. Best is trial 10 with value: 1.7320837497711181.\n",
            "[I 2025-10-07 12:52:55,034] Trial 14 finished with value: 1.8745082378387452 and parameters: {'n_estimators': 119, 'max_depth': 8, 'learning_rate': 0.1125273525564678, 'subsample': 0.6100493659926688, 'colsample_bytree': 0.9818757669341631}. Best is trial 10 with value: 1.7320837497711181.\n",
            "[I 2025-10-07 12:52:56,209] Trial 15 finished with value: 1.8728927373886108 and parameters: {'n_estimators': 292, 'max_depth': 9, 'learning_rate': 0.17234746738517984, 'subsample': 0.5937663157166836, 'colsample_bytree': 0.5016444316932455}. Best is trial 10 with value: 1.7320837497711181.\n",
            "[I 2025-10-07 12:52:56,656] Trial 16 finished with value: 1.7606319427490233 and parameters: {'n_estimators': 51, 'max_depth': 6, 'learning_rate': 0.08749102207565247, 'subsample': 0.5638406774527164, 'colsample_bytree': 0.9219900508555974}. Best is trial 10 with value: 1.7320837497711181.\n",
            "[I 2025-10-07 12:52:57,942] Trial 17 finished with value: 1.8788256406784059 and parameters: {'n_estimators': 165, 'max_depth': 6, 'learning_rate': 0.031405615672261515, 'subsample': 0.5494834187575843, 'colsample_bytree': 0.9069445451545826}. Best is trial 10 with value: 1.7320837497711181.\n",
            "[I 2025-10-07 12:52:59,738] Trial 18 finished with value: 1.896301007270813 and parameters: {'n_estimators': 172, 'max_depth': 7, 'learning_rate': 0.08335206810371563, 'subsample': 0.6660427411738311, 'colsample_bytree': 0.9261108860983648}. Best is trial 10 with value: 1.7320837497711181.\n",
            "[I 2025-10-07 12:53:00,559] Trial 19 finished with value: 1.8523404359817506 and parameters: {'n_estimators': 116, 'max_depth': 7, 'learning_rate': 0.03525172101730198, 'subsample': 0.5632603029202694, 'colsample_bytree': 0.7937116697296034}. Best is trial 10 with value: 1.7320837497711181.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 5: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}\n",
            "[Fold 5] RMSE: 3.2419, MAE: 2.3254, R: 0.3234\n",
            "\n",
            "=== Fold Metrics Summary ===\n",
            "      fold      RMSE       MAE        R2\n",
            "0      1.0  3.814776  2.755160 -0.840760\n",
            "1      2.0  2.038832  1.323095 -0.002117\n",
            "2      3.0  1.216004  0.821721  0.789738\n",
            "3      4.0  3.115711  2.601593 -0.228472\n",
            "4      5.0  3.241856  2.325407  0.323378\n",
            "Mean   3.0  2.685436  1.965395  0.008354\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0wAAAIjCAYAAAAwSJuMAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAA2u5JREFUeJzs3XdYU2cbwOFfEsLeAk4E3HvvVUfdWrd1i9vW7vV1q63de7rq3rut1lHrqHvvjQqKioKDLRDI+f44BUWGoIYT4Lmv61yS5M3Jkzc5xzznXTpFURSEEEIIIYQQQmSg1zoAIYQQQgghhLBWkjAJIYQQQgghRBYkYRJCCCGEEEKILEjCJIQQQgghhBBZkIRJCCGEEEIIIbIgCZMQQgghhBBCZEESJiGEEEIIIYTIgiRMQgghhBBCCJEFSZiEEEIIIYQQIguSMAkh8r3Zs2ej0+kICQl5aFl/f38CAwMtHlNBEBISgk6nY/bs2VqHYjUCAwPx9/fP09eMjY3Fx8eHBQsW5OnrCu2ZTCZ8fX359ddftQ5FiEJNEiYhxCNLTVQOHDiQ7v6oqCgaNGiAvb0969evz/a5mW1vv/12XoSfrQdjcnV15amnnuKvv/7SOrRCITAwMK3e7969m+HxoKCgtM/m66+/zvX+4+PjmTBhAlu3bn0C0VrWDz/8gIuLC/369Uu7b8KECem+n0ajEX9/f1566SUiIyMz7MPf3x+dTsfTTz+d6WtMnz49bV8PHs87duygY8eOlCxZEnt7e0qXLk3Xrl1ZuHBhunJZHc86nY6xY8c+0ntPff7IkSMzffy9995LK3Pz5s1My/Tt2xedTsf//ve/TB/funVrtrEvXrw413E/uE+DwYCPjw+9e/fm9OnT6cpu2bKFpk2b8tRTT1GlShUmTZqU9pjRaOS1117jk08+ISEhIddxCCGeDButAxBCFCzR0dG0a9eOY8eOsWrVKjp06JBt+Y8++oiAgIB091WrVs2SIeZY27ZtGTJkCIqicOnSJSZPnkzXrl1Zt24d7du31zo8i/Pz8+Pu3bsYjUZNXt/Gxob4+HhWr15N37590z22YMEC7O3tH/lHZHx8PBMnTgSgZcuWOX7e9OnTMZvNj/Saj8JkMvHDDz/w6quvYjAYMjw+efJknJ2diYuLY9OmTfz0008cOnSIHTt2ZChrb2/Pli1buH79OsWKFUv3WFb1uWzZMp599llq1arFyy+/jIeHB8HBwWzbto3p06czYMCAdOVTj5kHVahQ4VHeflrcK1as4Ndff8XW1jbdY4sWLcr2exAdHc3q1avx9/dn0aJFfP755+h0ukzLvvTSS9SvXz/D/Y0bN37k2FP3aTKZOHbsGFOmTGHr1q2cOHEi7TOoUKECf//9N05OToSFhVGmTBmaN2/OU089BcCwYcN4++23WbhwIcOHD3/kWIQQj04SJiHEExMTE0P79u05cuQIK1eupGPHjg99TseOHalXr14eRJd7FSpUYNCgQWm3e/XqRZUqVfjhhx/yPGGKi4vDyckpT19Tp9Nhb2+fp695Pzs7O5o2bcqiRYsyJEwLFy6kc+fOrFixIk9iSa3/vE4e16xZQ0RERIb3n6p37954eXkBMGbMGPr168eSJUvYt28fDRo0SFe2adOm7N+/nyVLlvDyyy+n3X/lyhW2b99Ojx49MtTnhAkTqFKlCnv27MmQrISHh2eI58Fj5kno0KEDf/75J+vWraNbt25p9+/atYvg4GB69eqV5fdgxYoVpKSkMHPmTFq3bs22bdvSEpEHNW/enN69ez/R2B/cZ8WKFXnuueeYO3cub731FgAlS5ZMe1yn02E2m9Hr73UAcnd3p127dsyePVsSJiE0Il3yhBBPRGxsLB06dODQoUOsWLGCzp07P5H9bt68mebNm+Pk5IS7uzvdunXL0KUlM4qiMGnSJEqVKoWjoyOtWrXi5MmTjxVL5cqV8fLy4sKFC+nuT0xMZPz48ZQrVw47Ozt8fX156623SExMTFfu7t27vPTSS3h5eeHi4sIzzzzD1atX0el0TJgwIa1canerU6dOMWDAADw8PGjWrFna4/Pnz6du3bo4ODjg6elJv379CA0NTfdaQUFB9OrVi2LFimFvb0+pUqXo168fUVFRaWU2btxIs2bNcHd3x9nZmYoVK/Luu++mPZ7VGKacfCap7+H8+fMEBgbi7u6Om5sbw4YNIz4+Psd1PmDAANatW5eum9n+/fsJCgrK0LqRKjIykldeeQVfX1/s7OwoV64cX3zxRVrLUEhICN7e3gBMnDgxrdtU6mcQGBiIs7MzFy5coFOnTri4uDBw4MC0xx4cw2Q2m/nhhx+oXr069vb2eHt706FDh3Rd2x5W11n5/fff8ff3p2zZsjmqr+bNmwNk+I6C2lLTs2fPDF3pFi1ahIeHR6YXAS5cuED9+vUzJEsAPj4+OYrpcZUsWZIWLVpkiHvBggVUr1492xbpBQsW0LZtW1q1akXlypU1HweW3eeTnJzMkCFD6Ny5c1q5VG3btmXHjh3cvn07T+IUQqQnLUxCiMcWFxdHx44d2b9/P8uXL6dLly45fm5UVFSGsQepV8z/+ecfOnbsSJkyZZgwYQJ3797lp59+omnTphw6dCjbwfcffvghkyZNolOnTnTq1IlDhw7Rrl07kpKSHuk9psZ6586ddD9ezWYzzzzzDDt27GD06NFUrlyZ48eP891333Hu3Dl+//33tLKBgYEsXbqUwYMH06hRI/79999sE8s+ffpQvnx5Pv30UxRFAeCTTz7hgw8+oG/fvowcOZKIiAh++uknWrRoweHDh3F3dycpKYn27duTmJjIiy++SLFixbh69Spr1qwhMjISNzc3Tp48SZcuXahRowYfffQRdnZ2nD9/np07d2ZbB7n9TPr27UtAQACfffYZhw4d4rfffsPHx4cvvvgiR3Xes2dPxo4dy8qVK9Ouri9cuJBKlSpRp06dDOXj4+N56qmnuHr1KmPGjKF06dLs2rWLd955h7CwML7//nu8vb2ZPHkyzz33HD169KBnz54A1KhRI20/ycnJtG/fnmbNmvH111/j6OiYZYwjRoxg9uzZdOzYkZEjR5KcnMz27dvZs2cP9erVe+S6BrUVJbP3mZXUiU88PDwyfXzAgAG0a9eOCxcupH2PFy5cSO/evTNtPfPz82PTpk1cuXKFUqVKPfT1ExISMh1L5OrqmmnSlVMDBgzg5ZdfJjY2FmdnZ5KTk1m2bBmvvfZalt3xrl27xpYtW5gzZw4A/fv357vvvuPnn3/ONJaYmJhMYy9SpEiW3fhyK6vPx2w2M2zYMGJjY1m1alWG59WtWxdFUdi1a1euzq9CiCdEEUKIRzRr1iwFUPz8/BSj0aj8/vvvuX5uZluqWrVqKT4+PsqtW7fS7jt69Kii1+uVIUOGZNhXcHCwoiiKEh4ertja2iqdO3dWzGZzWrl3331XAZShQ4c+ND5AGTFihBIREaGEh4crBw4cUDp06KAAyldffZVWbt68eYper1e2b9+e7vlTpkxRAGXnzp2KoijKwYMHFUB55ZVX0pULDAxUAGX8+PFp940fP14BlP79+6crGxISohgMBuWTTz5Jd//x48cVGxubtPsPHz6sAMqyZcuyfH/fffedAigRERFZlgkODlYAZdasWWn35fQzSX0Pw4cPT7fPHj16KEWKFMnyNVMNHTpUcXJyUhRFUXr37q20adNGURRFSUlJUYoVK6ZMnDgxLb77P4+PP/5YcXJyUs6dO5duf2+//bZiMBiUy5cvK4qiKBERERnq/f7XBpS3334708f8/PzSbm/evFkBlJdeeilD2dTvXk7qOjMmk0nR6XTK66+/nuGx1Po9e/asEhERoYSEhCgzZ85UHBwcFG9vbyUuLi5deT8/P6Vz585KcnKyUqxYMeXjjz9WFEVRTp06pQDKv//+m3Yc7d+/P+15M2bMUADF1tZWadWqlfLBBx8o27dvV1JSUjLElNXxDCiLFi3K1Xu/f5/jxo1Tbt++rdja2irz5s1TFEVR/vrrL0Wn0ykhISFpdfFg/X799deKg4ODEh0drSiKopw7d04BlFWrVqUrt2XLlmxjDwsLy3XcqfucOXOmEhERoVy7dk1Zv369Uq5cOUWn0yn79u1LK5uSkqIMHjxYadOmjRITE5Pp/q5du6YAyhdffJHrWIQQj0+65AkhHtuNGzewt7fH19c318/95Zdf2LhxY7oNICwsjCNHjhAYGIinp2da+Ro1atC2bVvWrl2b5T7/+ecfkpKSePHFF9NdGX7llVdyFduMGTPw9vbGx8eHevXqsWnTJt566y1ee+21tDLLli2jcuXKVKpUiZs3b6ZtrVu3BtQZsIC02QKff/75dK/x4osvZvn6D84stnLlSsxmM3379k33WsWKFaN8+fJpr+Xm5gbAhg0bsuz+5u7uDsAff/yR40kMHuUzefA9NG/enFu3bhEdHZ2j1wS1dWHr1q1cv36dzZs3c/369Sy74y1btozmzZvj4eGRro6efvppUlJS2LZtW45f97nnnntomRUrVqDT6Rg/fnyGx1K/e49S1wC3b99GUZQsW4tAHRPj7e2Nv78/w4cPp1y5cqxbty7LFjGDwUDfvn1ZtGgRoHZZ8/X1zdAFLNXw4cNZv349LVu2ZMeOHXz88cc0b96c8uXLs2vXrgzlu3XrluF43rhxI61atcrx+86Mh4cHHTp0SIt74cKFNGnSBD8/vyyfs2DBAjp37oyLiwsA5cuXp27dull2y/vwww8zjf3+73puDR8+HG9vb0qUKEGHDh2Iiopi3rx56SaXmDFjBvPmzSM+Pp4uXbrQsmXLDK1Mqd+BrGYCFEJYlnTJE0I8tqlTp/Laa6/RoUMHtm/fTsWKFQFISUkhIiIiXVlPT8903WEaNGiQ6aQPly5dAkjb1/0qV67Mhg0bspwIIfW55cuXT3e/t7d3tj8+H9StWzdeeOEFkpKS2L9/P59++inx8fHpBmQHBQVx+vTptDExD0odGH/p0iX0en2GGQHLlSuX5es/WDYoKAhFUTK8r1SpXaoCAgJ47bXX+Pbbb1mwYAHNmzfnmWeeYdCgQWnJ1LPPPstvv/3GyJEjefvtt2nTpg09e/akd+/e6d7f/R7lMyldunS6cqn1f+fOHVxdXbN87/dLHUe0ZMkSjhw5Qv369SlXrlym624FBQVx7Nixh34eD2NjY5OjLmgXLlygRIkS2f6ofpS6vp/yX3fMzKxYsQJXV1ciIiL48ccfCQ4OxsHBIdv9DRgwgB9//JGjR4+ycOFC+vXrl22Xs/bt29O+fXvi4+M5ePAgS5YsYcqUKXTp0oUzZ86kG8tUqlSpLKcuf1wDBgxg8ODBXL58md9//50vv/wyy7KnT5/m8OHDDBkyhPPnz6fd37JlS3755Reio6MzfP+qV6/+xGP/8MMPad68eVpXu8WLF2f4zEeNGsWoUaOy3U/qd+BJdQ0UQuSOJExCiMdWpUoV1q5dS5s2bWjbti07d+7E19eX0NDQDD/6t2zZkqtpnLV0/4+/Tp064eXlxQsvvECrVq3Sxr2YzWaqV6/Ot99+m+k+HqXVLdWDP3zNZjM6nY5169ZlOsW0s7Nz2t/ffPMNgYGB/PHHH/z999+89NJLfPbZZ+zZs4dSpUrh4ODAtm3b2LJlC3/99Rfr169nyZIltG7dmr///jvT/T+KrPaTXRLwIDs7O3r27MmcOXO4ePFiugkyHmQ2m2nbtm3aDGQPyun01nZ2djlKZnLiUeva09MTnU7HnTt3stx3ixYt0sb8de3alerVqzNw4EAOHjyYZfwNGzakbNmyvPLKKwQHB2fZWvcgR0dHmjdvTvPmzfHy8mLixImsW7eOoUOH5uj5j+uZZ57Bzs6OoUOHkpiYmOXMgaBOjALw6quv8uqrr2Z4fMWKFQwbNsxisaa6Pwnr3r078fHxjBo1imbNmuXq3JD6HUj9rIUQeUsSJiHEE9GgQQN+//13OnfuTNu2bdm+fTvFihVL62KXqmbNmjnaX2pXm7Nnz2Z47MyZM3h5eWU5zXbqc4OCgihTpkza/REREdn++HyYMWPG8N133/H+++/To0cPdDodZcuW5ejRo7Rp0ybbq79+fn6YzWaCg4PTtRDdf/X7YcqWLYuiKAQEBOToh3/16tWpXr0677//Prt27aJp06ZMmTIlbWFMvV5PmzZtaNOmDd9++y2ffvop7733Hlu2bMn0SvvjfCaPa8CAAcycORO9Xp9uAdcHlS1bltjY2Ie2FDypK/Vly5Zlw4YN3L59O9tWptzWNaitXGXLliU4ODhHsTg7OzN+/HiGDRvG0qVLs62n/v37M2nSJCpXrkytWrVytP/7pbYKh4WF5fq5j8rBwYHu3bszf/58OnbsmGXyoCgKCxcupFWrVhm6wAJ8/PHHLFiwIE8Spgd9/vnnrFq1ik8++YQpU6bk+Hmp34HKlStbKjQhRDZkDJMQ4olp06YNixYt4vz583To0IGkpCSefvrpdFtOu8QVL16cWrVqMWfOnHRTSp84cYK///6bTp06Zfncp59+GqPRyE8//ZSuJeP7779/1LcGqD9gX3/9dU6fPs0ff/wBqLPAXb16lenTp2cof/fuXeLi4gDSpmz+9ddf05X56aefcvz6PXv2xGAwMHHixAwtNIqicOvWLUBdrDM5OTnd49WrV0ev16dNdZ7Z9MSpP5wfnA491eN8Jo+rVatWfPzxx/z8888ZFl29X9++fdm9ezcbNmzI8FhkZGRavaSO8bn/fTyKXr16oShK2iK490v9jB6lrlM1btw43fTkDzNw4EBKlSr10FkIR44cyfjx4/nmm2+yLbdp06ZM708dr5ZZ90xLeuONNxg/fjwffPBBlmV27txJSEgIw4YNo3fv3hm2Z599li1btnDt2rU8jFxVtmxZevXqxezZs7l+/XqOn3fw4EF0Ot1jLaIrhHh00sIkhHiievTowfTp0xk+fDjPPPMM69evf+TFT7/66is6duxI48aNGTFiRNoU1m5ubtl2y/L29uaNN97gs88+o0uXLnTq1InDhw+zbt26x+7SEhgYyIcffsgXX3xB9+7dGTx4MEuXLmXs2LFs2bKFpk2bkpKSwpkzZ1i6dCkbNmygXr161K1bl169evH9999z69attGnFz507B+SsxaNs2bJMmjSJd955h5CQELp3746LiwvBwcGsWrWK0aNH88Ybb7B582ZeeOEF+vTpQ4UKFUhOTmbevHkYDAZ69eoFwEcffcS2bdvo3Lkzfn5+hIeH8+uvv1KqVKl0az496FE/k8el1+t5//33H1ruzTff5M8//6RLly4EBgZSt25d4uLiOH78OMuXLyckJAQvLy8cHByoUqUKS5YsoUKFCnh6elKtWrVs1/TJTKtWrRg8eDA//vgjQUFBdOjQAbPZzPbt22nVqhUvvPDCI9c1qOPo5s2bx7lz53LUqmg0Gnn55Zd58803Wb9+PR06dMi0nJ+fX44+r27duhEQEEDXrl0pW7YscXFx/PPPP6xevZr69evTtWvXdOXPnTuX1h3ufkWLFqVt27YAbN26lVatWjF+/Phcf2dq1qz50FbqBQsWYDAYspyy/5lnnuG9995j8eLF6SZw2b59e6ZTlNeoUSNtyvkJEyYwceLEx+pa/Oabb7J06VK+//57Pv/88xw9Z+PGjTRt2pQiRYo80msKIR6TNpPzCSEKgsymIU719ddfK4DSpUsXxWQy5eq59/vnn3+Upk2bKg4ODoqrq6vStWtX5dSpU5nuK3VacUVRp+qdOHGiUrx4ccXBwUFp2bKlcuLECcXPzy/H04qPGzcu08cmTJigAMqWLVsURVGUpKQk5YsvvlCqVq2q2NnZKR4eHkrdunWViRMnKlFRUWnPi4uLU8aNG6d4enoqzs7OSvfu3ZWzZ88qgPL555+nlctqmuRUK1asUJo1a6Y4OTkpTk5OSqVKlZRx48YpZ8+eVRRFUS5evKgMHz5cKVu2rGJvb694enoqrVq1Uv7555+0fWzatEnp1q2bUqJECcXW1lYpUaKE0r9//3TTcWc2rbii5Owzyeo9ZPZZZeb+acWzktm04oqiKDExMco777yjlCtXTrG1tVW8vLyUJk2aKF9//bWSlJSUVm7Xrl1K3bp1FVtb23RTjGf32g9OK64oipKcnKx89dVXSqVKlRRbW1vF29tb6dixo3Lw4EFFUXJW11lJTExUvLy80qYBT5XddyQqKkpxc3NTnnrqqbT7UqcVz05mx+SiRYuUfv36KWXLllUcHBwUe3t7pUqVKsp7772XNl13KrKZmvv+WFavXq0AypQpUx76/rM7DlPdXxdJSUlKkSJFlObNm2f7nICAAKV27dqKojx8WvH7p55//fXXFZ1Op5w+fTrb/afuM6up/Vu2bKm4uroqkZGR2e5HURQlMjJSsbW1VX777beHlhVCWIZOUXIx8lYIIcQTdeTIEWrXrs38+fMZOHCg1uEIK/Txxx8za9YsgoKCnthkHFp666230rru2tnZaR1OrjRo0AA/Pz+WLVuWZ6/5/fff8+WXX3LhwoWHzoAohLAMGcMkhBB55O7duxnu+/7779Hr9bRo0UKDiER+8OqrrxIbG8vixYu1DuWJ2LJlCx988EG+S5aio6M5evQoH330UZ69pslk4ttvv+X999+XZEkIDUkLkxBC5JGJEydy8OBBWrVqhY2NDevWrWPdunWMHj2aqVOnah2eEEIIITIhCZMQQuSRjRs3MnHiRE6dOkVsbCylS5dm8ODBvPfee9jYyBw8QgghhDWShEkIIYQQQgghsiBjmIQQQgghhBAiC5IwCSGEEEIIIUQWClWnebPZzLVr13BxccnRIpFCCCGEEEKIgklRFGJiYihRogR6fdbtSPk2Yfr888955513ePnll/n+++9z9Jxr167h6+tr2cCEEEIIIYQQ+UZoaCilSpXK8vF8mTDt37+fqVOnUqNGjVw9z8XFBVArxdXV1RKh5ZjJZOLvv/+mXbt2GI1GTWMpiKR+LUvq17Kkfi1L6teypH4tS+rXsqR+Lc+a6jg6OhpfX9+0HCEr+S5hio2NZeDAgUyfPp1Jkybl6rmp3fBcXV2tImFydHTE1dVV8y9LQST1a1lSv5Yl9WtZUr+WJfVrWVK/liX1a3nWWMcPG6qT7xKmcePG0blzZ55++umHJkyJiYkkJiam3Y6OjgbUD8pkMlk0zodJfX2t4yiopH4tS+rXsqR+LUvq17Kkfi1L6teypH4tz5rqOKcx5Kt1mBYvXswnn3zC/v37sbe3p2XLltSqVSvLMUwTJkxg4sSJGe5fuHAhjo6OFo5WCCGEEEIIYa3i4+MZMGAAUVFR2fY+yzcJU2hoKPXq1WPjxo1pY5celjBl1sLk6+vLzZs3raJL3saNG2nbtq3VNEcWJFK/liX1a1lSv5Yl9WtZUr+WJfVrWVK/lmdNdRwdHY2Xl9dDE6Z80yXv4MGDhIeHU6dOnbT7UlJS2LZtGz///DOJiYkYDIZ0z7Gzs8POzi7DvoxGo+YfUCpriqUgkvq1LKlfy5L6tSypX8uS+rUsqV/Lkvq1PGuo45y+fr5JmNq0acPx48fT3Tds2DAqVarE//73vwzJkhBCCCGEEEI8rnyTMLm4uFCtWrV09zk5OVGkSJEM9wshhBBCCCHEk5D1krZCCCGEEEIIUcjlmxamzGzdulXrEIQQQgghhBAFmLQwCSGEEEIIIUQWJGESQgghhBBCiCxIwiSEEEIIIYQQWZCESQghhBBCCCGyIAmTEEIIIYQQQmRBEiYNpJgV9gbf5uBNHXuDb5NiVrQOSQghhBBCCJGJfD2teH60/kQYE1efIiwqATAwN+gAxd3sGd+1Ch2qFdc6PCGEEEIIIcR9pIUpD60/EcZz8w/9lyzdcz0qgefmH2L9iTCNIhNCCCGEEEJkRhKmPJJiVpi4+hSZdb5LvW/i6lPSPU8IIYQQQggrIglTHtkXfDtDy9L9FCAsKoF9wbfzLighhBBCCCFEtiRhyiPhMVknS49STgghhBBCCGF5kjDlER8X+ydaTgghhBBCCGF5kjDlkQYBnhR3s0eXTRlHWwN1/TzyLCYhhBBCCCFE9iRhyiMGvY7xXasAZJk0xSel8NKiwySYUvIuMCGEEEIIIUSWJGHKQx2qFWfyoDoUc0vf7a64mz1jWgRga9Cz/uR1hs7cR3SCSaMohRBCCCGEEKlk4do81qFacdpWKcbu8+H8vX0v7Zo3pHE5Hwx6HU9V9GH03IPsDb7Ns1P3MGdYfXxcZUyTEEIIIYQQWpEWJg0Y9DoaBnhS10uhYYAnBr3aSa9JWS8Wj26El7Mdp8Oi6TVlFyE34zSOVgghhBBCiMJLEiYrU62kGyuea4xfEUdCb9+l95RdnLgapXVYQgghhBBCFEqSMFkhvyJOLB/bhKolXLkZm0S/aXvYdf6m1mEJIYQQQghR6EjCZKW8XexYPLoRjcsUITYxmcBZ+1l7PEzrsIQQQgghhChUJGGyYi72RmYNq0+n6sVISjEzbuEh5u25pHVYQgghhBBCFBqSMFk5e6OBn/rXYVCj0igKfPD7Cb7beA5FUbQOTQghhBBCiAJPEqZ8wKDX8XG3arzydHkAftgUxPu/nyDFLEmTEEIIIYQQliQJUz6h0+l45ekKfNy9GjodLNh7mRcWHiLBlKJ1aEIIIYQQQhRYkjDlM4Mb+fHLgDrYGvSsO3GdwFn7iE4waR2WEEIIIYQQBZIkTPlQp+rFmT2sPs52Nuy5eJt+U/cQHpOgdVhCCCGEEEIUOJIw5VNNynmxeHQjvJxtORUWTe/Ju7l0K07rsIQQQgghhChQJGHKx6qVdGP52CaU9nTk8u14ek3exYmrUVqHJYQQQgghRIEhCVM+5+/lxPLnGlOluCs3Y5PoN20Puy7c1DosIYQQQgghCgRJmAoAHxd7Fo9pRKMynsQmJhM4cz9rj4dpHZYQQgghhBD5niRMBYSrvZHZwxrQoWoxklLMjFt4iPl7LmkdlhBCCCGEEPmaJEwFiL3RwC8D6zCgYWkUBd7//QTf/3MORZEFboUQQgghhHgUkjAVMAa9jk+6V+OlNuUB+P6fID744wQpZkmahBBCCCGEyC1JmAognU7Ha20r8HG3quh0MH/PZV5cdIjE5BStQxNCCCGEECJfkYSpABvc2J+f+9fB1qBn7fHrBM7cT0yCSeuwhBBCCCGEyDckYSrgOtcozuxh9XGyNbD74i36TdtDREyi1mEJIYQQQgiRL0jCVAg0KefFkjGN8XK25eS1aHpP2cXlW/FahyWEEEIIIYTVk4SpkKhW0o3lY5vg6+nApVvx9Jy8i5PXorQOSwghhBBCCKsmCVMh4u/lxIqxTahc3JWbsYn0m7qH3RduaR2WEEIIIYQQVksSpkLGx9WeJWMa0TDAk5jEZIbO3Mf6E2FahyWEEEIIIYRVkoSpEHK1NzJneAPaVy1KUoqZ5xccYsHeS1qHJYQQQgghhNWRhKmQsjca+HVgXfo3KI1ZgfdWneCHf4JQFFngVgghhBBCiFSSMBViBr2OT3tU46XW5QD47p9zjP/zJClmSZqEEEIIIYQASZgKPZ1Ox2vtKjLxmarodDB39yVeWnSYxOQUrUMTQgghhBBCc5IwCQCGNvHnp/61MRp0/HU8jOGz9xObmKx1WEIIIYQQQmhKEiaRpkuNEswKbICTrYGd52/Rb9pubsYmah2WEEIIIYQQmpGESaTTrLwXi0Y3ooiTLSeuRtN78i5Cb8drHZYQQgghhBCakIRJZFCjlDvLn2tCKQ8HQm7F03PyLk5di9Y6LCGEEEIIIfKcJEwiUwFeTqx4rgmVirkQEZPIs1N3s+fiLa3DEkIIIYQQIk9JwiSyVNTVniVjGtMgwJOYxGSGzNzH+hPXtQ5LCCGEEEKIPCMJk8iWm4ORucMb0K5KUZKSzTy/4CCL9l3WOiwhhBBCCCHyhCRM4qHsjQZ+HViHfvV9MSvwzsrj/LQpCEWRBW6FEEIIIUTBJgmTyBEbg57PelbnhVblAPhm4znG/3kSs1mSJiGEEEIIUXBJwiRyTKfT8Ub7ikzoWgWdDubuvsRLiw+TmJyidWhCCCGEEEJYhCRMItcCmwbwQ7/aGA061hwLY8TsA8QmJmsdlhBCCCGEEE+cJEzikTxTswQzA+vjaGtgx/mb9J+2h5uxiVqHJYQQQgghxBMlCZN4ZM3Le7N4dCM8nWw5fjWKPlN2E3o7XuuwhBBCCCGEFUoxK+wNvs3Bmzr2Bt8mJZ+MhZeESTyWGqXcWT62MSXdHQi+GUfPybs4HRatdVhCCCGEEMKKrD8RRrMvNjNo5gHmBhkYNPMAzb7YzPoTYVqH9lCSMInHVsbbmZXPN6FSMRciYhLpO3U3ey/e0josIYQQQghhBdafCOO5+YcIi0pId//1qASem3/I6pMmSZjEE1HU1Z4lYxrTwN+TmIRkBs/cx4aT17UOSwghhBBCaCjFrDBx9Sky63yXet/E1aesunueJEziiXFzMDJ3RAPaVilKUrKZ5+YfZPG+y1qHJYQQQgghLExRFGISTFy5E8+pa9HsvnCLDSev8+X6MxlaltI9DwiLSmBf8O28CzaXbLQOQBQs9kYDkwfW4d1Vx1l64ApvrzzOzdhExrUqh06n0zo8IYQQQgiRhaRkM9EJJqLvmoi6ayI6IZmo1L9Tt4TU28n/lbn3+OM0EoXHZJ1UaU0SJvHE2Rj0fNGrBt4udvyy5QJf/32Om7FJfNilCnq9JE1CCCGEEJagKApxSSlpCUy6f/9LfqLT3Zc++blrSnnsGGwNelwdjLg52ODqYMSsKBwNjXro83xc7B/7tS1FEiZhETqdjjfbV8LL2Y6Jq08xe1cIt+KS+KZPTWxtpCeoEEIIIURmTCnmTJOcBxOc+1t70hKhhOQnMhbIxd4GV3sjbg5GXB1s1H/Tbqv/ZvWYvdGQbl8pZoVmX2zmelRCpuOYdEAxN3saBHg+dtyWIgmTsKhhTQPwdLLljWVHWX30GpHxSUweVBdnO/nqCSGEEKLgURSF+NRWngQTUfFZJz/RmXRti096/FYeo0GXLrl5WPKTetvNwYizvQ2GJ9gjyKDXMb5rFZ6bfwgdpEuaUl9lfNcqT/Q1nzT51Sosrlutkng42jJ2/kG2B91kwPQ9zAqsTxFnO61DE0IIIYTIwJRiJiYpKcvWnagMSc+9pCj6ronkJ9HKY6d2aXN1MOJqb5OxdcfeBjfHzJMfe6PeqsaOd6hWnMmD6jBx9al0E0AUc7NnfNcqdKhWXMPoHk4SJpEnWlTwZuGoRgyfvZ9jV6LoPWU3c4c3wNfTUevQhBBCCFHAKIrCXVPKfd3Vsu7adn/iExmfxJ1YA4m7/3nsGNJaeeyNaYmPWybJz/2tO6ktQM52NtgYCtYQhg7VitO2SjF2nw/n7+17ade8IY3L+Vh1y1IqSZhEnqnl686ysY0ZMmMfwTfj6DV5F3OGN6BycVetQxNCCCGElUlOMROTkL67Wlazs93fupN6+9Fbee79gHe2s8HV3iZdwpNlFzfH9Pc7GA1W1cpjDQx6HQ0DPLl1WqFhgGe+SJZAEiaRx8p6O7PiuSYMnbmPszdi6Dt1NzOG1rfqgX5CCCGENUgxK+wNvs3BmzqKBN+2+qvzqa086RKc+Ky7tqWfujqZ2MTkx47BRn9vLE9mXdsebN1xMuo4tGcnz3R8Gk9nhwLXyiMejSRMIs8Vc7Nn6ZjGjJiznwOX7jB4xl5+6l+bdlWLaR2aEEIIYZXWnwi7b/yHgblBByieB+M/UszKQ9feyW7KalPK44/lcbI1PJD05Hz2tty28phMJkIdwMPRVpIlkUYSJqEJN0cj80c25IWFh/jndDhj5x/ks57VebZ+aa1DE0IIIazK+hNhPDf/UIYpma9HJfDc/ENMHlQny6RJURQSTOYMCc69v5Oz7doW8wRaeQyprTz3te7kdPY2F3sbjJK4CI1JwiQ0Y280MGVQXd5ddZylB67wvxXHuRmbxPMty0qfXyGEEAK1hWfi6lOZrl+Tet8by46yLSiCmISUTKesTkoxP3YcjqmtPPclOK5Zte48MHubo62M5RH5myRMQlM2Bj1f9KpBEWc7Jm+9wFcbznIzNpEPOldBb8X9soUQQoi8sC/4drppmDMTm5jCwr2h2ZYx6HVpkxfcP3GBa7Zr89xrEZJWHlGYScIkNKfT6fhfh0p4Odvx8ZpTzNoZwq3YJL7uUxNbGzlBCyGEKLzCY7JPllK1r1qM+v4e943hSf+vk7TyCPHIJGESVmNEswC8nG15felR/jx6jTvxSUwZVBcnO/maCiGEKJx8XOxzVC6wiT+NyxaxcDRCFE5y+V5YlW61SvLb0Ho4GA1sD7rJgOl7uBWbqHVYQgghhCbK+zhjk00XdR1Q3M1elucQwoIkYRJWp2VFHxaOaoiHo5GjV6LoM2U3V+7Eax2WEEIIkaduxiYyaMbeLBdgTU2jxnetYtXrMQmR30nCJKxS7dIeLBvbhJLuDly8GUevybs4cz1a67CEEEKIPBEenUD/aXs4cz0Gbxc7PuxSheJu6bvnFXOzz3ZKcSHEkyGDQ4TVKufjzPLnGjN05j7O3Yil75TdzAisT31/6XYghBCi4LoelcCA6Xu4eDOOYq72LBzVkDLezgxt4s/u8+H8vX0v7Zo3pHE5H2lZEiIP5JsWpsmTJ1OjRg1cXV1xdXWlcePGrFu3TuuwhIUVd3Ng6ZjG1PXzIDohmUG/7WXjqRtahyWEEEJYxJU78fSdupuLN+Mo6e7AkjGNKOPtDKhTgzcM8KSul0LDAE9JloTII/kmYSpVqhSff/45Bw8e5MCBA7Ru3Zpu3bpx8uRJrUMTFubuaMv8EQ1pU8mHxGQzY+cfZOmB7NebEEIIIfKby7fieXbqHi7fjqe0pyNLxjTCr4iT1mEJUejlm4Spa9eudOrUifLly1OhQgU++eQTnJ2d2bNnj9ahiTzgYGtg6uC69K5bihSzwlvLj/Hr1vMoSuYDYYUQQoj85GJELH2n7uZq5F3KeDmxdExjSnk4ah2WEIJ8OoYpJSWFZcuWERcXR+PGjbMsl5iYSGLivSmpo6PVSQNMJhMmk8nicWYn9fW1jiO/+bRbZTwdbZi2PYQv158lPOou73SoiP6BbglSv5Yl9WtZUr+WJfVrWVK/uRcUHsvQWQeIiE2inLcTc4fVo4ijIdM6lPq1LKlfy7OmOs5pDDolH12iP378OI0bNyYhIQFnZ2cWLlxIp06dsiw/YcIEJk6cmOH+hQsX4ugoV23ysy3XdPx+yQBAXS8zA8qasck37aVCCCGE6moc/HrKQGyyjhKOCuOqpOBs1DoqIQqH+Ph4BgwYQFRUFK6urlmWy1cJU1JSEpcvXyYqKorly5fz22+/8e+//1KlSpVMy2fWwuTr68vNmzezrZS8YDKZ2LhxI23btsVolDPjo/jjyDXeXnWSZLNC83JF+KlfTZzs1EZTqV/Lkvq1LKlfy5L6tSyp35w7eS2awNkHibxromoJF2YNrYuHo222z5H6tSypX8uzpjqOjo7Gy8vroQlTvuqSZ2trS7ly5QCoW7cu+/fv54cffmDq1KmZlrezs8POzi7D/UajUfMPKJU1xZLf9K7vRxFXB56ff4jt528xdM4hZgXWx9Pp3n82Ur+WJfVrWVK/liX1a1lSv9k7fPkOQ2YdICYhmVq+7swZ3gA3h5zXl9SvZUn9Wp411HFOXz9fd2Iym83pWpBE4dOqog8LRzXE3dHI0dBIek/ZxZU78VqHJYQQQmRpf8htBs/YR0xCMvX9PZg3InfJkhAib+WbhOmdd95h27ZthISEcPz4cd555x22bt3KwIEDtQ5NaKx2aQ+Wj21MCTd7LkbE0Xvybs7diNE6LCGEECKD3RduMXTmPmITk2lcpgizhzXAxV6SJSGsWb5JmMLDwxkyZAgVK1akTZs27N+/nw0bNtC2bVutQxNWoJyPCyueb0J5H2euRyfQ/7f9XIzWOiohhBDinu1BEQybvY/4pBSal/diZmD9tLG3QgjrlW+O0hkzZmgdgrByxd0cWDa2MSPmHODgpTv8espAlbMRtK9WQuvQhBBCFHJbzoQzZv5BkpLNtK7kw68D62BvNGgdlhAiB/JNC5MQOeHuaMv8EQ1pWcELk6Lj+YVHWHYgVOuwhBBCFGIbTl5n9LwDJCWbaV+1KFMG1ZVkSYh8RBImUeA42Br4dUAtGnibSTErvLn8GFP+vUA+mkFfCCFEAfHXsTDGLTiEKUWhS43i/DygDraycKAQ+YocsaJAMhr0DChrZmQzfwA+X3eGT/46jdksSZMQQoi88fvhq7y46BDJZoWetUvy/bO1MBrkp5cQ+Y0ctaLA0ungf+0r8F6nygD8tiOY15cdxZRi1jgyIYQQBd3SA6G8uvQIZgX61ivFV31qYiPJkhD5khy5osAb1aIM3/atiY1ex6rDVxk55wDxSclahyWEEKKAWrD3Em8tP4aiwKBGpfm8Zw0Mep3WYQkhHpEkTKJQ6FmnFNOH1sPeqOffcxEMmL6XO3FJWoclhBCigJm1M5j3Vp0AYFhTfz7uVg29JEtC5GuSMIlCo1VFHxaOaoS7o5EjoZH0nrKLq5F3tQ5LCCFEATH13wtMXH0KgDFPleHDLlXQ6SRZEiK/k4RJFCp1SnuwfGxjirvZcyEijt6Td3HuRozWYQkhhMjnftoUxGfrzgDwUutyvN2hkiRLQhQQkjCJQqecjwsrnmtCOR9nwqIS6DNlNwcv3dY6LCGEEPmQoih8+/dZvtl4DoDX21bgtXYVJVkSogCRhEkUSiXcHVg2pjG1S7sTddfEwN/2svnMDa3DEkIIkY8oisLn68/w4+bzALzTsRIvtimvcVRCiCdNEqa8FBkK146oW9hR3OJDIOzovfsiQzUNr7DxcLJlwciGtKzoTYLJzKi5B1l+8IrWYQkhhMgHFEXhozWnmPrvRQDGd63CmKfKahyVEMISbLQOoNCIDIWf60JyIgBGoCXA2fvK2NjBCwfB3Tfv4yukHG1tmD6kHv9bfoyVh6/yxrKj3IpNlP/0hBBCZMlsVvjwzxPM33MZgE96VGNgQz+NoxJCWIq0MOWV+FtpyVKWkhPVciJPGQ16vu5Tk9EtygDw2bozfPLXKcxmRePIhBBCWJsUs8I7K48zf89ldDr4sncNSZaEKOAkYRIC0Ot1vNupMu92qgTA9O3BvLHsKKYUs8aRCSGEsBbJKWbeXHaUJQdC0evg27416VtPeoUIUdBJwiTEfUa3KMs3fWpi0OtYefgqo+YeID4pWeuwhBBCaMyUYublJUdYefgqBr2OH/vXpkftUlqHJYTIA5IwCfGAXnVLMX1IXeyNeraejWDgb3u5E5ekdVhCCCE0kpRs5oWFh/jrWBhGg45fB9ahS40SWoclhMgjkjAJkYnWlYqyYGQj3ByMHL4cSZ+pu7kWeVfrsIQQQuSxBFMKY+cfZMPJG9ja6Jk6uC7tqxbTOiwhRB6ShMnaJERrHYH4T10/D5aNbUwxV3vOh8fSa/Iugm7EaB2WEEKIPHI3KYVRcw+w+Uw4djZ6fhtSj9aVimodlhAij0nCZG1WjIDLe7SOQvynQlEXVjzfhLLeToRFJdBn6m4OXrqjdVhCCCEsLD4pmeGz97M96CYORgOzhtWnRQVvrcMSQmhAEqa84lhEXWcpWzqIC4dZnWDbV2BOyZPQRPZKujuwfGwTavm6ExlvYuBve9hyJlzrsIQQQlhITIKJoTP3sfviLZztbJg7ogFNynppHZYQQiOFcuHapCR1e5BeDzY26ctlRacDozEXZd191UVp429hMkGSKYVdu3fRpHETjKkvamOHbuf3GE8vgs2TIHgbpi7TUFyK5ygGkwmUbJYOsrV9tLLJyWDOZnbt3JQ1GtW4LVk2JUX9PEwmHUlJGd/ng2VTsslLbWzU74WHky3zhjfk+fmH2XYughGzDvJpj+r0rFMqQ9nc7De3Zc1mtS6yYjCom6XLZle/95dVFPW7lpP9Pqzs/cenpcpC9seyRc8RDxzLWdVvbo57OUdkXjYlJev6zaysJY7lgn6OyK5+rf0cEXXXxIjZ+zl6JRJneyOzhtanvr9H2uPWcI5ITs66fuUc8fhlU39HZPX+5Bzx+GWt5RyR3XF3v0KZMH3zDdhl0thTvjwMHHjv9ldfZV3x/v4QGHjv9vffQ3x85mVLlIDRowF3X3D35Zfv4datFIKCnNh2rjyG1E8a8PaazLjuLeCv1yF4G9NGTCHCdwgUKZthv+7u8Mor927PmgXXrmUeg6MjvPXWvdsLFkBISOZljUZ47717t5csgaCgzMsCTJhw7++VK+HUqazLvvvuvRPjmjVw5EjWZd98E5yc1L83bID9+7Mu+8oran0AbNoE27frCQqqwNGjeu6rXgCefx58fNS/t2+HrVuz3u+oUVCypPr3sUM2+IfWIyj4BmeuRzN2ZzwLyt+mrp8noH4f/P3VsgcPwtq1We93wACoUEH9+/hx+P33rMv26QNVq6p/nz4Ny5ZlXbZ7d6hVS/37/HlYuDDrsp06QYMG6t+XL8Ps2VmXbdsWmjZV/w4LgylTsq7fli3VDSAiAn79Nev9NmkC7dqpf0dFqcdRVurXh86d1b/j49XjMyu1aql1Aeox/OmnWZetUgX69r13O7uyFj9H/GfyZD3792dev97eMG7cvdvTpqn1nBk5R9xz/zli82Ydy5dnXr/w6OeIPXtg48asyxamc0R29WvN54gEUworD4URHuOFvbEonWuXJPiAPfXL3HuuNZwj1q4N4MiRzOtXzhH3POo54tgxbw4fzrx+Qc4RqQrCOSIxMevy95MuedZGp4NaA2DMNihaHZJi4fhSOL9JuuhZAYNeR/uqRalTWr3auD3oJtuDIlCyu8wmhBDC6sUnJbP84BXCYxJwMBroVacUPq72WoclhLACOqUQ/dKLjo7Gzc2NiIgoXF1dMzyel91tkpJMrFu3jo4dO2K878F0ZU0JmNZNQNn/m3q7eC3oOQ08y2S5X2lKV5ulExIyr9/Myj5qk/f0bRf4csNZALrXLsmXfapjZ9Q/9n6zK2stTel372Zdv9be3Sa7smAd3W3i402sXZt5/Up3m8cvm5BgYs2azOv3wbLW0IUmv50jEhNNrF6ddf1a4zni6u0Ehs7Yx/mIWLyc7Zg7vAHli7pkKAvanyNMJhN//rmODh0yr185RzxeWZPJxOrVa2nfvlOm9fvgfq3huJdzRM7KZnaOiI6OxtvbjaiozHODVIWyS56tbfqDM7tyudlnTqV+N4xGBVvb9Cer9AXtMT7zOVRoDr8/D+H7YWYL6Po9VO+d5X5zE0NO2OTiW2INZQ0G/qvXh9Qv6Q+0nOz3/rLjni6Lj7stb688zh/HrhCTlMQvA+rgYGt4rP1mR6/P+XfN0mVzUr86Xc73aw1lwTrKGo05q9/UsrnZryXKWsNxn9tzRE7r11LHckE/R+S0fq3huL8RncCQWXu4eDuO4h72LBzVkDLezlmW1+QcERkK8bfUv5OTKZIUjO2to/fGQDsWUbv9Z0LOEbkve+93RM7Kan3cyzni0cvmtHyhTJjynUqd4bmdsGIkXN6tTj1+cQt0/BJsnbSOrlDrU88XTydbxi08xOYz4Qz8bQ8zA+vj7piL/yWFEEJo4sqdeAZM38vl2/GUdHdg4aiG+BWxsv9XI0Ph57qQrA62MAItAc7eV8bGTp1YKoukSQjxeGQMU37hVgqGroGn/gfo4PB8mNYSrp/QOrJCr03loiwY2RBXexsOXY6kz5TdhEXd1TosIYQQ2bh8K55np+7h8u14Sns6smRMI+tLlkBtWUp+yMj05MR7LVBCiCdOEqb8xGADrd6FoX+CS3G4eQ6mt4Z907PvSCwsrq6fJ8ufa0IxV3uCwmPp9esuzofHaB2WEEKITFyMiKXv1N1cjbxLGS8nloxpRCkPR63DEkJYKUmY8qOAFjB2B5RvBymJsPYNWDII7t7ROrJCrUJRF1Y834Qy3k5ci0qg95TdHLosn4kQQliToBsxPDttD9ejEyjv48zi0Y0o7uagdViP704wmBK0jkKIAkkSpvzKyQsGLIX2n4LeCGfWwJTmcHmP1pEVaiXdHVg+tgk1fd2JjDcxcPpetpwN1zosIYQQwOmwaPpN20NETCKVirmwaHSjgjN1+LJA+KQYfFsVZneBP1+CHd/D6dVw4yQkZbHIkxDioWTSh/xMp4PG48CvCSwbpl5dmtVJ7bbX7FXQ53DKFPFEeTrZsmhUQ8bOP8S2cxGMmnOAr/rUoEftUlqHJoQQhdaJq1EMmrGXyHgT1Uq6Mm94QzycCtAEPUYnMMVB9BV1C9mesYxLcfAsC54B6hIl9292Wc8MKERhJwlTQVCitrrQ7V+vwfFlsPljCP4Xek4Hl2JaR1coOdra8NuQery1/Ci/H7nGq0uOcis2iZHNyzz8yUIIIZ6oI6GRDJmxl+iEZGr5ujNneAPcHHIxL7aWUrJZWOZ+w/4CN1+4ffHeduvCf39fgIQoiAlTt0s7Mj7fueh9CVTAf4nVf7fts16fRojCQBKmgsLeVU2QyrRSxzQFb4PJTaHHFCjfVuvoCiVbGz3f9q1FEWc7ZuwIZtJfp4mISeTtjpXQpa54J4QQwqIOhNwmcNZ+YhOTqe/vwczA+rjY55NkKSEa1r6Zw8I6tbu+kxf4Nsj4cPxtuB2sJk8PJlV3b0PsDXW7vDvjcx29oEjZB1ql/kuqHNwf5x0KkS9IwlSQ6HRQeyCUqg/Lh8ON47CgNzR5EVp/CDYFqOtBPqHX63i/c2W8nO34Yv0Zpm67yK24JD7vWR0bgwwhFEIIS9p94RYj5uwnPimFxmWK8NvQejjZ5ZOfPrHhML8XXD/28LI2duritdlx9FS3UnUzPnb3zn/J1MWMSVVcBMTfVLfQvRmf6+B5L4l6MKly8FB/mwiRz+WTs4bIFe8KMPIf2PgB7JsGu36CkJ3Qe6Z6RUjkKZ1Ox3Mty1LE2ZZ3Vh5n+cEr3IlL4ucBdXCwlXFmQghhCduDIhg19wAJJjPNy3sxbXC9/HPOvRMC83qoCYujF3T/BZzVLvam5GR27txJ06ZNMdr89zPOscjjLVrr4AElPaBknYyPJUSrY6TTuvcF30umYq+rrVNXb8PVAxmfa++Wvmvf/YmVYxFJpkS+IQlTQWW0h05fQcBT8Mc4uHZInUWv6/dQvbfW0RVKfev54uloy7iFh9h0JpxBM/YyY2g93B2l5U8IIZ6kLWfCGTP/IEnJZlpX8uHXgXWwN+aTZOn6cbVlKfYGuJeGwb+rCUYqk4kox6tQvCYY86Brob2r+lrFa2Z8LDFWTabSdfH779+Ya+q4qWuH1O1Bdq4PTD5xX2Ll7CPJlLAqkjAVdJW7qCe5laPUfskrRsDFrdDxC7C1whXNC7inqxRl/siGjJi9n4OX7tB36m7mDG9QMNYAEUIIK7Dh5HVeWHgIU4pCuypF+XlAHWxt8kkX6JCdsKgfJEaDT1UYtAJci2sdVdbsnKFYdXV7UFK82lL24Jip28EQdUV9j2FH1e1Bts6Zz+TnWVadzEqSKZHHJGEqDNx9Yega+PcL2PYVHJ4HofugzywoWlXr6Aqd+v6eLBvbhCEz93LuRiy9ft3F3BENKecjU7oKIcTj+OtYGC8vPkyyWaFzjeJ8/2wtjPllvOjpNer445REKN0Y+i/O3xMq2DpC0Srq9iDTXbhz6b4k6r6kKuoKJMWqLW3Xj2d8rtERPALuJVT3j5tyKQH6fPJ5i3xFEqbCwmADrd+DgOawYhTcPAvTWkGHT6HeCLlak8cqFnNhxXNNGDJzHxcj4ugzZRczA+tTu7SH1qEJIUS+9Pvhq7y29AhmBXrULslXvWvkn8l1Ds2F1S+DYoaKndQxx8YC3PPA6AA+ldTtQcmJEHn5vjFT922Rl8EUD+En1e1BNvb/JVNl0rdQFSkLriVlfUrxyCRhKmwCWsBzO+H35yDob/jrdbWL3jM/qYM+RZ4p5eHI8rFNGDZrH0evRDFg+l4mD6pDy4o+WocmhBD5ytIDofxvxTEUBfrWK8VnPWtg0OeDC4GKAju+hU0fqbdrD4IuP6gXOQsrGzvwKq9uD0pOgqjQzNeairwEyQkQcVrdHmSwBQ//TLr5lVHXryrMdS4eSr4dhZGTF/RfAnsnw8bxcHo1XDsCvWZA6YZaR1eoeDrZsnBUI8bOP8j2oJuMnHOAr/vUpHvtklqHJoQQ+cKCvZd4b9UJAAY1Ks1Hz1RDnx+SJbMZNryr/l8M0OxVaDNeenxkx8ZWbS26fxKMVCnJGZOp1O1OCKQkwc1z6vYgvRE8/MCzDHp3fwIi4tGdtwWfCurEG4Z8sm6XsBhJmAorvR4aj1P7SS8frs5yM6sjtHpXPWlLs3WecbKzYcbQ+ryx7Ch/Hr3GK0uOcDM2kZHNy2gdmhBCWLXZO4OZsPoUAMOa+vNhlyr5Y2Hw5CT443k4vky93f4zaPy8tjHldwab/7rhBQBt0j9mTlHHRmWWTN0OVseN3ToPt85jAGoALJmvPldnUJOmzNaacveTNS4LCUmYCruSdWDMNvjrNfXEvfljCN4GPaepM9GIPGFro+f7Z2tRxNmWWTtDmPTXaW7GJvG/DhXzx3/+QgiRx6Ztu8Cna88AMOapMrzdoVL+OF8mxsLSIXBhE+htoPtkqNFX66gKNr1BbUHy8IOyrdI/ZjarU6D/17Uv5eZ5bpzeTXG7u+huB0PyXfWi8p1g9TO7n04PbqUyrjVVpKyaTBnt8+49CouShEmoayz0nA5lWsHaNyD4X5jcFHpMhfJPax1doaHX6/iwSxW8Xez4cv1Zpvx7gVuxiXzWs3r+GbgshBB54KdNQXyzUe1a9VLrcrzatkL+SJbibsHCPnD1oDrbW9+5UL6t1lEVbvr/kh63UlDmKcwmE/sT19KpUyeMBoO6OG+GMVP/rT1lilMnooi8DBe3PLBj3X/JVAAZ1pny8FdnERT5hiRMQqXTQe2BUKo+LB8GN07Agl7Q5EVo/aE0OecRnU7H8y3LUcTJlndWHmfZwSvciU/ip/518s8K9UIIYSGKovDdxnP8uPk8AK+3rcCLbTKZHMAaRYbCvB5wK0idZGnAMvCtr3VUIjt6PbiWUDf/ZukfUxSIDc84LXrq4r1JMeqYqqhQtefOg1xLZpzNz7OselvWybQ6kjCJ9LwrwMhN8Pf7sH867PpJXUiv98z/+gWLvPBs/dJ4OtnxwsJD/HM6nMEz9jJjaH3cHGXgqRCicFIUhc/Xn2HqvxcBeKdjJcY8lcngf2sUfgbm94Toq+oP5cGrwLui1lGJx6HTgUtRdfNrnP4xRYG4m5mMl7qgJlOJUep3IfoqhGzPuG/nYvd173tgRj87l7x5fyIdSZhERkZ76Pw1lHkK/ngBrh2CqS2g6/dQrZfW0RUabasUZd6IhoyYs58Dl+7QZ+ou5g5vSDE36RMthChcFEXhozWnmLUzBIAPu1RheLN8chEvdB8s6AMJkeBVEQavVLtqiYJLpwNnb3V7cPZhRYG7d9JPiX5/QnX3jtoNMPY6XN6Vcd9OPg9Mix5wbyIKe7e8eX+FkCRMImuVu0LxWrBiJITuUWfTu7gVOnwhfW/zSIMAT5aNbcyQGfs4dyOWXpN3MWd4A8r5OGsdmhBC5AmzWeHDP08wf89lACZ1r8agRn4aR5VD5/5WJ3hIvgsl68HAZeDoqXVUQks6nfodcPSEUvUyPh5/W51gInWc1P1JVfxNiAtXt9A9GZ/rWOSB7n33JVXyvXsskjCJ7Ln7QuBf8O/nsO1rdTXyy3uhzywoWlXr6AqFSsVcWfFcE4bO3MfFm3H0mbKLWcMaUMvXXevQhBDColLMCu+uPM6SA6HodPBFzxr0re+rdVg5c3Qx/P48KClQ7ml1ggcZmyIeJjWZKlk342MJUf8lUhfuTYmemkzF3oD4W+p2ZX/G59q7p58S/f6kytFT1v96CEmYxMMZbKD1++DfHFaOhptnYXpraP8p1BsuB1ke8PV0ZNnYxgybvZ9jV6IYMH0PUwbVpUUFb61DE0IIi0hOMfPW8mOsPHwVvQ6+6VuTHrXzSVe2XT/D3++pf1fvC91/lcVPxeOzd4MStdTtQYkx6ROo+7eYMLVL6NWD6vYgO7f0Xfvu35y8n8zvvMhQNZkDSE7GLT4Ewo6CzX+piGMR9SK9lZKESeRcmafguZ2waiyc36iu3XRxKzzzozrjj7CoIs52LBzViOfmH2R70E2Gz97PN31r0q1WSa1DE0KIJ8qUYubVJUdYcywMg17HD/1q0aVGCa3DejhFgX/Gw84f1NuNxkG7Sepsa0JYkp0LFK+hbg9KioM7IZmMmwqG6CvqJBRhR9TtQbYuD8zkd9/ivc5Fc5ZMRYbCz3UhOREAI9AS4Ox9ZWzs4IWDVps0ScIkcsfJCwYshT2/wj8T4PSfcO0I9J4Bvg20jq7Ac7azYcbQ+ry+7Cirj17j5cVHuBWblH8GPwshxEMkJZt5cdEhNpy8gdGg4+cBdWhfNR8spJ6SDKtfhiPz1dtPT4Cmr0gvDKE9Wyd1GEVmQylMd+8lUw+uNRUVqk6Pfv2Yuj3I6Hjf1OgPtE65FL93oSD+VlqylKXkRLWcJEyiwNDrockL6jSay4erB9rMDtD6PWj6qlxJszBbGz0/PFuLIk62zN4VwkdrTnEzNpE321fMHws3CiFEFhJMKTy/4BCbz4Rja6NnyqA6tK5UVOuwHi4pXv3/8Nw60Omh649QZ7DWUQnxcEYH8Kmsbg9KToQ7lzKuM3X7orpYryleXbfzxomMz7VxuNcyVQDG7knCJB5dybowZjuseRVOLIdNH6mLs/WYpq5LICxGr9cxvmsVvF3s+GrDWX7deoGbsYl82qM6NgZJWIUQ+c/dpBRGzzvA9qCb2NnomT6kXv4Yp3n3Dizsp85aZmOvrltYqbPWUQnx+Gzs1PU5vStkfCw5SU2aHpwW/fZFNclKvgvhp9StAJCESTwee1fo9RuUbQVr31THNE1pCj2mqLMCCYvR6XSMa1WOIk62vLvqOEsPXOF2nImfB9TG3mjQOjwhhMix+KRkRsw+wO6Lt3AwGpgRWI8mZb20DuvhosPUBWnDT6kD5wcsBr8mWkclhOXZ2IJXOXV7UIpJ7c53+6K6UG/oXvXCej4ml6LF49PpoPYgGP0vFK0GcREwvxf8/YF6BUJYVL8GpZk8qC62Nnr+OX2DwTP2EhVv0josIYTIkZgEE0Nn7mP3xVs429kwd0SD/JEs3TwPM9qpyZJzMRi2VpIlIUCdEdKzjHrhvOFoaPKi1hE9NkmYxJPjXQFGboL6o9Tbu36EWR3UgYPCotpXLca84Q1wsbdhf8gd+k7dzY3oBK3DEkKIbEXdNTF4xj72h9zBxd6GeSMaUN8/HyywefUQzGwHUZfVwe4jNkCxalpHJYSwEEmYxJNltIfOX8Oz89VF0q4ehKkt4MQKrSMr8BqWKcLSMY3xdrHj7I0Yev66iwsRsVqHJYQQmYqMT2LQb3s5EhqJu6ORhSMbUbt0Plii4sIWmNNVndGreC0YvgE8/LWOSghhQZIwCcuo3BXG7gDfRpAYrc4e9OeL6kxCwmIqF3dl5XNNCPBy4mrkXfpM2c3R0EitwxJCiHRuxSbSb9oejl+NwtPJloUjG1G9lJvWYT3ciRWwoA8kxULAUxC4BpzzwcQUQmjJsYg6gUR2bOzUclZKJn0QluPuC4F/wb+fw7av4dBcCN2nziCU2VoA4onw9XRk2djGDJu1n+NXo+g/fQ9TB9eleXn5T10Iob3wmAQGTt9LUHgs3i52LBzZkPJFXbQO6+H2TVcnN0KBKt2h57SH/wgUQqi/B184qLbKAqbkZHbu3EnTpk0x2vyXijgWsdo1mEBamISlGWyg9fsw5A91UGzEGZjeGg7MVFdEFxbh5WzHotGNaFbOi/ikFIbP3s8fR65qHZYQopC7HpVAv6l7CAqPpZirPUtGN7L+ZElRYMunsPYNQIH6I9ULf5IsCZFz7r5Qopa6Fa9JlKM/FK957z4rTpZAEiaRV8o8Bc/thHJtITlBXbtp6RC4G6l1ZAWWs50NMwLr0aVGcUwpCi8vPsKsnTIBhxBCG1cj7/LstN1cvBlHSXcHloxpRBlvZ63Dyp45Rf3/6t8v1Nst34FOX4Nelm4QojCRhEnkHScvGLAU2n0CeiOc/hOmNFe76QmLsLMx8GO/2gxt7AfAxNWn+GrDGRRp3RNC5KHLt+LpO2U3l27FU9rTkSVjGuFXxEnrsLJnSoBlgXBwFqCDzt9Cy7fVpTSEEIWKJEwib+n10OQFdQpWD391StaZHWD7t2A2ax1dgaTX65jwTFVeb6uu1P3Llgu8veI4ySlS30IIy7sYEUvfqbu5GnmXMl5OLBnTiFIejlqHlb2EaFjQW72wZ7CFPrOh/gitoxJCaEQSJqGNknVhzHao1huUFNg0Eeb3gJgbWkdWIOl0Ol5sU57PelZHr4MlB0J5bsEhEkwpWocmhCjAgm7E8Oy0PVyPTqC8jzOLRzeiuJuD1mFlLzYcZneGkO1g6wwDl0PV7lpHJYTQkCRMQjv2rtDrN+j2Cxgd4eJWmNIUzv+jdWQFVv8Gpfl1YF1sbfRsPHWDITP2EXXXpHVYQogC6Mz1GPpN20NETCKVirmwaHQjfFzttQ4re7eDYUY7uH4MHL3UacPLPKV1VEIIjUnCJLSl00HtQTB6KxStBnERML8XbPwQUuSHvCV0qFaMucMb4GJnw76Q2zw7dTc3ohO0DksIUYCExsLgmQe4FZdEtZKuLBrVCC9nK59V7vpxmNke7gSDe2kY8TeUqK11VEIIKyAJk7AO3hVh5D/qdK0AO39QxzbdCdE0rIKqUZkiLBnTGG8XO85cj6HX5F1cjIjVOiwhRAFw9EoUv5wyEHnXRE1fdxaMbISHk63WYWUvZAfM6gSxN9SLdyM2QpGyWkclhLASkjAJ62F0gM7fwLPzwd4Nrh5QZ9E7sVLryAqkKiVcWTG2Cf5FHLly5y59puzm2JVIrcMSQuRjB0JuM3T2Ae6m6Khb2p35Ixrg5mDUOqzsnV4N83pCYjSUbqIuuO5STOuohBBWRBImYX0qd4WxO8C3ofof2PJh8OdLkBSvdWQFTukijix/rgnVSrpyKy6J/tP2sCPoptZhCSHyod0XbjFk5j7iElMo52pmxpA6uNhbebJ0cI66JmBKIlTsDINXgoO71lEJIayMJEzCOrmXhsC10PwNQAeH5sD0VnDjlNaRFTheznYsGtWIJmWLEJeUwrDZ+1h99JrWYQkh8pEdQTcZNnsf8UkpNC1bhDGVzDjZ2WgdVtYUBbZ9DatfAsWsjqXtO1ft6SCEEA+QhElYL4MNtPkAhvwBzkUh4oyaNB2Yqf5nJ54YF3sjs4bVp3P14phSFF5afJjZO4O1DksIkQ9sORPO8Dn7STCZaV3Jh6kDa2Fr0DqqbJjNsP5t2PyxervZa/DMz+r/OUIIkQlJmIT1K/MUjN0J5dpCcgKseRWWDYW7kVpHVqDY2Rj4sX9thjT2Q1FgwupTfPP3WRRJToUQWdhw8jqj5x0gKdlMuypFmTKoLnZGK86WkpNg5SjYO0W93eFzeHq8OmOrEEJkQRImkT84e8OApdBuEuht4NQf6oQQofu1jqxAMeh1THymKq+1rQDAT5vP8+6q4ySnmDWOTAhhbf46Fsa4BYcwpSh0rlGcXwbWwdbGin9WJMbComfhxHL1/5Gev0Gj57SOSgiRD1jxmU2IB+j10ORFdW0MD3+IuqyumbHjO7WLhXgidDodL7Upzyc9qqHXwaJ9oTy/4BAJphStQxNCWInfD1/lxUWHSDYr9Khdkh+erYXRYMU/KeJuwdxn4MJmdaH0/kugRh+toxJC5BNWfHYTIgsl68KY7VCtFygp8M8EmN8TYm5oHVmBMrChH78OrIOtQc/fp24wZOY+ou6aSDEr7A2+zcGbOvYG3ybFLF32hChMlh4I5dWlRzAr0KduKb7uUxMba06WIkPVi2tXD4KDBwxdDeWf1joqIUQ+IiMcRf5k7wq9ZkCZVrD2Tbi4BaY0hR5ToVwbraMrMDpUK86c4baMnnuAfcG36fTDNkwpCuExiYCBuUEHKO5mz/iuVehQrbjW4QohLGzB3ku8t+oEAAMblubjbtXQ6614/E/4aXWNpZhr4FpKnTbcu6LWUQkh8hkrviQkxEPodFBnMIz5F3yqQlyE2tK0cTykmLSOrsBoXLYIi8c0wsXehquRCf8lS/dcj0rgufmHWH8iTKMIhRB5YfbO4LRkKbCJP5O6W3mydHkvzOygJkteFWHEBkmWhBCPRBImkf95V4RRm6D+SPX2zu8xzO2CY2KEtnEVIJWKuWJvk/nMV6kd8iauPiXd84QooKZtu8CE1eo6eGNalGF81yrorHlmuXMbYG43SIiEUvVh+HpwK6V1VEKIfEoSJlEwGB2g8zfQdx7Yu6G/dpCWZ95Hd/oPrSMrEPYF3yYiNjHLxxUgLCqBfcG38y4oIUSe+HlzEJ+uPQPAi63L8XbHStadLB1ZBIv6Q/JddTmKIX+Ao6fWUQkh8rF8kzB99tln1K9fHxcXF3x8fOjevTtnz57VOixhbao8A2N3YC7VAKP5LjYrR8DqlyEpXuvI8rXwmIQclbsWKfUsREGhKArfbjzH13+fA+D1thV4vV1F606Wdv0Ev49VJwSq8Sz0XwS2TlpHJYTI5/JNwvTvv/8ybtw49uzZw8aNGzGZTLRr1464uDitQxPWxr00KYP/5GzRZ1DQwcHZML21OvhXPBIfF/sclZu4+hRfbzhLWNRdC0ckhLAkRVH4Yv1ZftwUBMA7HSvxYpvyGkeVDUWBvz+Av99Xbzd+AbpPAYNR27iEEAVCvpklb/369eluz549Gx8fHw4ePEiLFi0yfU5iYiKJife6EUVHRwNgMpkwmbSdFCD19bWOo6AypSicKdEbv5aDsFvzArqI0yjTWpLS9hOU2kNkVfdcql3KhWKudtyITiSrUUp6HUQnJPPzlvNM/vcCT1fyZnCj0jTw97DuK9JWSM4PliX1mz1FUfhk3Vnm7L4MwHudKhLYuHSO6yvP69ecjOGvV9EfWwRASusPMTd6EVJS1K2Ake+vZUn9Wp411XFOY9ApipIvR2mfP3+e8uXLc/z4capVq5ZpmQkTJjBx4sQM9y9cuBBHR0dLhyishK0pmjqXplE05hgAV90bcMR3GMk20k0jN47e0jHzXGqj9P0JkHoKCSxvRqeD7dd1nI++13hdzEGheTEz9b0V7DKfN0IIYSXMCiwP1rPzhnoM9wlIoVkx6/2ZYDAnUi/4F4pFH8GMnqOlh3O5SOYXUYUQ4kHx8fEMGDCAqKgoXF1dsyyXLxMms9nMM888Q2RkJDt27MiyXGYtTL6+vty8eTPbSskLJpOJjRs30rZtW4xG6TLwpGWoX8WMfu+v6LdMQmdORnErTUqPaSgl62kdar6y4eQNJq09w/Xoe8dVcTc73utYifZVi6bdd+5GDPP3hvL7kWvcNZkBcLazoWftEgxs4EsZb0lWsyPnB8uS+s1cilnhgz9PsezgVXQ6+KRbVfrULZnr/eRZ/d6NxLB0IPore1Fs7EnpMR2lQkfLvZ6VkO+vZUn9Wp411XF0dDReXl4PTZjyTZe8+40bN44TJ05kmywB2NnZYWdnl+F+o17BqM8kT9TpQX9flaQkZb1znQ70xkcrazaBWUGnmDAaHogls7JZ5bS5KQtgsH3EssmgmJ9MWb3xXnc4i5VNyaR+ddBkHPg1hlWj0d25hM2cztD6PWg0DvRZDOfT26jfi9T9Ktl078iLsopZrYus6AygN1isbJdapehYvTh7gq7xz859PN20AY3K+mDQ60ibYFxnoGopTz4r5cnbHSuz6mAwC/ZeJuRWHIv2XmDR3gs0LefF4IZ+tKxUHIPNf8ecoqjfyyxjuO/4tFRZeMixnHfniEzPD1mUlXNELssqKVnXb4b9WsFxnwfniOQUMx+sOs7vR69ip4cvetegey3fRz6fZFu/6co+7PjMomzMNVj4rDo21c4NXb/52Pg1LTTnCJ2SnE39yjniscsqKVnXb4b9WsFxn49+R6SWtfg5ItOyGY/7LD/jB+S7hOmFF15gzZo1bNu2jVKlHnFNhdPfgHPGRAqX8hAw8L5yX2Vd8c7+UCbw3u2z30NyFjOEOZaAcqPv3T73C/qEW1QwBaE/ffTeBw1g7w0Vxt27fX4aJGSxnpCtO1R65d7ti7Mg/lrmZW0cocpb926HLIDYkMzL6o1Q7b17ty8tgZigzMsC1Jhw7+/QlRB1KuuyVd+9d2K8ugbuHMm6bJU3IbXbXNgGuLU/67KVXlHrA+DGJvQ3tmdevwBDV8LGT+DkStj1IVyaDpW6gK1zxv2WGwWO/11hvbUHwjZmHUOZQPV7AXD7IFxbm3VZ/wHgWkH9O+o4hP6eddnSfcC96n9lT8PlZVmX9e0OHrXUv2POQ8jCrMuW6AReDdS/4y7DxdlZly3eFrybAmBIvE6jhGkUcQ2ifMIRDKcfqN+iLdUNcNNHEui2iKFtFS7djufolUiCb8ZBNBzfCGv+qUWl2r3oW88XD2M8nPk+6xiK1IeSndW/U+Lh1FdZl/WopdYFqMfwyU+zLutWBfz63rudXdk8Okfoz0+mgml/5t9fOUfc84jnCN2NzVQwLc+8fgEqPA/2PurfEdvhxtas91sAzhEpisLfJ69T8kYMLxTV0aFqMSraHIHbj3aOICEs+/q97xxBYgSc+zXr/Xo3geLt1L9NUeo5Iv4WHFsCKVFQ3BlqdIWYLRAWW2jOEQHJa9GfPpJ5/co54p5HPEd4pxxDf/pw5vULhe4ckaVH/B1h8XNEVjL7HZHNkin3yzcJk6IovPjii6xatYqtW7cSEBCgdUgiv7J3hd4zoWwr2PwK3AmGAzPVpMmzjNbRFUg6nQ7/Ik74F3Ei6q6JY1ciOXEtmptRiXy27gzfbjzHszWdeaFEQo5n5BNCPL4Us8LaE2FciIhFr9PRqVpxyvlkcvHIWsRcg+PLwBQPDp7q1OH27lpHJYQo4B5rDFNCQgL29nnz4+b5559n4cKF/PHHH1SsWDHtfjc3NxwcHHK0j+joaNzc3Ii6HZF5P8U87G5jMiWxbt06OnbsiNHGmG1ZaUrPbdkUTKaEzOv3wbI3TsKKERD+39WsJi9By3fuTUUrTemZljUl3c26fnPYPH7XlMKaYzeYtTuUU2HRgIJRl0yd0h4MbFSaDlWKY2tzX1fJQtQlz5QYz7p1a7OoXzlHPG5ZU1IC69auybx+M+zXCo57C50jEpJMvLRwP1vOhmNr0PNT/9q0rlQ007K52a8pKZF1a1dnXb+P2t3m/CZYMghMcVC8JvRfDE7e95UtHOcIk8nEur/+pGPHDlnUr5wjHqesyWRi7V+r6dSxfeb1m2G/VnDc57PfERY7RzzCcR8dHY2bp/eTH8NkNpv55JNPmDJlCjdu3ODcuXOUKVOGDz74AH9/f0aMGJHbXebI5MmTAWjZsmW6+2fNmkVgYGDudmawTX9wZlcuN/vMKb0R9KDojKC3zX6dCH02j+VZ2Vx8TayirAH0tjmr36JVYdQW2PAeHJihLnp4aTf0ngEe/hn3Sw6nebNUWZ0+5981S5bNaf3qdFnu18EAfRr407u+H4cu32HOrkusPR7G3kux7L10Ci/nCwxo4MuAhn4Uc7PP8X5zE0OmrKGs3piz+v2vbG72a5my1nDc56KszpCL+rWC494C54gEUwqj5h1ie9Ad7GzsmDykHi0qeGf9hFyeI3Jcvzk9Po8vh1Vj1R9DZVrBs/PBzuXx95vbsmAVZRWdTc7qF+Qc8ShldYZc1K8VHPf58HfEEz9HPGrZHJbP9cK1kyZNYvbs2Xz55ZfY2t57kWrVqvHbb7/ldnc5pihKpluukyUhHmR0gC7fQt+5YO8GVw/AlOZwcpXWkRUKOp2Oun6e/Ni/Nrvebs2rT1egqKsdN2MT+XHzeZp+sZlxCw6x9+It8uGknkJYnfikZIbN2s/2oJs4GA3MGlY/+2RJa3unwYqRarJUtQcMWJp9siSEEE9YrhOmuXPnMm3aNAYOHIjBcC+brVmzJmfOnHmiwQmRp6p0g7E7oFQDSIyGZYGw+mVIymIQrnjifFztefnp8uz4X2t+GVCHBgGepJgV/joexrPT9tDxh+0s3HuZ+KRsmv2FEFmKSTAxdOY+dl+8hbOdDXNHNKBJWS+tw8qcosDmT2Ddm4AC9UdBrxlgk8mkTUIIYUG5TpiuXr1KuXLlMtxvNputYsVeIR6Le2kYthaavw7o4OBsmN5anbpW5BmjQU/nGsVZOqYx615uTv8GpXEwGjhzPYZ3Vx2n4aeb+Gj1KXXGPSFEjkTdNTF4xj72h9zBxd6GeSMaUN/fU+uwMmdOgTWvwLYv1dst34VOX2U9a5kQQlhQrhOmKlWqsH379gz3L1++nNq1az+RoITQlMEIbT6EwavAuShEnIZpLeHArOwHuAqLqFzclc96VmfPO214v3Nl/Io4EpOQzMydwbT6eitDZ+5j85kbpJjlsxEiK5HxSQz6bS9HQiNxdzSycGQjapf20DqszJkSYNlQ9YIVOuj8LbT8371B9kIIkcdyPenDhx9+yNChQ7l69Spms5mVK1dy9uxZ5s6dy5o1aywRoxDaKNsKxu6EVWPgwib1aufFrdD1B3Bw1zi4wsfN0cjI5mUY3jSAbUERzN19iS1nw/n3XAT/nougtKcjgxv50adeKdwdczHQWogC7lZsIgN/28uZ6zF4Otkyf0RDqpTIejYoTSVEweKBELJdHYzd6ze1u7QQQmgo1y1M3bp1Y/Xq1fzzzz84OTnx4Ycfcvr0aVavXk3btm0tEaMQ2nH2hoHLoe3H6gw7p36Hqc0hNJvF74RF6fU6Wlb0YWZgfba+0ZJRzQNwtbfh8u14Pll7mkafbeLtFcc4eS1K61CF0Fx4TAL9pu3hzPUYvJztWDy6kfUmSzE3YHZnNVmydYFBKyRZEkJYhUdauLZ58+Zs3JjNSsVCFCR6PTR9CfyawvJhEHkJZnWA1u9Dk5fVx4Um/Io48V7nKrzWtiJ/HLnKnN2XOB0WzeL9oSzeH0o9Pw+GNPGnQ9Vi6dd0EqIQuB6VwIDpe7h4M46irnYsHNWIst5Wuijt7YswrwfcCVHXVhq4HErU0joqIYQAHjFhEqJQKlUXxm6H1a/AyZXwzwS4+C/0nAbOPlpHV6g52Bro16A0z9b35cClO8zZFcL6E9c5cOkOBy7dwdvFjgENSjOgYWmKuubNYttCaOlq5F0GTN/DpVvxlHR3YOGohvgVcdI6rMyFHYP5vSAuHNz91PGjRcpqHZUQQqTJdcKk1+vRZTPwMiUlm5WGhcjv7N2g90x1fNPat+DiFpjcFHpOhbKttY6u0NPpdNT396S+vyfh0Qks3HeZBXsvExGTyA+bgvhly3k6VCvG0Cb+1PPzyPZcJkR+dflWPP2n7+Fq5F18PR1YNKoRpTwctQ4rc8HbYfEAdSmHotVh0HJwKaZ1VEIIkU6uE6ZVq9Iv5mkymTh8+DBz5sxh4sSJTywwIayWTgd1hqjrNS0fBuGn1K4kTV9Ru+nlZGVwYXE+rva88nQFnm9Zjg0nrzN3dwj7Q+6w5lgYa46FUbm4K0Mb+9GtVkkcbGWqYlEwBN+MY8D0PYRFJRDg5cTCUQ0p7uagdViZO/WnuiBtSqLa5bn/IvWilBBCWJlcJ0zdumUcgNm7d2+qVq3KkiVLGDFixBMJTAir51MJRm2GDe/CgZmw83u4tFOd1cnDX+voxH9sbfR0rVmCrjVLcPJaFPN2X+L3I1c5HRbN2yuP8+na0/St58vgxn7W22VJiBwIuhHDgN/2EhGTSHkfZxaMbIiPtXZBPTgb1rwKihkqdVEXpDVaaaxCiELviY2CbtSoEZs2bXpSuxMifzA6QJfvoO9csHODK/thSgs4uerhzxV5rmoJNz7vVYM977ThvU6VKe3pSHRCMr/tCKbl11sZNmsfW86GY5Y1nUQ+czosmn7T9hARk0ilYi4sGt3IOpMlRYFtX8Hql9VkqfZg6DNHkiUhhFV7IpM+3L17lx9//JGSJUs+id0Jkf9U6QbFa6ndS67sg2WB6oQQHT5TkyphVdwdbRnVogwjmgXw77kI5uwOYevZCLb8t/kV+W9Np7q+uDlKF0th3U5cjWLQjL1ExpuoVtKVecMb4uFkhWuRmc2w/m3YN1W93fx1aP2BLEgrhLB6uU6YPDzSD5RWFIWYmBgcHR2ZP3/+Ew1OiHzFww+GrYUtn8KO7+DgLLi8B/rMAp/KWkcnMqHX62hVyYdWlXwIvhnH/D2XWHoglEu34pn012m++fsc3WuXZEhjPyoXt9K1a0ShdiQ0kiEz9hKdkExNX3fmDm+Am4MVJvnJSfD7WDixQr3d4QtoNFbbmIQQIodynTB999136RImvV6Pt7c3DRs2xMPD44kGJ0S+YzDC0+MhoAWsHA0Rp2FaK+j4OdQZKldSrViAlxMfdKnC6+0q8Pvha8zdHcKZ6zEs2neZRfsu08DfkyFN/GhftRhGg6zpJLR3IOQ2gbP2E5uYTD0/D2YNq4+LvRUmS4mxsHQwXNisLgDefQrU6KN1VEIIkWO5TpgCAwMtEIYQBUzZVvDcTlg1Fi5sUvvrX9wKXX+QWaCsnKOtDQMalqZ/A1/2Bd9m7p5LrD9xnX0ht9kXcpuirnYMaOBH/4a++LjIuAuhjd0XbjFizn7ik1JoVMaTGUPr42RnhUsrxt2EBX3g2iEwOsGzc6Hc01pHJYQQuZKjs+uxY8dyvMMaNWo8cjBCFCjOPupq9bt/gk0fqRNBXD0IvWdBqXpaRyceQqfT0bBMERqWKcL1KHVNp4V7L3MjOpHv/jnHz1uC6FitOEOb+FGntKzpJPLOjqCbjJy7nwSTmeblvZg2uJ51To0fFQqL+sKtIHDwhIHL5NwnhMiXcpQw1apVC51Oh6JkP3OUTqeThWuFuJ9eD01fVtcYWT4MIi/DzPbqQOcmL6mPC6tXzM2e19pW4IVW5Vh3Ioy5uy9x8NId/jx6jT+PXqNqCVeGNPbjmZqyppOwrC1nwhkz/yBJyWZaVfRm8qC62But7zvncvcKNnP+BzFh4FoKBq8C7wpahyWEEI8kRwlTcHCwpeMQomArVQ/G7lC75p1cBf+Mh+B/ocdUtSVK5Au2Nnq61SpJt1olOXE1irm7Q/jjyDVOXovmfyuO8+naMzxb35dBDf0oXcRR63BFAfP3yeuMW3gIU4pC2ypF+XlAbexsrC9Z0l3ZR7OgT9ClxIF3JRi0EtxkFl0hRP6Vo4TJz8/P0nEIUfDZu6nd8cq0gnX/UwdAT24KPadC2dZaRydyqVpJN77sXZN3OlZm2cFQ5u6+xJU7d5m27SLTt1+kdUUfhjTxp3k5L/R66a4nHs9fx8J4efFhks0KnasX5/t+taxz8pFzGzAsHYpNyl3MJeujH7gUHD21jkoIIR7LI48QPXXqFJcvXyYpKSnd/c8888xjByVEgaXTQd2h4NsAlg1TZ9Gb1xOavQKt3lNn2RP5ioeTLaNblGVEszJsPRvOnN2X2HYugk1nwtl0JpwALycGN/KjV91S1jnds7B6vx++ymtLj2BWoEftknzVuwY21pgsHVkIf7yATknhumtNigxcgd5RJrkRQuR/uU6YLl68SI8ePTh+/Hi6cU2pA55lDJMQOeBTGUZvgQ3vwoGZ6rpNITug1wx1PSeR7xj0OtpULkqbykW5GBHLvD2XWH7gCsE34/hozSm+/vts2ppOlYrJmk4iZ5YeCOV/K46hKNCnbik+71UDgzW2WO78ETZ+AIC5el/2GTrQ0SjdUoUQBUOuL1G9/PLLBAQEEB4ejqOjIydPnmTbtm3Uq1ePrVu3WiBEIQooowN0+Q76zAE7N7iyH6Y0h5O/ax2ZeExlvJ0Z37Uqe95tw6Tu1ahQ1Jn4pBQW7r1Mh++38+zU3aw9HoYpxax1qMKKLdh7ibeWq8nSwIal+cIakyVFgb8/SEuWaPwCKV1/RtFZ4RTnQgjxiHJ9Rtu9ezebN2/Gy8sLvV6PXq+nWbNmfPbZZ7z00kscPnzYEnEKUXBV7Q4lasOKEWrStGwoXBwGHT5TkyqRbznZ2TCokR8DG5Zmz8XbzNsTwoaTN9gbfJu9wbcp5mrPwIal6degNN4udlqHK6zI7J3BTFh9CoDAJv6M71rF+qauTzHBny/B0YXq7bYfqbOCmkzaxiWEEE9YrluYUlJScHFxAcDLy4tr164B6sQQZ8+efbLRCVFYePjBsHXQ7FVABwdnwfTWEH5a68jEE6DT6Whctgi/DqzLjv+14sXW5fBytuV6dALfbDxHk8838criwxy8dOehyzeIgm/atgtpydKYFmWsM1lKiofFA9VkSWeAbr+qyZIQQhRAuW5hqlatGkePHiUgIICGDRvy5ZdfYmtry7Rp0yhTpowlYhSicDAY4ekJENACVo6B8FMwrRV0/ALqDFEnjBD5XnE3B15vV5EXWpdj3fHrzNkdwuHLkfx+5Bq/H7lGtZKuDGzgi40MBy2Uft4cxNd/nwPgxdbleK1tBetLluJvw6J+ELoXbOyhz2yo2FHrqIQQwmJynTC9//77xMXFAfDRRx/RpUsXmjdvTpEiRViyZMkTD1CIQqdsa3huJ6wao049vvoluLgFuv6gTk0uCgQ7GwPda5eke+2SHLsSydzdl/jz6DVOXI3mnVUncbQxcM72HEOaBODrKYPnCzpFUfjunyB+3BQEwGttK/BSm/IaR5WJ6GvqzJ4Rp9XzUf8l4NdY66iEEMKicpww1atXj5EjRzJgwABcXdUZnsqVK8eZM2e4ffs2Hh4e1ncVTIj8ytkHBq6AXT/C5o/VxW6vHoLeM9VFcEWBUqOUO1/3cefdTpVZeiCUebtDuBqZwPQdIfy2M4Q2lYoytIkfTcvKmk4FkaIofLH+LFP+vQDA2x0rMfapshpHlYmbQTCvB0SFgktxGLQCilbVOiohhLC4HI9hqlmzJm+99RbFixdnyJAh6WbE8/T0lGRJiCdNr1fXZxq+AdxLQ+QlmNkedv4AZpldrSDydLJl7FNl2fRqc0ZWTKFp2SIoCvxz+gaDZ+zj6e/+ZfbOYGISZFB9QaEoCh+vOZ2WLH3QpYp1JktXD6rnn6hQKFJOPS9JsiSEKCRynDDNmDGD69ev88svv3D58mXatGlDuXLl+PTTT7l69aolYxSicCtVD8ZshyrdwZwMGz+EBb0hNlzryISFGPQ6qnsqzA6syz+vPUVgE3+c7Wy4GBHHhNWnaPTpJt7//TjnbsRoHap4DGazwod/nGTmzmAAPu5ejRHNAjSOKhPnN8HsrhB/S53Rc/gGWS9OCFGo5GqWPEdHRwIDA9m6dSvnzp2jX79+TJ06FX9/fzp37szKlSstFacQhZuDuzqwuusP6iDrC5tgclO4sEXryISFlfNxZsIz6ppOH3evRnkfZ+KSUpi/5zLtvttG/2l7WH8ijGRZ0ylfSTErvLvqOPP2XEKngy971WBwIytMQo4vh4XPgikOyrSEoavByUvrqIQQIk/lelrxVGXLlmXSpEmEhISwaNEi9uzZQ58+fZ5kbEKI++l0UDcQRm8F78oQF66OJ/hnoroeiijQnO1sGNzIj79fbcHCUQ3pULUYeh3svniLsfMP0fzLLfyy5Tw3YxO1DlU8RHKKmTeXHWXx/lD0Ovi2b0361vfVOqyM9k6FFSPBbIKqPWHAMrBz0ToqIYTIc4+cMAFs3bqVwMBAAgMDSUlJYdSoUU8qLiFEVnwqw6jNUHcYoMCOb2FWJ7hzSevIRB7Q6XQ0KevFlMF12f6/1oxrVRZPJ1vCohL4asNZmny2mdeWHOFIaKTWoYpMmFLMvLLkCCsPX8Wg1/FDv9r0qF1K67DSUxTYPAnWvQUo0GA09JoBNrZaRyaEEJrIdcJ05coVJk2aRLly5WjdujUhISH8+uuvhIWFMWXKFEvEKIR4kK0jdP1e7aZn5wZX9sGU5nDqD60jE3mopLsDb7avxK63W/Nt35rU9HUnKcXMysNX6f7LTp75eQfLD14hwSSLOlmDpGQzLyw8xJpjYRgNOn4ZUIeuNUtoHVZ65hRY8wps+0q93eo96PilOgmNEEIUUjmeVnzp0qXMnDmTTZs24ePjw9ChQxk+fDjlypWzZHxCiOxU7aEOwl4+Aq4egKVDoN5waP8pGB20jk7kEXujgZ51StGzTimOhqprOq0+do1jV6J4Y9lRPvnrFP0alGZgw9KU8pA1nbSQYEph3IJDbDoTjq1Bz+RBdWhTuajWYaVnSoAVI+DMGtDpofM36vlECCEKuRxfMho0aBAODg6sWrWK0NBQPv30U0mWhLAGHv4wfD00e1W9fWAmTG8N4Wc0DUtoo6avO9/0rcnut1vzVoeKlHCz5068iclbL9Diyy2MnnuAnedvoiiK1qEWGgmmFEbNPcCmM+HY2ej5bWg960uWEqJgfi81WTLYQp85kiwJIcR/ctzCdOXKFXx8fCwZixDiURmM8PQE8G8Oq8ZA+CmY1hI6fgF1hqgTRohCpYizHc+3LMfo5mXYdCacubtD2Hn+Fn+fusHfp25Q1tuJIY396VmnJC72Rq3DLbDik5IZMfsAuy/ewsFoYMbQejQpZ2WzzMXcUJOlG8fB1gX6L4SAFlpHJYQQViPHLUySLAmRD5RrA2N3QplWkHwXVr8Ey4erV49FoWRj0NO+ajEWjGzEP6+1YEhjP5xsDVyIiGP8nydp9OkmPvzjBOfDZU2nJy02MZnAmfvZffEWTrYG5gxvYH3J0u2LMLOdmiw5ecOwvyRZEkKIB8goTiEKGpeiMGglPD0R9DZwcqU6IcSVg1pHJjRWzseFj7pVY8+7bfioW1XKejsRl5TC3N2XePrbbQz8bQ8bTl6XNZ2egKi7JgbP2Mu+kNu42Nswb2RDGgR4ah1WemFHYUZ7uBPyX9feDVC8ptZRCSGE1ZGESYiCSK+HZq/AsPXgXhoiL6lXkXf+AGb5MVzYudgbGdLYn39ee4oFIxvSrkpR9DrYef4WY+Yd5KmvtvLr1vPckjWdHklkfBKDftvL4cuRuDkYWTiyEXVKe2gdVnrB22FWZ3U9t6LVYfjfUKSs1lEJIYRVkoRJiILMtz6M2Q5VuoM5GTZ+CAt6Q2yE1pEJK6DT6WhazotpQ+qx7a1WPNeyLB6ORq5G3uXL9Wdp/PlmXlt6hKOyplOO3YpNpN+0PRy/GoWnky2LRjWieik3rcNK79SfML8nJMWAXzO1G56LlU1CIYQQViTXCdP+/fvZu3dvhvv37t3LgQMHnkhQQognyMFdXa+py/dgYw8XNsGUpnBhi8aBCWtSysOR/3WoxO532vB1n5rUKOVGUrKZlYeu0u2XnXT7ZScrD10hMVnWdMpKeEwC/abt4cz1GLyc7Vg8uhFVSrhqHVZ6B2bBsqGQkgSVusCgFWBvZQmdEEJYmVwnTOPGjSM0NDTD/VevXmXcuHFPJCghxBOm00G9YTBqC3hXhtgbMK8HbPoIUkxaRyesiL3RQO+6pfjzhWb8Pq4pPWuXxNag52hoJK8tPUqTzzbz1YYzXIu8q3WoVuV6VAL9pu4hKDyWoq52LBnTiApFXbQO6x5FgX+/UhelVcxQZyj0nQtGe60jE0IIq5frhOnUqVPUqVMnw/21a9fm1KlTTyQoIYSFFK0CozZD3UBAge3fwKxOEHlZ68iEFarl6863z9Zi1zutebN9RYq72XMrLolftlyg2RebGTvvILsuyJpOVyPv8uy03Vy8GUdJdweWjmlMWW9nrcO6x2yGdW/Blknq7eZvQNcfQG/QNi4hhMgncp0w2dnZcePGjQz3h4WFYWOT42WdhBBasXVUfyz1mQ12rnBlH0xpBqf+0DoyYaW8nO0Y16oc299qxZRBdWhcpghmBdafvM6A6Xtp99025u0OITYxWetQ89zlW/H0nbKbS7fi8fV0YMmYRvgVcdI6rHuSk2DlSNg3Tb3d8Uto84GszSaEELmQ64SpXbt2vPPOO0RF3VvXJTIyknfffZe2bds+0eCEEBZUtQeM3Q4l66nrNC0dAmteA5N0tRKZszHo6VCtOItGN+LvV1swqFFpHG0NBIXH8sEf6ppOE/48yYWIWK1DzRPBN+N4dtpurkbeJcDLiaVjGlPKw1HrsO5JjIGFfeHECtAbodcMaDhG66iEECLfyXXC9PXXXxMaGoqfnx+tWrWiVatWBAQEcP36db755htLxCiEsBQPfxi+Hpq+ot4+MAOmt4GIs1pGJfKBCkVdmNS9OnvebcOErlUo4+VEbGIys3eF0Oabfxk8Yy8bT90gxVwwu+udD4+h79TdhEUlUM7HmSWjG1HczUHrsO6JuwlzusLFLWB0ggFLoHpvraMSQoh8Kdd96EqWLMmxY8dYsGABR48excHBgWHDhtG/f3+MRqMlYhRCWJLBCG0nQkALWDUGwk/C1Keg05dQe7B03RHZcrU3Etg0gCGN/dl54SZzdl1i05kbbA+6yfagm5R0d2BwYz+ereeLh5Ot1uE+EWeuRzNw+l5uxSVRqZgL80c2xMvZTuuw7om8rE7qcus8OHjCwOVQqq7WUQkhRL71SIOOnJycGD169JOORQihpXJtYOxONWm6uAX+fBEuboUu38m0w+Kh9Hodzct707y8N6G345m/9xJL9odyNfIun687w7cbz/FMzRIMbexvfesS5cKJq1EMnrGXO/EmqpZwZf6IhtaVCN44pa6xFBMGbr4waCV4V9A6KiGEyNdylDD9+eefdOzYEaPRyJ9//plt2WeeeeaJBCaE0IBLUfUH1q4fYNPH6tiHqweh90woKVeoRc74ejryTsfKvPp0BVYfvcac3SGcuBrN8oNXWH7wCrVLuzO0sT8dqxfDzib/zNR2JDSSITP2Ep2QTE1fd+YOa4CboxX1rLi8Rx2zlBClLh8waAW4ldQ6KiGEyPdylDB1796d69ev4+PjQ/fu3bMsp9PpSEmRRQ2FyNf0emj2Kvg1heUj4E4IzGgHTV6Gyl3VLnrJybjFh0DYUUidHdOxCLj7ahm5sDL2RgN96vnSu24pDodGMndXCH8dD+Pw5UgOXz7CpL9s6d+gNAMalrau8T+ZOBBym8BZ+4lNTKaenwezhtXHxd6KkqWz69UFaZMTwLch9F8Mjp5aRyWEEAVCjhIms9mc6d9CiALMt4E6i97ql9Qpx3d8o26AEWgJcP/cEDZ28MJBSZpEBjqdjjqlPahT2oP3Oldh8b7LLNh7mevRCfy0+Ty/br1A+6pFGdLYn4YBnuisbNzc7gu3GDFnP/FJKTQq48mMofVxsrOiZTQOL1C70CopUL69umSArRXN1ieEEPlcrmbJM5lMtGnThqCgIEvFI4SwJg7u0GcONH/94WWTEyH+lsVDEvmbt4sdL7Ypz/b/teLXgXVoGOBJillh7fHr9Ju2h/bfb2P+nkvEWcmaTjuCbjJs9j7ik1JoXt6LWYENrCtZ2vkD/PG8mizVHAD9FkiyJIQQT1iuEiaj0cixY8csFYsQwhrpdFBZxiaKJ8to0NOpenGWjGnMhldaMLBhaRyMBs7diOX930/Q6NNNTFx9kosarum05Uw4w+fsJ8FkplVFb6YPqYeDrZWMuTKb4e/3YeOH6u0mL0L3X9VZL4UQQjxRuV6HadCgQcyYMcMSsQgh8rub59QfckLkQsViLnzSQ13T6cMuVQjwciImMZlZO0No/c2/DJm5j02n83ZNp79PXmf0vAMkJZtpW6UoUwbXxd5oJclSikltVdr1k3q77UfQbpIsASCEEBaS634FycnJzJw5k3/++Ye6devi5OSU7vFvv/32iQUnhMhnVo6CdW+BfzPwb6Gu7eRdUX7IiRxxczAyvFkAgU382X7+JnN3hbD5bDjbzkWw7VwEvp4ODG7kR996vrg7Wm4q77+OhfHy4sMkmxU6Vy/O9/1qYTTk+vqiZSTFw7JACNoAOgN0+xlqDdA6KiGEKNBynTCdOHGCOnXqAHDu3LknHpAQIh+zcYC7d+D0anUDcPKBgOZq8hTQAjwCJIES2dLrdTxVwZunKnhz+da9NZ1Cb9/l07Vn+Obvc3SrVYIhjf2pVvLJrun0x5GrvLrkCGYFutcqwdd9amJjLclS/G1Y+Cxc2acea31mQ8UOWkclhBAFXq4Tpi1btlgiDiFEQRC4BhQFgv+FkO3qujBx4ep6TidWqGXcfMH/vgRK1okR2ShdxJF3O91b02n2rhBOhUWz9MAVlh64Ql0/D4Y09qNjteLY2jxeYrPsQChvrTiGokDvuqX4olcNDHorSe6jrqoL0kacUReSHrAMSjfUOiohhCgUcp0wDR8+nB9++AEXF5d098fFxfHiiy8yc+bMJxacECKf0dtAiVrgWx9avKHOnHdlPwRvg+Dt6t9RoXB0oboBeJa91wLl3wKcvTV9C8I6Odga6Fvflz71SnHo8h3m7LrE2uNhHLx0h4OX7vCx82kGNCzNwIalKepqn+v9L9x7mXdXHQdgQMPSTOpWDb21JEsR59RkKSoUXIqri0sXraJ1VEIIUWjkOmGaM2cOn3/+eYaE6e7du8ydO1cSJiEKIsci6jpLyYlZl7GxU8s9eJ9/M3VrBSTFqa1OIdvVJOraYbh9Qd0Ozlaf41PlXguUf1Nw8LDUuxL5kE6no66fJ3X9PHm/c2UW7Qtlwd5LhMck8uOmIH7dcp721YoxpJEfDTJZ0ynFrLA3+DYHb+ooEnybxuV8mLc7hAmrTwEQ2MSf8V2rWM9aUFcOwoLecPc2FCkHg1eBe2mtoxJCiEIlxwlTdHQ0iqKgKAoxMTHY29+7gpeSksLatWvx8fGxSJBCCI25+6qL0v63zpIpOZmdO3fStGlTjDb/nUYcizx80VpbJyjXRt0AEqLg0i619Sl4G9w4DuGn1G3fVEAHxWv+1wL1FJRuDHbOlnufIl/xcbXn5afL83yrsmw4eZ25uy6xL+Q2fx0L469jYVQq5sKQxv50r10CR1sb1p8IY+LqU4RFJQAG5gYdwMXehpgEdc2nMS3K8HbHStaTLJ3fBEsGgykOStSBgcvAyUvrqIQQotDJccLk7u6OTqdDp9NRoUKFDI/rdDomTpz4RIMTQlgRd997CZHJRJTjVTWZMT7Gui/2blCxo7oBxN1SW59SW6BunoOwI+q26ye1y1/JuvdaoHwbgNHhcd+ZyOeMBj1dapSgS40SnA6LZu7uS/x++Cpnrsfw7qrjfLbuNA38Pdl0JjzDc1OTpY7VillXsnR8OawaA+ZkKNMKnp0vFwuEEEIjOU6YtmzZgqIotG7dmhUrVuDp6Zn2mK2tLX5+fpQoUcIiQQohCgmnIlC1u7oBRIfdS56Ct0HkJQjdq27bvwaDnZo0pU4gUaIO2Fhuumlh/SoXd+WzntV5u0Mllh0MZd6eS1y6FZ9psnS/I6GRmBUwWEO+tGcKrP+f+ne1XtB9inyvhRBCQzlOmJ566ikAgoODKV26tPVchRNCFFyuxaFGX3UDuHMpfQIVE3avRWrLJ2B0Ar/G91qgitcEvZUsNirylJujkZHNyzC8aQBTt13gi/Vnsy0fFpXAvuDbNC5bJNtyFqUosHmSejEAoMEY6PA56K1kWnMhhCikcj3pg5+fH9u3b2fq1KlcvHiRZcuWUbJkSebNm0dAQADNmjWzRJxCCAEefupWe5D64/LW+XvJU8h2dYzV+X/UDcDOTZ04IrUFyruy/PgsZPR6HSXcc9ZtMzwmwcLRZCMlGf56DQ7NUW+3fh+avyFrlgkhhBXIdcK0YsUKBg8ezMCBAzl06BCJieqsWVFRUXz66aesXbv2iQcphBAZ6HTgVV7d6o8As1mdLCK1BSpkByRGwdm16gbg6KXO2BfQQp1EokhZ+UFaCPi45Gya8ZyWe+JMCbBiBJxZAzo9dP4W6g3TJhYhhBAZ5DphmjRpElOmTGHIkCEsXrw47f6mTZsyadKkJxqcEELkmF4PxaqpW6Pn1Cv214/em4Hv8m6Ivwmnflc3AJcS99aACmgh0zUXUA0CPCnuZs/1qASUTB7XAcXc7GkQ4JnJoxaWEAWLBsClHeqYvF6/QZVn8j4OIYQQWcp1wnT27FlatGiR4X43NzciIyOfRExCCPH4DP/NqFeyLjR7BZKT4Nqhe134QvdCzDU4tkTdADz8/xv/9JSaSLkU0/IdiCfEoNcxvmsVnpt/CB2kS5pS2xfHd62CIa8Xqo25DvN7q9Pp27lCv4Xq904IIYRVyXXCVKxYMc6fP4+/v3+6+3fs2EGZMmWeVFxCCPFk2dhC6Ubq9tRbYLqrJk2pLVBXD8KdEHU7PE99jlfFey1Q/s3BUYMWCPFEdKhWnMmD6ty3DpOqmJs947tWoUO14nkb0O2LMK+H+n1z8oFBK6B4jbyNQQghRI7kOmEaNWoUL7/8MjNnzkSn03Ht2jV2797NG2+8wQcffGCJGIUQ4skzOkCZluoGkBgDl3ZDyH8tUGHH4OZZddv/G6BTu/v5/9d9z68J2Ltq+AZEbnWoVpy2VYqx+3w4f2/fS7vmDWlczifvW5bCjsL8XhAXobZqDl4FnnLBUQghrFWuE6a3334bs9lMmzZtiI+Pp0WLFtjZ2fHGG2/w4osvWiJGIYSwPDsXqNBO3QDib8OlnfdaoCJOw/Xj6rbnF9AZoETtey1Qvo3A1lHb9yAeyqDX0TDAk1unFRoGeOZ9shS8TR2zlBQDxarDwBXgUjRvYxBCCJEruU6YdDod7733Hm+++Sbnz58nNjaWKlWq4OwsK5ALIQoQR0+o3FXdAGLD068BdfsiXD2gbju+A71RXUQ3dQ2oUvXAxk7b9yCsy6k/YMVISElSvyf9FoC9m9ZRCSGEeIhcJ0ypbG1tqVKlypOMRQghrJezD1TrpW4AUVfutT4Fb4PoK2qL1KWd8O/nYOMApRvem8K8eC11IgpROB2YCWteAxQ1Ce/5Gxg1msZcCCFEruT4f+/hw4fnqNzMmTMfORghhMg33EpBrf7qpihqi9P9LVBxEXBxq7oB2Lqo455SpzAvWk0W0S0MFAW2fQVbPlFv1w1U11nSGzQNSwghRM7lOGGaPXs2fn5+1K5dG0XJbCULIYQopHQ6dRHcImXVH8SKAhFn/muB+lddRDchEoI2qBuAg8d/i+g+pSZQXhVkEd2CxmyGdW/B/unq7RZvQat35XMWQoh8JscJ03PPPceiRYsIDg5m2LBh/L+9+46L6sr/P/4aOgiKCIggdin2EisaTdTYN0YFY0k0mk3VbL7Zb7Imu79Nsj3Z726KGpNsLCmWgEbNGo0xRaPYCxZEELsUsaMgODD398fEMURRQIdh8P18PO5D59wzdz5zuFzmM+fcc8aNG0dAgKbYFRG5jskEwdHWrcsTYCmGk3uv9T4d3QCXz0HKf60bgG/da/c/hXe3Jl3ivIoKYclTkPwFYIKBb0CXJx0dlYiIVECZE6YZM2bw73//my+++ILZs2fz8ssvM3jwYCZNmsQDDzyASd+YiYjcmIsr1Gtr3bpPgWIzZCZZe5+uLqJ76STsXQR7F+EO9PMIxNXyNTTtbU2kaoU5+E1ImRVehM/HWYdjurjDQ+9D65GOjkpERCqoXHcge3p6Mnr0aEaPHs3Ro0eZO3cuzzzzDEVFRSQnJ2umPBGRsnB1h/BO1u3e/wVzgXW2vZ96oIwTW/G5chp2L7BuAHWaXeuBatQTfIMc+x7kxvJOw7yRkLkT3GvAw59B0/sdHZWIiNyGCk/Z5OLigslkwjAMiouL72RMIiJ3F3cv6/1MjXrAfa9QlHeerV9Mp0twIa5H10NWEpxJt27b51ifE9zy2hpQDWPA29+R70AAzh2Fz4Zbf04+dWBsAoR1dHRUIiJym8qVMBUWFtqG5K1fv54hQ4Ywffp0BgwYgItmexIRuTM8anCqZmss9w/C1d0dCi5Y73u6eg/Uyb2Qk2zdNr8PJhfrcL9GPa2TSDToCp7q8a9UJ5PhsxFwMQtqNYBHvoDA5o6OSkRE7oAyJ0zPPPMMCxcuJDw8nIkTJ7JgwQICAwPtGdt1fvzxR/75z3+yfft2srKyWLJkCcOGDavUGEREKp1XLYgcaN3AOuzryPprCdSZA9YhYJk7YcO74OJm7dm4OoV5/c5a88eejm6EBaOsiW1QtDVZqhnq6KhEROQOKXPC9P7779OgQQOaNGnC2rVrWbt27Q3rffHFF3csuF/Ky8ujbdu2TJw4keHDh9vtdUREqrQagdBymHUDyM36aQ2onyaROH/MOpHE8c3WNYBcPSG887UpzMM6WO+jktuXuhISJkBRAYR3gdELwUczyIqIVCdlTpgeffRRh8+EN3DgQAYOHOjQGEREqpya9aBNnHUDOHfkpzWgfuqBupRtTaiOrIMfsE5G0LDbtR6okDZaSLUidn4GXz4HRjFEDICRc8DDx9FRiYjIHVauhWudTWFhIYWFhbbHubm5AJjNZsxms6PCssXw83/lzlL72pfa175uu319w6D1w9bNMOBsOi5H1mE6sh7T0fWYLp+F9G+tG2B41cJo0B2jUU8sDXtCUFS1Xlz1Tpy/LhvfxfX7PwFgafMwxYPeApM76HdC1wc7U/val9rX/qpSG5c1BpNhOOfqiCaT6Zb3ML322mu8/vrr15XPnz8fHx99CygidyHDQs2CEwRe3EfgxRQCL+3H3XK5RJUCt5qc8Y3ilF8LTvu2IM+zbrVOoMrFsNAy83Oa5awE4EDwYPaFxql9REScUH5+PmPGjOHChQvUrFmz1HrVOmG6UQ9TeHg4p0+fvmmjVAaz2czq1avp168f7u66l+BOU/val9rXviq1fS1FmLJ3/9T7tA7T8c2YzPklqhh+oRiNemBp2BOjUU+oVd++MdlZhdu32IzrV7/BZU+89WGf17B0nWynKJ2Xrg/2pfa1L7Wv/VWlNs7NzSUwMPCWCVOF12FyBp6ennh6el5X7u7u7vAf0FVVKZbqSO1rX2pf+6qc9nWHhl2sG7+FoiuQsf3a/U8ntmC6mIlpT7wtUaB245/WgOplncrcr66dY7SPcrXvlTxYPAEOfAMmV3hwBq7tRqM7v0qn64N9qX3tS+1rf1Whjcv6+tU6YRIRkXJy87BOCNGwG/T+HZgvW2fbu5pAZeyAc4et245PrM8JivppDah7rYvvVrdZ4vLPwvw4OLEV3Lwh7mOI6O/oqEREpJI4VcJ06dIl0tPTbY8PHz5MUlISAQEBNGjQwIGRiYhUU+7e0KS3dQMovGhdd+jqFObZe+DUfuu29T+ACUJaXZvCvEE38HLsEOjbciEDPhtufX9e/jAmHhp0cXRUIiJSiZwqYdq2bRv33Xef7fELL7wAwPjx451yFj8REafj6QcRD1g3sPa+HE38qQdqHZxKsSZR2Xtg43Tr8LXQ9temMA/v4jxTb59Kg08fgtwT4BdqXZA2ONrRUYmISCVzqoSpd+/eOOkcFSIi1ZNPAEQPtW4AF0/+tIjuj9Z/zx6CjG3Wbf2/wdUD6ne6lkCF3WMdBljVnNgG82Lh8lmo0xweWQL+4Y6OSkREHMCpEiYREani/OpC65HWDeD88WsJ1OEfITfD2iN1NBHW/N16T1CDrj8lUL2gXltwdfCfpvRv4fNHwJwPoR1g7CKoUcexMYmIiMMoYRIREfvxD4d2Y6ybYVh7nK4mT4d/hPzTcOgH6wbgWRMadr/WAxXcElxcKi/e3Qmw9CmwFEHT+yHuU/D0rbzXFxGRKkcJk4iIVA6TCeo0tW73PGZNoE7tv5Y8HVkHBRcg7WvrBuAdYJ1572oPVGBz+y0Su2kmfD3V+v9WI2HYzKo5XFBERCqVEiYREXEMk8k6iUJwNHR5EizF1skiriZQxzZa7yFK+dK6AfiG/LQG1E89ULUb3X4chgHf/xnW/cv6uMtT0P/vlduzJSIiVZYSJhERqRpcXCG0nXWLeQ6KzZC589oU5se3wKVs2JNg3QBqNbiWPDXuCTVDb3zs88ch/4z1/0VF1Mo/Alm7wMUE6/8F+7+y7rv//0HP39qvF0tERJyOEiYREamaXN0hvLN1u/dFMBdYF4+1LaK7DS4cg6TPrBtAnWbXEqhGPaFGoDVZmt4RigoBcAd6A6T+4vX6vAY9/6fS3p6IiDgHJUwiIuIc3L1+Go7XE/g9FF6C45uurQGVlQRn0q3bttnW5wS3tA75+ylZuqmm9926joiI3HWUMImIiHPy9IVmfa0bwOXzcHTDtQkkTu6FnGTrJiIiUkFKmEREpHrw9oeoQdYNIO+0NXHa+8W1SSOAjV6e/KNObaaeOUe3gjL0PImIyF1NUwCJiEj1VCMQWj5kncThJwbwToA/hzw8eCfAH8Nx0YmIiJNQwiQiIneNDd5eJHt6ApDs6ckGby8HRyQiIlWdEiYREan2LptMLPGtwf8EB14rNAz+VCcAs+PCEhERJ6B7mEREpNo6dP4QCamfsSw8jIuuv/iO0GQi092NAeGh/PbseQYYFn2LKCIi11HCJCIi1cqV4it8e/Rb4tPi2X5yu7XQ1QV3i0GRCYyfL0prGOS4ufG74EBmbXmNyR3/h97hvTFp4VoREfmJEiYREakWjuceJ+FAAksPLOVc4TkAXEwu9Krfi1Y16jNt/6fXP+mnxMjbxZO03MM898NztAlsw5QOU+har2tlhi8iIlWUEiYREXFaZouZtcfXEp8az8asjbbyYJ9gRjYfyUPNH6KuT11GfzUaEyaMG8yLZ8JEQ//GxITGMH//fHaf3s2vv/k1nUM6M6X9FNoFt6vEdyQiIlWNEiYREXE6WZeyWHxgMV8c+IJTl08B1sSne1h34iLiuLf+vbi5WP/EXSm+QnZe9g2TJQADg1P5p3im3TOMazGO/+z+DwlpCWzJ3sIjKx+hd/3eTG4/mciAyEp7fyIiUnUoYRIREadQbCkmMTORhNQEfsz4EYthASDAK4CHmj3EiIgRhPuFX/c8D1cPFg5ZyNmCswAUFRWRuD6RmB4xuLm52Y7h4epBoHcgL3d5mfEtx/P+rvdZdnAZa06sYc2JNQxsNJBn2j1Do1qNKu09i4iI4ylhEhGRKu1U/imWpC9hUdoisvKybOWdQzoTGxlLn/A+uLu63/QYITVCCKkRAoDZbOaw22GiA6Jxd7/x80J9Q/lTzJ94rNVjvJf0Hl8f+ZqVR1byzdFveLDZgzzV5inq+da7c29SRESqLCVMIiJS5VgMC5uzNpOQlsAPx36gyCgCoKZHTR5s9iAjI0bSpFYTu8fRuFZj/tnrn0xqPYnpO6ez9sRavjjwBf89+F/iIuN4vPXjBHoH3vpAIiLitJQwiYhIlXGu4BzL0peRkJbAsYvHbOXtgtoRFxlHv4b98HLzqvS4ogKimN5nOkk5SUzbOY0t2VuYlzKPLw58wdjosUxoOYFanrUqPS4REbE/JUwiIuJQhmGwM2cn8WnxfHPkG8wWMwA13GswpMkQYiNiq8yEC+2C2zGr/yw2ZW1i2o5p7D69m4/2fMTn+z9nQqsJjIseh4+7j6PDFBGRO0gJk4iIOETulVyWH1xOQloC6efTbeXRAdHERcYxqPGgKpt8dK3XlS6DurDm+BqmJU3jwLkDTNs5jXkp83i89ePERcbh6erp6DBFROQOUMIkIiKVxjAMks8kE58az8rDKykoLgDA282bgY0HEhcRR8vAlg6OsmxMJhP3NbiPXuG9+Prw18xImsGxi8d4c+ubfJz8MU+1fYoHmz2Iu8vNJ6QQEZGqTQmTiIjYXb45n68Of0VCagIpZ1Ns5c38mxEXGceQJkPw8/BzYIQV52JyYVCTQfRr1I8v079k5q6ZnMw/yesbX2f23tk82+5ZBjYeiIvJxdGhiohIBShhEhERu0k9m0pCWgLLDy0nz5wHgIeLBw80eoC4yDjaBbXDZDI5OMo7w93FnRERIxjSdAgJqQn8Z89/OH7xOFPXTWXW3llMbjeZ+8LvqzbvV0TkbqGESURE7qiCogK+OfoN8anx7Dq1y1besGZDYiNi+VXTX1Hbq7YDI7QvT1dPxrUYx/Dmw5mXMo85e+dw4NwBfvPDb2gd2Jop7afQtV5XJU4iIk5CCZOIiNwRhy4cIiE1gS8PfknulVwA3Exu3N/gfuIi4+gc0vmuShJ83H34dZtfExcZx8fJH/NZymfsOb2HJ1Y/QaeQTjzX/jnaBbdzdJgiInILSphERKTCzMVmvjv2HfFp8WzN3morD60RysiIkTzU/KG7fmHXWp61eK7Dc4yJHsOsPbP4PPVztmZv5ZGVj3Bv/XuZ0n4KUQFRjg5TRERKoYRJRETK7fjF4yxKW8TS9KWcLTgLWCc/uLf+vcRFxNE9tDuuLq4OjrJqCfQO5Hedf8ejLR7lg90fsDR9KT+e+JEfT/xI/0b9ebbdszSu1djRYYqIyC8oYRIRkTIpshSx9sRaElITSMxMtJUHeQcxImIEI5qPIKRGiAMjdA71fOvxWvfXeKzVY8xImsHKwytZdWQVq4+u5ldNf8XTbZ8m1DfU0WGKiMhPlDCJiMhNZedl88WBL1ictpicyzm28pjQGGIjYrk3/F6tNVQBDWs25M1732RSq0lMT5rOmuNrWJq+lOWHlhMXEcev2/z6rh/OKCJSFShhEhGR6xRbitmQuYH4tHh+PPEjFsMCQIBXAMOaDWNk85GE1wx3cJTVQ2RAJNPun8auU7uYtnMam7M2M3//fJakL2FM1Bgea/UYtTxrOTpMEZG7lhImERGxOX35NEvTl7IobREZlzJs5Z1COhEbEUufBn3wcPVwYITVV9ugtnz0wEdsztrMuzvfZfep3czaa50kYkLLCYxrMY4a7jUcHaaIyF1HCZOIyF3OMAy2ZG8hPjWe7499T5FRBICfhx8PNn2Q2IhYmvg3cXCUd48u9brwWchnrD2xlmk7p5F2Lo3pSdOZlzKPx1s/zqioUXi6ejo6TBGRu4YSJhGRu9T5gvMsO7iMRWmLOJJ7xFbeJqgNcRFx9G/UHy83L8cFeBczmUz0Du/NvfXvZdWRVcxImsHR3KP8c9s/+XjfxzzV9imGNRume8dERCqBEiYRkbuIYRgknUoiITWBVUdWccVyBQAfNx+GNh1KbEQskQGRDo5SrnIxuTCw8UD6NezHlwe/ZOaumWTnZfOnjX9izt45PNPuGQY2Gqgp3EVE7EgJk4jIXeDilYssP7Sc+NR40s+n28qjA6KJjYxlUONBuj+mCnNzcWN48+EMaTKEhLQEPtz9IccvHufldS8za88sJrebzP0N7sdkMjk6VBGRakcJk4hINZZ8OpmEtARWHF7B5aLLAHi5ejGw8UBiI2JpFdhKH7KdiIerB2Ojx/JQs4eYv38+s/fOJv18Os+veZ5WdVoxpf0UuoV2089UROQOUsIkIlLN5JvzWXl4JfFp8ew7s89W3rRWU2IjYxnadCg1PWo6MEK5XT7uPjze+nHiIuOYu3cun6V8xt4ze3ny2ye5p+49PNfhOdoHt3d0mCIi1YISJhGRaiLtXBoJqQksP7ScS+ZLALi7uNOvYT/iIuPoENxBPQ/VTE2PmjzX4TnGRo/loz0fEZ8az7aT23h05aP0DOvJlPZTiK4T7egwRUScmhImEREnVlhcyDdHviE+NZ6kU0m28gZ+DYiNiOXBZg9S26u24wKUSlHHuw6/6/w7xrccz/u73mdp+lLWZaxjXcY6Hmj4AM+2f5YmtTQ1vIhIRShhEhFxQkcuHCEhLYFlB5dxofACAG4mN+5rcB+xEbF0qdcFF5OLg6OUyhZSI4TXur/GxFYTmZE0g5WHV/LN0W/49ti3DG0ylKfbPU2Yb5ijwxQRcSpKmEREnIS52Mx3x79jUeoiNmdvtpXXq1GPkREjeajZQwT5BDkwQqkqGtRswBv3vsGk1pOYvnM6Pxz/gWUHl/HV4a8Y2XwkT7R5QueKiEgZKWESEaniTlw8weIDi/niwBecLTgLgAkT99a/l7jIOGJCY7QOj9xQRO0I3r3/XXaf2s20ndPYlLWJhakLWZq+lNHRo5nYciL+Xv6ODlNEpEpTwiQiUgUVG8WsPbGWxQcXk5iRiIEBQJB3EMObD2dE8xHU863n4CjFWbQJasN/HvgPW7K28O7Od9l1ahdz9s4hITWBR1s+yqMtHtU6XCIipVDCJCJShZzMO0lCagILcheQ+2OurbxbvW7ERcbRK7wX7i7uDoxQnFnnep35NORT1mWs490d75J6LpX3kt5jQcoCJrWexKjIUXi5eTk6TBGRKkUJk4iIg1kMCxsyN5CQmsDaE2spNooB8Pf056FmDzEyYiQNajZwcJRSXZhM1uGcPcJ68M3Rb5ixcwZHco/wf9v+j0+SP+HJtk/yUPOHlJiLiPxECZOIiIOcvnyapelLWZS2iIxLGbbyDsEdaHqpKb8d+ltqeGmYlNiHi8mFAY0G0LdBX/578L/M3DWTrLws/rzpz8zZO4dn2j3DoMaDdH+ciNz1lDCJiFQiwzDYdnIb8anxfHvsW4osRQD4ufvxq2a/IjYilgY1GrBixQo8XD0cHK3cDdxc3Hio+UMMbjKYRWmL+HD3h5y4dIJX1r/CrD2zmNx+Mn0a9NGixyJy11LCJCJSCS4UXmBZ+jIS0hI4knvEVt4msA2xkbH0b9QfbzdvAMxms4OilLuZh6sHY6LHMKzZMBbsX8DsvbM5eOEg/7Pmf2hZpyVT2k+he2h3JU4icls2Z2/mndx3qJNdhx7hPRwdTpkoYRIRsRPDMNh1ahcJaQmsOrKKwuJCAHzcfBjcZDCxEbFE14l2cJQiJfm4+zCp9SRiI2P5JPkTPt33Kclnknnq26foWLcjz7V/jg51Ozg6TBFxQoZhMC1pGqcsp5iWNI2Y+jFO8SWMEiYRkTvs0pVLLD+0nIS0BNLOpdnKI2tHEhcZx+AmgzWFs1R5NT1qMrn9ZMZEj2HWnlks3L+Q7Se3M/7r8fQI68GU9lNoUaeFo8MUESey8shK9p3dB8C+s/vYkLmBmLAYB0d1a0qYRETukH1n9hGfGs+Kwyu4XHQZAE9XTwY0GkBcZBytA1s7xTdpIj8X4BXAi51e5JEWj/DB7g9YemAp6zPWsz5jPf0a9mNyu8k08W/i6DBFpAoyW8zsytlFYmYi60+sZ/+5/bZ9LiYXpu2c5hRDfZUwiYjchnxzPquOrCI+NZ69Z/bayhvXakxcRBxDmw6llmctB0YocmeE1Ajh1W6vMrHlRN7b9R5fHfqK1UdX892x7xjSZAhPt32a+n71HR2miDhYdl426zPWk5iRyKasTVwyX7phPYthIflMslP0MilhEhGpgAPnDpCQlsDyg8u5aL4IWGcb69ewH3ERcXSs27HKf2MmUhHhNcP5e8+/M7HVRGYkzeC7Y9/x5cEvWXF4BSOaj2Bii4mODlFEKtGV4ivsyNnB+hPrScxMJP18eon9tT1r0y20G7tO7SIrLwuLYbHtc5ZeJiVMIiJlVFhcyOqjq0lITWBHzg5bebhfOCMjRvJg0wep413HgRGKVJ7mtZvz9n1vs/f0XqbtnMaGzA18nvo5y9KXcY/bPXQv7E6Qe5CjwxQROzh+8TiJGYkkZiSyOXuzbRg6WJOg1oGt6RHWgx5hPYgOiGZT1iZWHF5x3XGcpZdJCZOIyC0czT1KQmoCyw4u43zheQBcTa7cF34fsZGxdK3XFReTi2ODFHGQVoGt+KDfB2zN3sq0ndPYmbOT9cXrGbpsKONbjueRFo/g6+Hr6DBF5DYUFBWwNXsriZnWJOnny2MABHoHEhMaQ4/6PehWr1uJoeiGYTBt5zRMmDAwrju2CVOV72VSwiQicgNmi5kfjv1AfFo8m7M228rr+tRlZMRIhjcfTrBPsAMjFKlaOoV04uMBH7Pm6Br+vv7vZBVl8d6u95i/fz6TWk3i4aiH8XLzcnSYIlIGhmFwJPeI7V6kbSe32ZbGAHAzudEuuB0xYTH0COtBZO3IUpMds8VMdl72DZMlAAOD7LxszBZzlV2wXQmTiMjPZF7KZFHaIpakL+H05dOA9duvnvV7EhsRS4+wHri56NIpciMmk4keYT142vdpPFt78v6e9zl84TD/2v4vPtn3CU+2eZLhzYfj7uru6FBF5BfyzflsztpsndEuYz0ZlzJK7A+pEWIdZhfag871OuPn4Vem43q4erBwyELOFpwFoKioiMT1icT0iMHNzfr3NMAroMomS6CESUSEYksx6zLWEZ8az/qM9bZvwep41WF48+GMjBhJqG+og6MUcR4uJhf6NejHA40fYPmh5cxMmklmXiZ/2fwX5iTP4Zl2zzC48WBcXVwdHarIXcswDA6cP2C7F2l7znaKLEW2/e4u7nSs29F2L1KTWk0qPGQupEYIITVCADCbzRx2O0x0QDTu7s7x5YkSJhG5a+Xk5/DFgS9YfGAx2XnZtvKu9boSGxHLfQ3uw93FOS7mIlWRm4sbw5oNY1DjQSw+sJgPd39IxqUMfr/+98zaM4vJ7SfTt0HfKnvfgkh1k3sll02Zm2y9SDn5OSX21/etb0uQOoV0wsfdx0GRVi1KmETkrmIxLGzK3ER8Wjxrjq+h2CgGwN/Tn2HNhjEyYiQNazZ0bJAi1YyHqwejo0YzrNkwFuxfwOy9szl04RAvrHmBFnVaMKX9FGJCY5Q4idxhFsPC/rP7ScywJki7Tu2y/d0D8HL1olNIJ9u9SPr7d2NKmETkrnC24CxL05eSkJrAiUsnbOUdgjsQGxlLv4b98HT1dGCEItWft5s3E1tNJDYilk/2fcInyZ+w78w+nv72aToEd+C5Ds/RsW5HR4cp4tTOFZxjY+ZG24x2ZwrOlNjfuFZj271IHep20GQsZaCESUSqLcMw2HZyGwmpCaw+tto2NtvP3Y+hTYcSGxFLs9rNHBylyN3Hz8OPZ9s9y+io0czeM5uFqQvZkbODCV9PICYshintp9CyTktHhyniFIotxew9s9d2L9Ke03tKzEjn4+ZDl3pd6BHWg5iwGMJ8wxwYrXNSwiQi1c6Fwgv89+B/iU+L5/CFw7byVnVaERcZR/9G/TUuW6QKCPAK4H87/S+PtHiED3d/yBcHvrB96OvXsB/PtnuWpv5NHR2mSJVz+vJp2+/KhqwNXCi8UGJ/RO0I6zC70B60D26vmSlvkxImEakWDMNgz+k9xKfG8/WRr23rRXi7eTO4yWBiI2JpUaeFg6MUkRupW6Mu/6/b/2NCqwnMTJrJ8kPLWX10Nd8d+44hTYbwVNunCPcLd3SYIg5jtpjZfWq37V6klLMpJfb7ufvRLbQbPcJ60D20O3Vr1HVQpNWTEiYRcWp55jy+OvQV8anxpJ5LtZU3r92cURGjGNxkML4evg6MUETKKtwvnL/1/BsTW01kRtIMvj32LV8e/JIVh1YwImIET7R5QgtGy10jOy/b2ouUmcjGzI1cMl8qsb9FnRbEhMbQs35PWge21hqBdqSWFRGnlHImhYS0BL469BX5RfkAeLp60r9Rf2IjYmkb1FYzbok4qWa1m/HWfW+RfDqZaTunkZiZyOepn7M0fSmjo0YzsdVEanvVdnSYInfUleIr7MjZYetFSj+fXmK/v6c/3UO723qR6njXcVCkdx8lTCLiNC4XXebrw1+TkJbAntN7bOWNajYiLjKOXzX9FbU8azkwQhG5k1oGtuT9fu+zLXsb03ZOY0fODuYmzyUhLYFHWjzCoy0exc/Dz9FhilTYiYsnbAnS5uzNXC66bNvnYnKhdWBr271ILeq00GLPDqKESUSqvIPnD5KQlsCX6V9y0XwRsC6I2bdBX+Ii47in7j3qTRKpxu4JuYe5A+aSmJnIuzveJeVsCu/vep8F+xcwqdUkHo56GG83b0eHKXJLBUUFbDu5zZYkHck9UmJ/Ha86xITF0DOsJ13rdcXfy98hcUpJSphEpEq6UnyFb49+S3xaPNtPbreVh/mGERsRy7BmwzQcQeQuYjKZrNMih8bw7bFvmbZzGocvHObf2//Np/s+5Yk2TzCi+QjNBiZVimEYHMk9Yk2QMtezLXubbVIiAFeTK+2C21nXRQrrQUTtCFxMLg6MWG5ECZOIVCnHco+xKG0RS9OXcq7wHGD9g9Krfi/iIuPoFtpNf0xE7mImk4l+Dftxf/j9LD+0nJm7ZpJxKYO/bv4rc5Pn8nTbpxnSZIiGLonD5Jvz2Zy1mcRMay9SxqWMEvvr+tS1JUhd6nXRsFInoIRJRBzObDGz5vgaElIT2Ji10VZe16cuIyJGMLzZcE2RKiIluLq48mCzBxnUeBCLDyzmw90fknEpgz8k/oFZe2cxud1k+jbsqy9YxO4MwyD9fDrrM9aTmJHI9pzttoXSAdxd3OlQtwM9w3oSExpDU/+mGkbuZJQwiYjDZF3KYtGBRXxx4AtOXz4NgAkTMWExxEXE0bN+T02TKiI35e7qzsNRD/NgswdZuH8hs/bO4vCFw/x27W+JDohmSvsp9AjroQ+ockflXsllc9Zm1mesZ33GenLyc0rsD/MNo0dYD3qG9aRTSCctlu7k9ElERCpVsaWYxMxE4lPjWZexDothASDAK4ARzUcwImIEYb5hDo5SRJyNt5s3j7V6jNiIWD7Z9wmf7PuElLMpPPPdM7QPbs9z7Z/jnpB7HB2mOCmLYWH/2f22yRp2ndpFsVFs2+/p6kmnkE62oXYN/BooSa9GlDCJSKU4lX+KJelLWJS2iKy8LFt5l5AuxEbGcn/4/bpZW0Rum6+HL8+0e4bRUaOZs3cO8/fPZ2fOTh5b9RjdQ7vzXPvnaBnY0tFhihM4X3CeDZkbSMxMJDEjkTMFZ0rsb1SzkS1B6li3I15uXg6KVOxNCZOI2I3FsLA5azMJaQn8cOwHigzrmO5anrV4sOmDjIwYSeNajR0cpYhUR7W9avPCPS8wrsU4Ptz9IYvTFrMhcwMbMjfQp0EfJrebTLPazRwdplQhxZZi9pzew3eXv2PhqoUkn0nGwLDt93bzpku9LvQM60n30O7U96vvwGilMilhEpE77mzBWZalL2NR2iKOXTxmK28f3J7YiFgeaPQAnq6eDoxQRO4WwT7B/KHrH5jQcgIzd81k+aHlfHfsO74/9j1Dmgzh6XZPE+4X7ugwxUFOXz7NhswNrM9Yz8bMjZwvPG/d8dPM381rN6dHqLUXqX1we42EuEs5XcI0Y8YM/vnPf5KdnU3btm2ZNm0anTt3dnRYIneVzdmbeSf3Hepk16FHeA/AOkvQjpwdxKfGs/roaswWMwC+7r4MaTKE2MhYImpHODJsEbmL1ferz197/JVJrSYxPWk6q4+u5r+H/svKwysZ3nw4T7R5QrNx3gWKLEXsOrXLdi9SytmUEvt93X1pSENGdBjBveH36pwQwMkSps8//5wXXniB999/ny5duvD222/Tv39/UlNTCQ4OLvNxrhRf4UrxlevKXUwuJWbkulGdq0yYSnzLUJ665mIzV4qvYLZY/zVcjJvW/Xl38K2OW1pdAA9XjwrVLbIU2W7Mv9267i7utpsg7VW32FJcavveqO7Pb9r8JTcXN9uUtFWhrsWwlJiq9JdcTa62tUfsVbfYUszbO97mZPFJ3t7xNpF1IllxaAWL0xZzMPcgJkyYTCZa1mlJbEQsfRr0sc0O9Mvfk58f1zAMW5J1Iz///bRX3RvFWNG6t3uNKO381TXi9usWW4pLbd8b1XX0772zXSMshuWm7Vue33t7XCPq+9XnzXvfJO1cGtN2TmPdiXUsTF3IkvQlxEXE8Virx6jtVfuGx4WqcY0oMopKbV9dI66vm52XzY/HfyQxM5HNWZu5aL5Yol507Wh61u9Jz/o9ifaPZuXKlfRv1B93d/cb/lx0jbj9ulXlGnGz37ufMxmGUfrZXsV06dKFTp06MX36dAAsFgvh4eFMmTKFqVOn3vL5ubm51KpVi6nLp+JZ4/rhQM0DmjO2zVjb47/++NdSG76RfyMmtJtge/xm4pvkm/NvWDfUL5QnOj5he/z2prc5k3eGAwcO0Lx5c1xdry2uF+QTxLOdn7U9nrFlBqfyT93wuP5e/jzf9Xnb4w+3f0jmxcwb1vVx9+GlmJdsj+cmzeXI+SM3rOvu4s7v7/297fG83fM4cPbADesCvNb7Ndv/45Pj2XdqX6l1X+n5iu3CuHT/UpKyk0qt+2L3F6nhUQOAr9K+Ymvm1lLrPt/1efy9/AH45uA3rDuy7obtC/BMp2cIrmFNsNccWcOaI2tKPe6vO/yasJrWGdsSjyWy+tDqUutOaDeBRv6NANiSsYUVB1aUWndM6zFE1LH2tiRlJ7F0/9JS68a2iKVlsPUG5eScZBL2JZRad1jUMNqFtAMg7Uwa8/fML7XuoOaD6Bxm7Z09cv4Ic5Pmllq3X5N+xDSIAWBp2lKeWHHtfHY1udou1O4u7vRr0o8Xu75Iy8CW5OTl8N7W90o9bvfw7jzQ9AHAenPt25veLrVup9BODI4YDEDelTz+ueGfpdZtF9KOYVHDAOuHkL+t+1updVsEtSCuZZzt8WtrXiu1bmVdI/6V+C+27tl6w/NX14hrKnqNWJG6gk9++OSG7Qu6RlxV0WvEkbNHmBo/tdT27d2oN70b9QaoEteIzZmbeeKrJ2yT0bi7uNM2qC1tg9vi6epZ5a4RZrOZp+Y+RWiT0Bu2r64R1gQiKy+LpkFN2XJyC+nn07mYf5H8Qmv7erl5Ee4XTgO/BoT7hePj7mO7RpjNZv668K8UhxbfsH1B14irqsM1ojCvkH8M+QcXLlygZs2apT7XaXqYrly5wvbt23n55ZdtZS4uLvTt25eNGzfe8DmFhYUUFhbaHufm5gJQXFxMcfH1mXhRcRFm87ULW7HlxvWuHqNE3VKOWVpdi8X6jcjVf+/UcUut61L2ui64lKhbVFxUal2gZN2iW9c1WUxlrms2mcscg9nVbDtuae1rq2u+Vvemxy0qR93yHPdndc1F5jLHUK665pvXLSoqKnfdgqIC3tr+Vol9xUYxAZ4BtKjTgub+zenXpB8RtSJs7/GOxVBc/njB+i1oWesCZY4B7HuNgBufv7pGlKxb0WsE3Lh9bXV1jajQNQKgyHzz9q3oce11jWgV0IpfNfkVxy8eZ+vJrZy6fIptJ7ex5/Qe2gW1o1GtRlXqGnH139La9269Rly8cpHjl45z/OJxMi5lYLaYCTodhIuLCyZMNKzZkBouNQj3DSfIO6jElN9X28Hsar5l+16NQdeI6nGNuFn9n3OaHqbMzEzCwsLYsGED3bp1s5W/9NJLrF27ls2bN1/3nNdee43XX3/9uvKPP/sYH5/rFxBzMbngarqW6d6sW89kMuFmcqtQ3SKjiNKa3V51wfqNT0XqFhvFt+zyLmtdN5PbtW5sJ6jranK91uVdBepaDMtNu91/fg7fybp5Rh7brmxjY+FGCoyC64ZhjPMZR3P35tcd1zAM28x4t4qhKtSFm/8u6xpx47q6RugaUdG6VeH3/ud1DcMgpSiF7wu+55TF2ivjZ/LjPu/7uMfjHtxMbrpGVKCuPa4RZsNMujmdA+YDpBelc9pyukQ9X5MvEW4RRHhE0MytGZ4mT10jqBq/91XpGpGfn8/4ceNv2cNUrROmG/UwhYeHc/r06Zs2SmUwm82sXr2afv364e6uGVfuNLXv7TMMg92nd7MwbSHfHfuu1IuQi8mFqNpRfNr/Uy3Sd4fo/LUvta99VYf2LbYUs/LoSj7Y/QEZeRkA1KtRjydaPcHgxoNL3KdU2apD+1aEYRgcu3iMxMxENmRtYHvOdgqLr33GczW50iawDTGhMXSv152I2hG2JKE87tb2rUxVqY1zc3MJDAysPkPyAgMDcXV15eTJkyXKT548SUhIyA2f4+npiafn9fcqubu7O/wHdFVViqU6UvuW35XiK6w6sop5KfNIPpNsK29WqxnpF9Kvq28xLOw7u4+tp7YSExZTmaFWezp/7Uvta1/O3L7uuPNQxEMMaTqEJelL+GDXB2TlZfH65tf5OOVjJrefTL+G/Sr0gfyOxejE7VtW+eZ8tmRvYX3GetZnrCfjUkaJ/XV96toWju1Srwt+Hn537LXvhvZ1tKrQxmV9fadJmDw8POjYsSPfffcdw4YNA6xjH7/77jsmT57s2OBEqoFT+aeIT4snPjWeswVnAfBw8WBQk0GMjhzNnzb9CROmG86KZMLEtJ3T6B7aXb1MIlJtuLu6ExcZx6+a/orPUz/noz0fcST3CP+79n+JCohiSvsp9AzrqeveHWIYBunn061TfmeuZ8fJHSWGKrq5uNGxbkfbukhN/Zuq7aVSOE3CBPDCCy8wfvx47rnnHjp37szbb79NXl4ejz32mKNDE3Fau0/tZl7KPL458o1t2F2wTzAPRz7MiIgRBHgFcKX4Ctl52aVOIWtgkJ2XjdliLjE9rIhIdeDl5sX4luMZ0XwEn6Z8yifJn7D/7H6e/e5Z2gW147kOz9EppJOjw3RKF69cZFPWJtu6SCfzS44kCvMNs/UidQ7pbFumQqQyOVXCNGrUKE6dOsUf//hHsrOzadeuHV9//TV162pRMZHyMBebWXV0FfNT5rPn9B5befvg9oyJHkOfBn1K3ITr4erBwiELbT1PRUVFJK5PJKZHDG5u1stIgFeAkiURqdZ8PXx5uu3TjI4czezk2SxIWUDSqSQmrppIt3rdeK7Dc7QKbOXoMKs0i2Eh9WwqiZmJrDuxjl2ndpWYAMDT1ZN7Qu6hZ1hPYkJjaFizoXqRxOGcKmECmDx5sobgiVTQ6cunSUhNID4tntOXrTMKubu4M7DxQMZEj6FlnZalPjekRgghNaz3C5rNZg67HSY6INrh449FRCqbv5c/L3R8gUeiH+HD3R+y6MAiNmZtZONXG7k//H4mt59M89rNHR1mlXG+4DwbszayPmM9iRmJnCk4U2J/o5qNbL1IHet2xMvNy0GRityY0yVMIlJ+e0/vZV7KPL4+8rVtJe4g7yBGRY5iZMRI6njXcXCEIiLOJ8gniN93/T0TWk1gZtJM/nvov3x//Ht+OP4Dg5oM4pm2z9CgZgNHh1npii3FJJ9Jtt2LtPf03hLTXnu7edOlXhd6hPYgJiyG+n71HRityK0pYRKppszFZlYfXc28/fPYfWq3rbxtUFvGRo+lb8O+JYbdiYhIxYT5hvGXHn9hYquJzEiawTdHv+GrQ1+x6vAqhjUfxpNtnrT10FdXpy+fZmPmRtZlrGNj5kbOF54vsb+ZfzPrMLuwGNoHt9cQbnEqSphEqpnTl0+TkJZAQmoCpy5bF160DbuLGkPLwNKH3YmISMU18W/Cv3r/i5QzKUzbOY11GetYlLaIL9O/ZFTUKB5v/TgBXgGODvOOKLIUsfvUbtuU3ylnU0rs93X3pVtoN3qE9aB7aPdqnzBK9aaESaSaSD6dbBt2d3Ua1kDvQNuwu0DvQAdHKCJyd4iuE817fd9jZ85O3tnxDttPbufTfZ+yOG0x41qMY3zL8dT0KH2RzKoqOy+bDZkbWJ+xnk2Zm7hovlhif3RAtO1epNZBrTWKQaoNJUwiTsxsMfPt0W+ZlzKPXad22crbBLVhbNRY+jXsh7ur/mCJiDhC++D2zOk/h42ZG3l357skn0nmw90fsnD/Qh5r9RhjosZU6WmyzcVmduTssN2LdODcgRL7a3nWontod1svkr6Yk+pKCZOIEzpz+QyL0hYRnxpPzuUcwLqg34BGAxgTNYbWQa0dHKGIiACYTCa6h3WnW2g3vj/2PdOTppN+Pp13drzDZ/s+49dtfk1sRGyVuacn41IG60+sZ33mejZnbeZy0WXbPhMmWge2pkeYdbKGlnVa4uri6sBoRSqHEiYRJ7LvzD7mpcxj5eGVtmF3dbzqMCpyFLGRsfp2T0SkijKZTPRp2Ife4b1ZcXgF7yW9x4lLJ/jHln/wcfLHPN32aYY2HYqbS+V+NCsoKmD7ye22e5GO5B4psT/AK8A2zK5bvW74e/lXanwiVYESJpEqzmwx892x75ifMp+dOTtt5a3qtGJsi7H0b9hfw+5ERJyEq4srQ5sOZUDjASxNX8r7u94nKy+LP274I7P3zubZds/yQKMHcDG52OX1DcPgaO5REjMTWZ+xnm3Z2ygoLrgWn8mVtkFtbUlSZECk3WIRcRZKmESqqLMFZ1mctpiFqQvJyf9p2J3JjQcaPcDY6LG0CWrj4AhFRKSi3F3ciY2IZWiTocSnxvPRno84knuEF398kY/2fMSU9lO4t/69mEym236tfHM+W7K32BaOPXHpRIn9wT7Btim/u9Tr4pQTUojYkxImkSpm/9n9zEuZx4pDK7hiuQJYh0TERcYRGxFLsE+wgyMUEZE7xcvNi0dbPsqIiBF8tu8z5ibPJfVcKpO/n0zboLY81/45OtfrbKu/OXsz7+S+Q53sOvQI73HDYxqGwcHzB0nMTGRdxjp2nNxhG8YN1nteOwZ3tN2L1My/2R1JzESqKyVMIlVAkaWI7499z7yUeezI2WErb1GnBeOix9G/Uf8qc0OwiIjceTXca/Bk2yd5OOphZu+dzfyU+ew6tYtJ30yia72uPNf+OVoFtmJa0jROWU4xLWkaMfVjbInOxSsX2Zy12dqLlJlIdl52ieOH+YbZhtl1DulcpWfnE6lqlDCJONC5gnMsPrCYz1M/t/1xczO50a9hP8ZEj6FtUFt96ycichep5VmL/+n4P4yLHsd/9vyHhLQENmVtYlPWJtoEtmHf2X0A7Du7j4X7F5JXlMf6jPXsytlFkVFkO46nqyf3hNxDj1BrL1Kjmo3090SkgpQwiThA6tlU5u+fz1eHvqKwuBCwDrsbGTGSuIg46tao6+AIRUTEkYJ8gnilyyuMbzme93e9z7L0Zew+vbtEnb9t+VuJx41qNrINs7un7j14uXlVZsgi1ZYSJpFKUmQpYs3xNcxLmce2k9ts5dEB0YxrYR125+nq6bgARUSkygnzDePPMX+mbVBbXt/4+nX72wS2YWjTocSExRDuF+6ACEWqPyVMInZ2ofACiw8sZuH+hWTlZQHWaVv7NezH2OixGnYnIiI3ZRgGi9IW4WJywWJYbOUuJheKjWJGRY7S3xERO1LCJGInaefSmJ9iHXZ3dY2L2p61rcPuIuMIqRHi4AhFRMQZbMjcQPKZ5OvKLYaF5DPJbMjcQExYjAMikzvFYrFw5coVR4dRKcxmM25ubhQUFFBcXGzX13J3d8fV1fW2j6OESeQOKrYUs+bEGuanzGdL9hZbeVRAFGOjxzKw8UANuxMRkTIzDINpO6dhwoSBcd1+Eyam7ZxG99Du6mVyUleuXOHw4cNYLJZbV64GDMMgJCSE48ePV8o56+/vT0hIyG29lhImkTvgQuEFlhxYwoL9C8jMywSsw+76NOjD2OixtA9urz9kIiJSbmaLmey87BsmSwAGBtl52ZgtZi0/4YQMwyArKwtXV1fCw8NxcXFxdEh2Z7FYuHTpEr6+vnZ9v4ZhkJ+fT05ODgD16tWr8LGUMInchgPnDjB//3yWH1xuG3bn7+nPyIiRjIocpWF3IiJyWzxcPVg4ZCFnC84CUFRUROL6RGJ6xODmZv0YF+AVoGTJSRUVFZGfn09oaCg+PnfH2lhXhx96eXnZPUH09vYGICcnh+Dg4AoPz1PCJFJOxZZi1p5Yy/yU+WzO3mwrj6wdaRt2p6lcRUTkTgmpEWL7As5sNnPY7TDRAdG4u7s7ODK5XVfv4fHwUMJrL1cTUbPZrIRJxN4uFF5gafpSFuxfQMalDMA6Q1GfBn0YEzWGjnU7atidiIiIlJs+P9jPnWhbJUwit3Dw/EHmp8znv4f+y+Wiy4B1JfYRzUfwcOTD1POt+JhYEREREanalDCJ3ECxpZh1GeuYlzKPTVmbbOXNazdnbNRYBjUZhLebtwMjFBEREbEqthhsOXyWnIsFBPt50blxAK4u6rW6U6r/VBwi5ZB7JZdPkj9hyJIhTPl+CpuyNuFicqFvg77M7j+bxUMXMyJihJIlERERqRK+3ptFjze+Z/R/NvGbhUmM/s8merzxPV/vzbLr606YMAGTyYTJZMLd3Z3GjRvz0ksvUVBQYKtzdf+mTZtKPLewsJCgoCBMJhNr1qyxla9du5b777+fgIAAfHx8aN68OePHj7etUbVmzRrbMX+5ZWdn2+29qodJBDh0/hDz98/ny4Nf2obd+Xn4MbL5SEZFjSLMN8zBEYqIiIiU9PXeLJ7+bMd1k85nXyjg6c92MHNcBwa0st+tAwMGDGDOnDmYzWa2b9/O+PHjMZlMvPHGG7Y64eHhzJkzh65du9rKli9fjq+vL2fPnrWV7du3jwEDBjBlyhTeffddvL29OXDgAIsXL75ugdvU1FRq1qxZoiw4ONhO71IJk9zFLIaF9RnrmZcyjw2ZG2zlzfybMSZ6DIMbD8bH/e6Y4lNEREQczzAMLpuLb10R6zC8V79MvuEKXQZgAl77ch8xzQLLNDzP29213BMkeHp6EhJincExPDycvn37snr16hIJ0/jx43n33Xd5++23bdN8z5s3j0cffZS//OUvtnrffPMNISEhvPnmm7aypk2bMmDAgOteNzg4GH9//3LFejuUMMld5+KViyxLX8b8/fM5fvE4YF0pvXd4b8ZGj6VzSGfNViMiIiKV7rK5mBZ/XHVHjmUA2bkFtH7tmzLV3/en/vh4VDw12Lt3Lxs2bKBhw4Ylyjt27EijRo1YvHgx48aN49ixY2zYsIGZM2eWSJhCQkLIysrixx9/5N57761wHPaghEnuGocvHGbB/gUsS19GflE+AH7ufgxvPpyHox6mvl99B0coIiIi4jyuDq0rKiqisLAQFxcXpk+ffl29iRMnMnv2bMaNG8fHH39Mv379CAoKKlEnNjaWVatW0atXL0JCQujatSt9+vTh0UcfvW74Xf36JT+zNWzYkOTk5Dv/Bn+ihEmqNYth4ccTPzJ//3wSMxJt5U1rNWVM9BiGNBmiYXciIiJSJXi7u7LvT/3LVHfL4bNMmLP1lvXmPtaJzo0DyvTa5XXfffcxc+ZM8vLyeOutt3Bzc2PEiBHX1Rs3bhxTp07l0KFDfPzxx/ztb3+7ro6rqytz5szhL3/5C99//z2bN2/mb3/7G2+88QZbtmyhXr1r92KtW7cOPz8/22N7L+KshEmqpUvmS2ws3MiHyz/k2MVjgHXYXa/wXoyNHkuXkC4adiciIiJVislkKvOwuJ7Ng6hXy4vsCwU3vI/JBITU8qJn8yC7TTFeo0YNmjVrBsDs2bNp27Yts2bNYtKkSSXq1alThyFDhjBp0iQKCgro169fqccMCwvjkUce4ZFHHuHPf/4zERERvP/++7z++uu2Oo0bN9Y9TCIVdeTCERbsX8DS9KUlht091PwhHo56mHC/cAdHKCIiInL7XF1MvDq0BU9/tgMTlEiarqZHrw5tUWnrMbm4uPDKK6/wwgsvMGbMGNsED1dNnDiRQYMG8dJLL+Hq6orFYrnlMWvXrk29evXIy8uzV9hlooRJnJ7FsLAhcwPzUuaxPmO9rTzIJYjHOz7OsObDNOxOREREqp0Breoxc1wHXv/vPrIuXFv/KKSWF68ObWHXKcVvJDY2lhdffJEZM2bwv//7vyVjHTCAU6dO4evrW2Ktpqs++OADkpKSeOihh2jatCkFBQV88sknJCcnM23atBJ1c3JyrjtGnTp17DY0TwmTOK08cx7L0pexYP8CjuQeAazD7u6tfy+jmo/izI4zDG4+2O7jWkVEREQcZUCrevRrEcKWw2fJuVhAsJ8XnRsHVFrP0s+5ubkxefJk3nzzTZ5++ukS+0wmE4GBgVgslhsmTJ07d2b9+vU89dRTZGZm4uvrS8uWLVm6dCm9evUqUTcyMvK652/cuLHEWk93khImcTrHco+xYP8ClqQvIc9s7aL1dfdlWLNhjI4aTYOaDTCbzawwrXBwpCIiIiL25+piolvTOpX6mnPnzr1h+dSpU5k6dSpgXVeqNP7+/iX2t2/fnk8//fSmr9m7d++bHtNelDCJUzAMg42ZG5m3fx7rTqzD+GmkbqOajRgTPYZfNf0VNdxrODhKEREREalulDBJlZZvzufLg18yf/98Dl84bCvvGdaTsdFj6RbaDReTiwMjFBEREZHqTAmTVEnHc4+zIHUBSw4s4ZL5EgA13GvYht01rNnwFkcQEREREbl9SpikyjAMg01Zm5ifMp+1J9baht01rNmQ0VGjebDpg/h6+Do4ShERERG5myhhEofLN+ez/NBy5qXM49CFQ7bymLAYxkaNJSYsRsPuRERERMQhlDCJwxy/eJyF+xey5MASLpovAuDj5sOwZsN4OOphGtdq7OAIRURERORup4RJKpVhGGzO3sy8lHmsPX5t2F0DvwaMiR6jYXciIiIiUqUoYZJKcXXY3YL9C0g/n24rjwmNYUz0GHqE9dCwOxERERGpcpQwiV1lXMpg4f6FLD6wmItXrMPuvN28ebDpg4yOHk2TWk0cHKGIiIiISOmUMMkdZxgGW7O3Mn//fH44/gMWwwJAfd/6jIkew7Bmw/Dz8HNwlCIiIiJO7vxxyD9T+n6fOuAfXnnxVFNKmOSOuVx0ma8OfcW8lHklht11q9eNsdFj6RHWA1cXVwdGKCIiIlJNnD8O0ztCUWHpddw8YfJ2uyRNEyZM4OOPP+bJJ5/k/fffL7Hv2Wef5b333mP8+PHMnTvXVr5x40Z69OhBnz59+Prrr0s858iRIzRufOMJvzZu3EjXrl3v+HsoKyVMctsyL2WyMHUhi9MWk3slF7AOu/tV018xOmo0Tf2bOjhCERERkWom/8zNkyWw7s8/Y7depvDwcBYuXMhbb72Ft7c3AAUFBcyfP58GDRpcV3/WrFlMnjyZ2bNnk5mZSf369a+r8+2339KyZcsSZXXq1LFL/GWlhEkqxDAMtp3cxvyU+Xx//HvbsLsw3zBGR43moeYPUdOjpoOjFBEREXEihgHm/LLVLbpc9npX8m5dz90HTKayHfMnHTp04ODBg3zxxReMHTsWgC+++IIGDRpc11t06dIlPv/8c7Zs2cLx48f5+OOP+f3vf3/dMevUqUNISEi54rA3JUxSLgVFBaw4vIJ5KfNIO5dmK+9Srwtjo8Zyb/17NexOREREpCLM+fC30Dt7zNkDylbvlUzwqFHuw0+cOJE5c+bYEqbZs2fz2GOPsWbNmhL14uPjiYqKIjIykri4OP7whz/wyiuvYCpnkuYISpikTLLzslm4fyGLDiziQuEFALxcvRjadChjosbQrHYzB0coIiIiIpVt3LhxvPzyyxw9ehSAxMREFi5ceF3CNGvWLMaNGwdA3759mTJlCmvXrqV3794l6nXv3h0Xl5JLzVy6dMlu8ZeFEiYplWEY7MjZwbyUeXx/7HuKjWLg2rC7Yc2GUcuzloOjFBEREakm3H2sPT1lkb27bL1HE7+GkDZle+0KCAoKYvDgwcydOxfDMBg8eDCBgYEl6qSmprJlyxaWLFkCgJubG3FxccyaNeu6hOnzzz8nOjq6QrHYixImuU5hcSErDq1g/v757D+731beJaQLY6LH0Kt+Lw27ExEREbnTTKayD4tz8y57vQoMtSuPiRMnMnnyZABmzJhx3f5Zs2ZRVFREaOi14YaGYeDp6cn06dOpVevaF/Dh4eE0a1a1Ri4pYRKb7Lxs4lPjWZS2iHOF5wDrsLshTYcwOmo0EbUjHByhiIiIiFQ1AwYM4MqVK5hMJvr3719iX1FREZ988gn/+te/eOCBB7BYLFy6dAlfX1+GDx/OggULeOqppxwUedkoYbrLGYZB0qkk5qXM49uj39qG3dWrUY/RUaMZ3ny4ht2JiIiIVDU+dazrLN1qHSYf+0/J7erqSkpKiu3/P7d8+XLOnTvHpEmTqFWrFhaLhdzcXGrWrMmIESOYNWtWiYTpzJkzZGdnlziGv78/Xl5edn8fpVHCdJcqLC7k68NfMy9lHilnU2zlnUI6MTZqLL3Ce+HmotNDREREpEryD7cuSpt/pvQ6PnXstgbTL9WseePlZGbNmkXfvn1LDLu7asSIEbz55pvs3r3b9vy+ffteV2/BggU8/PDDdzbgctAn4rvMybyTfJ76OYsPLOZswVkAPF09GdLEOuwuMiDSwRGKiIiISJn4h1daQvRLc+fOven+pUuX3vIYnTt3xjAM2+Of/78qUcJ0FzAMg12ndtmG3RUZRQCE1Ajh4ciHGdF8BP5e/o4NUkRERESkClLCVI1dKb7C10esw+72ndlnK+9YtyNjo8dyX/h9GnYnIiIiInIT+rRcDeXk5xCfGk9CWoJt2J2HiweDmwxmTPQYogKiHByhiIiIiIhzUMJUTRiGwe7Tu5mXMo/VR1bbht0F+wQzOmo0I5qPoLZXbQdHKSIiIiLiXJQwObkrxVdYdWQV81Pms/fMXlt5h+AOjIkew/0N7sfdxd2BEYqIiIiIOC8lTE7qVP4pEtISiE+N50yBdTpJdxd3BjUexJjoMbSo08LBEYqIiIiIOD8lTE5mz6k9zNs/j1VHVlFk+WnYnXcwo6JGMaL5COp4239xMhERERGRu4USJidgLjbzzdFvmJ8yn92nd9vK2wW1Y2z0WPo07KNhdyIiIiIidqCEqQo7ffm0bdjd6cunAeuwu4GNBzImagwtA1s6OEIRERERkepNCVMVtPf0XuanzGflkZW2YXdB3kHERcYxMmIkgd6BDo5QREREROTu4OLoAO5Wm7M3807uO2zO3gxYh92tOLSCsSvGMvqr0fz30H8pshTRNqgtb977JqtGrOKptk8pWRIRERGR62zM3MiDSx9kY+bGSnm9CRMmYDKZMJlMuLu707hxY1566SUKCgoAeP3113nggQdo1aoVo0ePprCwsFLisgf1MDmAYRhMS5rGKcsp3trxFnvO7CEhLYFTl08B4ObixsBGAxkTPYZWga0cHK2IiIiIVGWGYfDOjnc4dOEQ7+x4h671umIymez+ugMGDGDOnDmYzWa2b9/O+PHjMZlMvPHGG7z88st4eHgA0Lx5cw4dOkR0dLTdY7IHJUwOsCFzA/vO7gMg7XwaaefTAAj0DiQuMo7YiFj1JImIiIjcZQzD4HLR5XI/b1PmJpLPJAOQfCaZH479QNfQruU6hrebd7mTLE9PT0JCQgAIDw+nb9++rF69mjfeeMOWLP3xj39k+PDhTpssgRKmSmcYBv/c+s8SZd6u3vyx2x/p36g/7q6a7U5ERETkbnS56DJd5ne57eP8Zs1vyv2czWM24+PuU+HX3Lt3Lxs2bKBhw4YA5Obm8tRTT9GtWzemTJlS4eNWBbqHqZJtyNzAwQsHS5RdLr5Mba/aSpZERERExGksX74cX19fvLy8aN26NTk5Obz44osAPPLII3z33XfMmzePrl27kpiY6OBoK049TJXIMAym7ZyGi8kFi2GxlbuYXJi2cxrdQ7tXynhTEREREal6vN282Txmc5nrG4bBY6seI/Vc6nWfLSNrRzKn/5wyf7b0dvMud7z33XcfM2fOJC8vj7feegs3NzdGjBgBwLJly8p9vKpKCVMl2pC5wTa+9OcshoXkM8lsyNxATFiMAyITEREREUczmUzlGhaXmJFIytmU68othoWUsykknUqy62fLGjVq0KxZMwBmz55N27ZtmTVrFpMmTbLbazqChuRVkqu9SyZunOWbMDFt5zQMw6jkyERERETE2VS1z5YuLi688sor/OEPf+Dy5fJPXFGVKWGqJGaLmey8bAxufNIaGGTnZWO2mCs5MhERERFxNlXxs2VsbCyurq7MmDGj0l6zMmhIXiXxcPVg4ZCFnC04C0BRURGJ6xOJ6RGDm5v1xxDgFYCHq4cjwxQRERERJ/DLz5Y3UtmfLd3c3Jg8eTJvvvkmTz/9NDVq1Ki017YnJUyVKKRGCCE1rHPVm81mDrsdJjogGnd3zY4nIiIiIuXz88+WlW3u3Lk3LJ86dSpTp06t3GDsTEPyRERERERESqGESUREREREpBRKmERERERERErhNAnTX//6V7p3746Pjw/+/v6ODkdERERERO4CTpMwXblyhdjYWJ5++mlHhyIiIiIicsdoHU77uRNt6zSz5L3++utA6TNyiIiIiIg4E1dXV8DaMeDt7e3gaKqn/Px8gNualdppEqaKKCwspLCw0PY4NzcXsE7pbTY7doHYq6/v6DiqK7Wvfal97Uvta19qX/tS+9qX2te+Krt9DcPAy8uLnJwcXF1dcXFxmsFfFWYYBleuXOHy5cuYTCa7vk5+fj6nTp2iZs2aWCwWLBZLiTpl/TmbDCfrA5w7dy7PP/8858+fv2Xd1157zdYz9XPz58/Hx8fHDtGJiIiIiJSdi4sLQUFBWpfTDiwWCxcvXuTixYs33J+fn8+YMWO4cOECNWvWLPU4Du1hmjp1Km+88cZN66SkpBAVFVWh47/88su88MILtse5ubmEh4fzwAMP3LRRKoPZbGb16tX069dPvyB2oPa1L7Wvfal97Uvta19qX/tS+9qXo9rXYrFgNpvvinuZioqK2LBhA927d8fNzX6piMlkws3NzTbs8Uaujj67FYcmTL/97W+ZMGHCTes0adKkwsf39PTE09PzunJ3d/cqc5GpSrFUR2pf+1L72pfa177Uvval9rUvta99OaJ9b/SZtToym80UFRXh6+vr8HO4rK/v0IQpKCiIoKAgR4YgIiIiIiJSKqeZ9OHYsWOcPXuWY8eOUVxcTFJSEgDNmjXD19fXscGJiIiIiEi15DQJ0x//+Ec+/vhj2+P27dsD8MMPP9C7d28HRSUiIiIiItWZ0yRMc+fOve01mK7eSFfWG7zsyWw2k5+fT25ursPHb1ZHal/7Uvval9rXvtS+9qX2tS+1r32pfe2vKrXx1ZzgVpNtOE3CdCdcnVIwPDzcwZGIiIiIiEhVcPHiRWrVqlXqfqdbh+l2WCwWMjMz8fPzs+tCWWVxdYrz48ePO3yK8+pI7Wtfal/7Uvval9rXvtS+9qX2tS+1r/1VpTY2DIOLFy8SGhp600WD76oeJhcXF+rXr+/oMEqoWbOmw0+W6kzta19qX/tS+9qX2te+1L72pfa1L7Wv/VWVNr5Zz9JVpadSIiIiIiIidzklTCIiIiIiIqVQwuQgnp6evPrqq3fNqs6VTe1rX2pf+1L72pfa177Uvval9rUvta/9OWMb31WTPoiIiIiIiJSHephERERERERKoYRJRERERESkFEqYRERERERESqGESUREREREpBRKmOzkxx9/ZOjQoYSGhmIymVi6dOktn7NmzRo6dOiAp6cnzZo1Y+7cuXaP01mVt33XrFmDyWS6bsvOzq6cgJ3I3//+dzp16oSfnx/BwcEMGzaM1NTUWz4vISGBqKgovLy8aN26NStWrKiEaJ1PRdp37ty51527Xl5elRSxc5k5cyZt2rSxLYjYrVs3Vq5cedPn6Nwtu/K2r87d2/OPf/wDk8nE888/f9N6Oocrpiztq3O4fF577bXr2isqKuqmz3GG81cJk53k5eXRtm1bZsyYUab6hw8fZvDgwdx3330kJSXx/PPP8/jjj7Nq1So7R+qcytu+V6WmppKVlWXbgoOD7RSh81q7di3PPvssmzZtYvXq1ZjNZh544AHy8vJKfc6GDRsYPXo0kyZNYufOnQwbNoxhw4axd+/eSozcOVSkfcG6IvrPz92jR49WUsTOpX79+vzjH/9g+/btbNu2jfvvv58HH3yQ5OTkG9bXuVs+5W1f0LlbUVu3buWDDz6gTZs2N62nc7hiytq+oHO4vFq2bFmivdavX19qXac5fw2xO8BYsmTJTeu89NJLRsuWLUuUjRo1yujfv78dI6seytK+P/zwgwEY586dq5SYqpOcnBwDMNauXVtqnbi4OGPw4MElyrp06WI8+eST9g7P6ZWlfefMmWPUqlWr8oKqZmrXrm189NFHN9ync/f23ax9de5WzMWLF43mzZsbq1evNnr16mX85je/KbWuzuHyK0/76hwun1dffdVo27Ztmes7y/mrHqYqYuPGjfTt27dEWf/+/dm4caODIqqe2rVrR7169ejXrx+JiYmODscpXLhwAYCAgIBS6+j8rbiytC/ApUuXaNiwIeHh4bf8Rl+siouLWbhwIXl5eXTr1u2GdXTuVlxZ2hd07lbEs88+y+DBg687N29E53D5lad9QedweR04cIDQ0FCaNGnC2LFjOXbsWKl1neX8dXN0AGKVnZ1N3bp1S5TVrVuX3NxcLl++jLe3t4Miqx7q1avH+++/zz333ENhYSEfffQRvXv3ZvPmzXTo0MHR4VVZFouF559/npiYGFq1alVqvdLOX90jdnNlbd/IyEhmz55NmzZtuHDhAv/3f/9H9+7dSU5Opn79+pUYsXPYs2cP3bp1o6CgAF9fX5YsWUKLFi1uWFfnbvmVp3117pbfwoUL2bFjB1u3bi1TfZ3D5VPe9tU5XD5dunRh7ty5REZGkpWVxeuvv07Pnj3Zu3cvfn5+19V3lvNXCZPcFSIjI4mMjLQ97t69OwcPHuStt97i008/dWBkVduzzz7L3r17bzr+WCqurO3brVu3Et/gd+/enejoaD744AP+/Oc/2ztMpxMZGUlSUhIXLlxg0aJFjB8/nrVr15b6oV7Kpzztq3O3fI4fP85vfvMbVq9erYkF7KAi7atzuHwGDhxo+3+bNm3o0qULDRs2JD4+nkmTJjkwstujhKmKCAkJ4eTJkyXKTp48Sc2aNdW7ZCedO3dWInATkydPZvny5fz444+3/BattPM3JCTEniE6tfK07y+5u7vTvn170tPT7RSdc/Pw8KBZs2YAdOzYka1bt/LOO+/wwQcfXFdX5275lad9f0nn7s1t376dnJycEiMfiouL+fHHH5k+fTqFhYW4urqWeI7O4bKrSPv+ks7h8vH39yciIqLU9nKW81f3MFUR3bp147vvvitRtnr16puOC5fbk5SURL169RwdRpVjGAaTJ09myZIlfP/99zRu3PiWz9H5W3YVad9fKi4uZs+ePTp/y8hisVBYWHjDfTp3b9/N2veXdO7eXJ8+fdizZw9JSUm27Z577mHs2LEkJSXd8MO8zuGyq0j7/pLO4fK5dOkSBw8eLLW9nOb8dfSsE9XVxYsXjZ07dxo7d+40AOPf//63sXPnTuPo0aOGYRjG1KlTjUceecRW/9ChQ4aPj4/x4osvGikpKcaMGTMMV1dX4+uvv3bUW6jSytu+b731lrF06VLjwIEDxp49e4zf/OY3houLi/Htt9866i1UWU8//bRRq1YtY82aNUZWVpZty8/Pt9V55JFHjKlTp9oeJyYmGm5ubsb//d//GSkpKcarr75quLu7G3v27HHEW6jSKtK+r7/+urFq1Srj4MGDxvbt242HH37Y8PLyMpKTkx3xFqq0qVOnGmvXrjUOHz5s7N6925g6daphMpmMb775xjAMnbu3q7ztq3P39v1yFjedw3fWrdpX53D5/Pa3vzXWrFljHD582EhMTDT69u1rBAYGGjk5OYZhOO/5q4TJTq5OY/3Lbfz48YZhGMb48eONXr16Xfecdu3aGR4eHkaTJk2MOXPmVHrczqK87fvGG28YTZs2Nby8vIyAgACjd+/exvfff++Y4Ku4G7UrUOJ87NWrl62tr4qPjzciIiIMDw8Po2XLlsZXX31VuYE7iYq07/PPP280aNDA8PDwMOrWrWsMGjTI2LFjR+UH7wQmTpxoNGzY0PDw8DCCgoKMPn362D7MG4bO3dtV3vbVuXv7fvmBXufwnXWr9tU5XD6jRo0y6tWrZ3h4eBhhYWHGqFGjjPT0dNt+Zz1/TYZhGJXXnyUiIiIiIuI8dA+TiIiIiIhIKZQwiYiIiIiIlEIJk4iIiIiISCmUMImIiIiIiJRCCZOIiIiIiEgplDCJiIiIiIiUQgmTiIiIiIhIKZQwiYiIiIiIlEIJk4iI3LV69+7N888/f9M6jRo14u23366UeEREpOpRwiQiIk5twoQJmEym67b09HRHhyYiItWAm6MDEBERuV0DBgxgzpw5JcqCgoIcFI2IiFQn6mESERGn5+npSUhISInN1dWVtWvX0rlzZzw9PalXrx5Tp06lqKio1OPk5OQwdOhQvL29ady4MfPmzavEdyEiIlWRephERKRaysjIYNCgQUyYMIFPPvmE/fv38+tf/xovLy9ee+21Gz5nwoQJZGZm8sMPP+Du7s5zzz1HTk5O5QYuIiJVihImERFxesuXL8fX19f2eODAgURERBAeHs706dMxmUxERUWRmZnJ7373O/74xz/i4lJykEVaWhorV65ky5YtdOrUCYBZs2YRHR1dqe9FRESqFiVMIiLi9O677z5mzpxpe1yjRg2effZZunXrhslkspXHxMRw6dIlTpw4QYMGDUocIyUlBTc3Nzp27Ggri4qKwt/f3+7xi4hI1aWESUREnF6NGjVo1qyZo8MQEZFqSJM+iIhItRQdHc3GjRsxDMNWlpiYiJ+fH/Xr17+uflRUFEVFRWzfvt1Wlpqayvnz5ysjXBERqaKUMImISLX0zDPPcPz4caZMmcL+/ftZtmwZr776Ki+88MJ19y8BREZGMmDAAJ588kk2b97M9u3befzxx/H29nZA9CIiUlUoYRIRkWopLCyMFStWsGXLFtq2bctTTz3FpEmT+MMf/lDqc+bMmUNoaCi9evVi+PDhPPHEEwQHB1di1CIiUtWYjJ+PVRAREREREREb9TCJiIiIiIiUQgmTiIiIiIhIKZQwiYiIiIiIlEIJk4iIiIiISCmUMImIiIiIiJRCCZOIiIiIiEgplDCJiIiIiIiUQgmTiIiIiIhIKZQwiYiIiIiIlEIJk4iIiIiISCmUMImIiIiIiJTi/wPVtCac8ZrRGQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "loaded_results = run_ssl_pipeline_kfold_with_r2(\n",
        "    labeled_df=df,\n",
        "    load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_multi\",\n",
        "    fine_tune_epochs = 0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BvooSC9qXUq"
      },
      "source": [
        "#### SSL Multi Task + Finetuning == Horrible R2 <0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxejCoB7qUhb",
        "outputId": "72f42ab6-ea20-4520-fbeb-ec11b64d9f1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run directory: /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_multi\n",
            "Loaded pretrained backbone and projection head from saved run.\n",
            "\n",
            "--- K-Fold 1/5 ---\n",
            "Fold 1 Epoch 1 Loss: 0.9931\n",
            "Fold 1 Epoch 2 Loss: 0.9701\n",
            "Fold 1 Epoch 3 Loss: 1.0150\n",
            "Fold 1 Epoch 4 Loss: 1.0004\n",
            "Fold 1 Epoch 5 Loss: 0.9691\n",
            "Fold 1 Epoch 6 Loss: 1.0030\n",
            "Fold 1 Epoch 7 Loss: 0.9383\n",
            "Fold 1 Epoch 8 Loss: 0.9814\n",
            "Fold 1 Epoch 9 Loss: 0.9685\n"
          ]
        }
      ],
      "source": [
        "loaded_results = run_ssl_pipeline_kfold_with_r2(\n",
        "    labeled_df=df,\n",
        "    load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_multi\",\n",
        "    fine_tune_epochs = 25\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fbi_Dm9FefBP",
        "outputId": "99143b11-2eba-4a5e-fb35-c499df73f531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run directory: /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\n",
            "Loaded pretrained backbone and projection head from saved run.\n",
            "\n",
            "--- K-Fold 1/5 ---\n",
            "Fold 1 Epoch 1 Loss: 2.6601\n",
            "Fold 1 Epoch 2 Loss: 2.0024\n",
            "Fold 1 Epoch 3 Loss: 2.1261\n",
            "Fold 1 Epoch 4 Loss: 2.4039\n",
            "Fold 1 Epoch 5 Loss: 2.5450\n",
            "Fold 1 Epoch 6 Loss: 2.2874\n",
            "Fold 1 Epoch 7 Loss: 2.3637\n",
            "Fold 1 Epoch 8 Loss: 2.5455\n",
            "Fold 1 Epoch 9 Loss: 2.2838\n",
            "Fold 1 Epoch 10 Loss: 2.2622\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 11:43:13,097] A new study created in memory with name: no-name-f56c0198-0776-42bb-8955-b7883d72a4b6\n",
            "[I 2025-10-07 11:43:14,173] Trial 0 finished with value: 1.407820987701416 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.407820987701416.\n",
            "[I 2025-10-07 11:43:14,521] Trial 1 finished with value: 1.6261560440063476 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.407820987701416.\n",
            "[I 2025-10-07 11:43:14,911] Trial 2 finished with value: 1.3167827725410461 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.3167827725410461.\n",
            "[I 2025-10-07 11:43:15,522] Trial 3 finished with value: 1.3661500334739685 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.3167827725410461.\n",
            "[I 2025-10-07 11:43:16,816] Trial 4 finished with value: 1.2000540494918823 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 4 with value: 1.2000540494918823.\n",
            "[I 2025-10-07 11:43:18,407] Trial 5 finished with value: 1.404698669910431 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 4 with value: 1.2000540494918823.\n",
            "[I 2025-10-07 11:43:20,216] Trial 6 finished with value: 1.6500699043273925 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 4 with value: 1.2000540494918823.\n",
            "[I 2025-10-07 11:43:21,937] Trial 7 finished with value: 1.4523851871490479 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 4 with value: 1.2000540494918823.\n",
            "[I 2025-10-07 11:43:22,489] Trial 8 finished with value: 1.82504403591156 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 4 with value: 1.2000540494918823.\n",
            "[I 2025-10-07 11:43:23,731] Trial 9 finished with value: 1.302507185935974 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 4 with value: 1.2000540494918823.\n",
            "[I 2025-10-07 11:43:25,767] Trial 10 finished with value: 1.1951113581657409 and parameters: {'n_estimators': 289, 'max_depth': 7, 'learning_rate': 0.023969116039403743, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8200442512337782}. Best is trial 10 with value: 1.1951113581657409.\n",
            "[I 2025-10-07 11:43:27,816] Trial 11 finished with value: 1.136268675327301 and parameters: {'n_estimators': 278, 'max_depth': 7, 'learning_rate': 0.02610016466426293, 'subsample': 0.5324266600311697, 'colsample_bytree': 0.8218563683119117}. Best is trial 11 with value: 1.136268675327301.\n",
            "[I 2025-10-07 11:43:30,431] Trial 12 finished with value: 1.2001055121421813 and parameters: {'n_estimators': 299, 'max_depth': 8, 'learning_rate': 0.026438783878681333, 'subsample': 0.5168705837603152, 'colsample_bytree': 0.8635318007701933}. Best is trial 11 with value: 1.136268675327301.\n",
            "[I 2025-10-07 11:43:33,524] Trial 13 finished with value: 1.1917131304740907 and parameters: {'n_estimators': 291, 'max_depth': 7, 'learning_rate': 0.026763343082659687, 'subsample': 0.5021577654211222, 'colsample_bytree': 0.8372442252269388}. Best is trial 11 with value: 1.136268675327301.\n",
            "[I 2025-10-07 11:43:35,999] Trial 14 finished with value: 1.228536105155945 and parameters: {'n_estimators': 260, 'max_depth': 8, 'learning_rate': 0.037552712637241095, 'subsample': 0.6044320776354289, 'colsample_bytree': 0.9572053463393433}. Best is trial 11 with value: 1.136268675327301.\n",
            "[I 2025-10-07 11:43:37,146] Trial 15 finished with value: 1.3422045707702637 and parameters: {'n_estimators': 153, 'max_depth': 6, 'learning_rate': 0.018557958741270655, 'subsample': 0.5856019492388042, 'colsample_bytree': 0.8089603169963587}. Best is trial 11 with value: 1.136268675327301.\n",
            "[I 2025-10-07 11:43:42,138] Trial 16 finished with value: 1.209712851047516 and parameters: {'n_estimators': 273, 'max_depth': 8, 'learning_rate': 0.08719209000999652, 'subsample': 0.5597417999448122, 'colsample_bytree': 0.9219900508555974}. Best is trial 11 with value: 1.136268675327301.\n",
            "[I 2025-10-07 11:43:44,687] Trial 17 finished with value: 1.2004213452339172 and parameters: {'n_estimators': 226, 'max_depth': 6, 'learning_rate': 0.03812162884608817, 'subsample': 0.6703562504225158, 'colsample_bytree': 0.7581379014222452}. Best is trial 11 with value: 1.136268675327301.\n",
            "[I 2025-10-07 11:43:48,287] Trial 18 finished with value: 1.2379562973976135 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.04104975015072629, 'subsample': 0.6471208002214496, 'colsample_bytree': 0.910236126593346}. Best is trial 11 with value: 1.136268675327301.\n",
            "[I 2025-10-07 11:43:49,469] Trial 19 finished with value: 1.3172070980072021 and parameters: {'n_estimators': 181, 'max_depth': 9, 'learning_rate': 0.017768699649968665, 'subsample': 0.5054945946218856, 'colsample_bytree': 0.773295223440848}. Best is trial 11 with value: 1.136268675327301.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 1: {'n_estimators': 278, 'max_depth': 7, 'learning_rate': 0.02610016466426293, 'subsample': 0.5324266600311697, 'colsample_bytree': 0.8218563683119117}\n",
            "[Fold 1] RMSE: 1.9346, MAE: 1.4166, R: 0.5266\n",
            "\n",
            "--- K-Fold 2/5 ---\n",
            "Fold 2 Epoch 1 Loss: 1.8051\n",
            "Fold 2 Epoch 2 Loss: 1.9291\n",
            "Fold 2 Epoch 3 Loss: 1.7906\n",
            "Fold 2 Epoch 4 Loss: 1.4357\n",
            "Fold 2 Epoch 5 Loss: 1.5202\n",
            "Fold 2 Epoch 6 Loss: 1.5954\n",
            "Fold 2 Epoch 7 Loss: 1.6088\n",
            "Fold 2 Epoch 8 Loss: 1.7306\n",
            "Fold 2 Epoch 9 Loss: 1.6036\n",
            "Fold 2 Epoch 10 Loss: 1.6402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 11:44:19,306] A new study created in memory with name: no-name-0a9092a0-61a6-4e79-82ee-109aa2e35a80\n",
            "[I 2025-10-07 11:44:20,345] Trial 0 finished with value: 1.9406578063964843 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.9406578063964843.\n",
            "[I 2025-10-07 11:44:20,684] Trial 1 finished with value: 1.9136887073516846 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.9136887073516846.\n",
            "[I 2025-10-07 11:44:21,087] Trial 2 finished with value: 1.622477149963379 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.622477149963379.\n",
            "[I 2025-10-07 11:44:21,665] Trial 3 finished with value: 1.7455123662948608 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.622477149963379.\n",
            "[I 2025-10-07 11:44:22,682] Trial 4 finished with value: 1.7256396293640137 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.622477149963379.\n",
            "[I 2025-10-07 11:44:23,673] Trial 5 finished with value: 1.772912859916687 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.622477149963379.\n",
            "[I 2025-10-07 11:44:24,982] Trial 6 finished with value: 1.7061416745185851 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 1.622477149963379.\n",
            "[I 2025-10-07 11:44:27,147] Trial 7 finished with value: 1.7124322175979614 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 1.622477149963379.\n",
            "[I 2025-10-07 11:44:28,017] Trial 8 finished with value: 1.918528437614441 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 1.622477149963379.\n",
            "[I 2025-10-07 11:44:29,727] Trial 9 finished with value: 1.7499417781829834 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 1.622477149963379.\n",
            "[I 2025-10-07 11:44:30,829] Trial 10 finished with value: 1.6071676015853882 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 10 with value: 1.6071676015853882.\n",
            "[I 2025-10-07 11:44:32,557] Trial 11 finished with value: 1.719211792945862 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.28009047436880896, 'subsample': 0.5199086519616609, 'colsample_bytree': 0.8692027723625104}. Best is trial 10 with value: 1.6071676015853882.\n",
            "[I 2025-10-07 11:44:33,026] Trial 12 finished with value: 1.756683874130249 and parameters: {'n_estimators': 58, 'max_depth': 8, 'learning_rate': 0.14196217019146598, 'subsample': 0.5149340702175954, 'colsample_bytree': 0.8370887965872971}. Best is trial 10 with value: 1.6071676015853882.\n",
            "[I 2025-10-07 11:44:33,818] Trial 13 finished with value: 1.949703299999237 and parameters: {'n_estimators': 132, 'max_depth': 8, 'learning_rate': 0.29889801029971286, 'subsample': 0.6095659107531833, 'colsample_bytree': 0.7760131200377596}. Best is trial 10 with value: 1.6071676015853882.\n",
            "[I 2025-10-07 11:44:35,001] Trial 14 finished with value: 1.8615617275238037 and parameters: {'n_estimators': 119, 'max_depth': 8, 'learning_rate': 0.1125273525564678, 'subsample': 0.6100493659926688, 'colsample_bytree': 0.9818757669341631}. Best is trial 10 with value: 1.6071676015853882.\n",
            "[I 2025-10-07 11:44:36,047] Trial 15 finished with value: 1.7613388776779175 and parameters: {'n_estimators': 292, 'max_depth': 9, 'learning_rate': 0.17234746738517984, 'subsample': 0.5937663157166836, 'colsample_bytree': 0.5016444316932455}. Best is trial 10 with value: 1.6071676015853882.\n",
            "[I 2025-10-07 11:44:36,478] Trial 16 finished with value: 1.5665998935699463 and parameters: {'n_estimators': 51, 'max_depth': 6, 'learning_rate': 0.08749102207565247, 'subsample': 0.5638406774527164, 'colsample_bytree': 0.9219900508555974}. Best is trial 16 with value: 1.5665998935699463.\n",
            "[I 2025-10-07 11:44:37,787] Trial 17 finished with value: 1.72634596824646 and parameters: {'n_estimators': 165, 'max_depth': 6, 'learning_rate': 0.031405615672261515, 'subsample': 0.5494834187575843, 'colsample_bytree': 0.9069445451545826}. Best is trial 16 with value: 1.5665998935699463.\n",
            "[I 2025-10-07 11:44:39,655] Trial 18 finished with value: 1.584455943107605 and parameters: {'n_estimators': 172, 'max_depth': 7, 'learning_rate': 0.08335206810371563, 'subsample': 0.6660427411738311, 'colsample_bytree': 0.9261108860983648}. Best is trial 16 with value: 1.5665998935699463.\n",
            "[I 2025-10-07 11:44:42,641] Trial 19 finished with value: 1.6479164361953735 and parameters: {'n_estimators': 179, 'max_depth': 6, 'learning_rate': 0.09020014992536825, 'subsample': 0.6674711398764077, 'colsample_bytree': 0.929268849230458}. Best is trial 16 with value: 1.5665998935699463.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 2: {'n_estimators': 51, 'max_depth': 6, 'learning_rate': 0.08749102207565247, 'subsample': 0.5638406774527164, 'colsample_bytree': 0.9219900508555974}\n",
            "[Fold 2] RMSE: 2.4668, MAE: 2.1984, R: -0.4670\n",
            "\n",
            "--- K-Fold 3/5 ---\n",
            "Fold 3 Epoch 1 Loss: 1.5858\n",
            "Fold 3 Epoch 2 Loss: 1.4344\n",
            "Fold 3 Epoch 3 Loss: 1.7467\n",
            "Fold 3 Epoch 4 Loss: 1.6404\n",
            "Fold 3 Epoch 5 Loss: 1.4740\n",
            "Fold 3 Epoch 6 Loss: 1.6029\n",
            "Fold 3 Epoch 7 Loss: 1.7919\n",
            "Fold 3 Epoch 8 Loss: 1.6236\n",
            "Fold 3 Epoch 9 Loss: 1.6161\n",
            "Fold 3 Epoch 10 Loss: 1.5573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 11:45:11,679] A new study created in memory with name: no-name-f66c018e-7bfa-40a3-9438-099b6c28dfc1\n",
            "[I 2025-10-07 11:45:12,733] Trial 0 finished with value: 1.2837544202804565 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.2837544202804565.\n",
            "[I 2025-10-07 11:45:13,087] Trial 1 finished with value: 1.596200132369995 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.2837544202804565.\n",
            "[I 2025-10-07 11:45:13,488] Trial 2 finished with value: 1.3586804866790771 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 1.2837544202804565.\n",
            "[I 2025-10-07 11:45:14,098] Trial 3 finished with value: 1.3810317754745483 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 1.2837544202804565.\n",
            "[I 2025-10-07 11:45:15,133] Trial 4 finished with value: 1.4044832348823548 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 0 with value: 1.2837544202804565.\n",
            "[I 2025-10-07 11:45:16,094] Trial 5 finished with value: 1.3150917530059814 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 0 with value: 1.2837544202804565.\n",
            "[I 2025-10-07 11:45:17,497] Trial 6 finished with value: 1.5812568426132203 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 0 with value: 1.2837544202804565.\n",
            "[I 2025-10-07 11:45:19,267] Trial 7 finished with value: 1.4488105416297912 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 0 with value: 1.2837544202804565.\n",
            "[I 2025-10-07 11:45:19,826] Trial 8 finished with value: 1.941755223274231 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 0 with value: 1.2837544202804565.\n",
            "[I 2025-10-07 11:45:21,195] Trial 9 finished with value: 1.2791009068489074 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 9 with value: 1.2791009068489074.\n",
            "[I 2025-10-07 11:45:24,638] Trial 10 finished with value: 1.467051351070404 and parameters: {'n_estimators': 291, 'max_depth': 7, 'learning_rate': 0.03001215871999436, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8259332753890892}. Best is trial 9 with value: 1.2791009068489074.\n",
            "[I 2025-10-07 11:45:25,592] Trial 11 finished with value: 1.2482789516448975 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.11285320502212905, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.5037892523442965}. Best is trial 11 with value: 1.2482789516448975.\n",
            "[I 2025-10-07 11:45:26,612] Trial 12 finished with value: 1.1483211517333984 and parameters: {'n_estimators': 144, 'max_depth': 8, 'learning_rate': 0.09958344834282504, 'subsample': 0.8676850494543448, 'colsample_bytree': 0.5083162241661173}. Best is trial 12 with value: 1.1483211517333984.\n",
            "[I 2025-10-07 11:45:27,105] Trial 13 finished with value: 1.4045170426368714 and parameters: {'n_estimators': 154, 'max_depth': 8, 'learning_rate': 0.2959041775234907, 'subsample': 0.8903098153397658, 'colsample_bytree': 0.5032838113344728}. Best is trial 12 with value: 1.1483211517333984.\n",
            "[I 2025-10-07 11:45:28,059] Trial 14 finished with value: 1.3162203550338745 and parameters: {'n_estimators': 128, 'max_depth': 9, 'learning_rate': 0.10133215384123687, 'subsample': 0.8961689436930799, 'colsample_bytree': 0.5136572344958813}. Best is trial 12 with value: 1.1483211517333984.\n",
            "[I 2025-10-07 11:45:29,399] Trial 15 finished with value: 1.2869763016700744 and parameters: {'n_estimators': 172, 'max_depth': 8, 'learning_rate': 0.09894162917596783, 'subsample': 0.9198438358121845, 'colsample_bytree': 0.6543556252119602}. Best is trial 12 with value: 1.1483211517333984.\n",
            "[I 2025-10-07 11:45:30,620] Trial 16 finished with value: 1.4896354913711547 and parameters: {'n_estimators': 123, 'max_depth': 6, 'learning_rate': 0.035245980217394675, 'subsample': 0.997430985975306, 'colsample_bytree': 0.8441147559750997}. Best is trial 12 with value: 1.1483211517333984.\n",
            "[I 2025-10-07 11:45:32,110] Trial 17 finished with value: 1.2064007222652435 and parameters: {'n_estimators': 170, 'max_depth': 9, 'learning_rate': 0.15209710869157952, 'subsample': 0.8608596682963604, 'colsample_bytree': 0.9980084946806067}. Best is trial 12 with value: 1.1483211517333984.\n",
            "[I 2025-10-07 11:45:33,208] Trial 18 finished with value: 1.653246021270752 and parameters: {'n_estimators': 184, 'max_depth': 8, 'learning_rate': 0.26637689336012876, 'subsample': 0.6471208002214496, 'colsample_bytree': 0.9476321564223729}. Best is trial 12 with value: 1.1483211517333984.\n",
            "[I 2025-10-07 11:45:34,736] Trial 19 finished with value: 1.2044023871421814 and parameters: {'n_estimators': 116, 'max_depth': 7, 'learning_rate': 0.07855640435412374, 'subsample': 0.8312585492417999, 'colsample_bytree': 0.910183802382454}. Best is trial 12 with value: 1.1483211517333984.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 3: {'n_estimators': 144, 'max_depth': 8, 'learning_rate': 0.09958344834282504, 'subsample': 0.8676850494543448, 'colsample_bytree': 0.5083162241661173}\n",
            "[Fold 3] RMSE: 0.4279, MAE: 0.3052, R: 0.9740\n",
            "\n",
            "--- K-Fold 4/5 ---\n",
            "Fold 4 Epoch 1 Loss: 1.4433\n",
            "Fold 4 Epoch 2 Loss: 1.3271\n",
            "Fold 4 Epoch 3 Loss: 1.4881\n",
            "Fold 4 Epoch 4 Loss: 1.6251\n",
            "Fold 4 Epoch 5 Loss: 1.6870\n",
            "Fold 4 Epoch 6 Loss: 1.4385\n",
            "Fold 4 Epoch 7 Loss: 1.4586\n",
            "Fold 4 Epoch 8 Loss: 1.6718\n",
            "Fold 4 Epoch 9 Loss: 1.5940\n",
            "Fold 4 Epoch 10 Loss: 1.3917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 11:46:07,199] A new study created in memory with name: no-name-bdf4e1a2-bb17-41a6-bb43-fde1a9da1b60\n",
            "[I 2025-10-07 11:46:08,217] Trial 0 finished with value: 1.6683461666107178 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.6683461666107178.\n",
            "[I 2025-10-07 11:46:08,563] Trial 1 finished with value: 1.574157476425171 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.574157476425171.\n",
            "[I 2025-10-07 11:46:08,939] Trial 2 finished with value: 1.4564182877540588 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:09,515] Trial 3 finished with value: 1.65777747631073 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:10,517] Trial 4 finished with value: 1.5066529750823974 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:11,429] Trial 5 finished with value: 1.6967060565948486 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:12,832] Trial 6 finished with value: 1.6365700006484984 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:14,511] Trial 7 finished with value: 1.608537530899048 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:15,083] Trial 8 finished with value: 1.9185880899429322 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:16,279] Trial 9 finished with value: 1.6780598878860473 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:17,903] Trial 10 finished with value: 1.712538766860962 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:20,538] Trial 11 finished with value: 1.5820990085601807 and parameters: {'n_estimators': 158, 'max_depth': 6, 'learning_rate': 0.024781852487147565, 'subsample': 0.5996007697216827, 'colsample_bytree': 0.739627013585424}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:23,125] Trial 12 finished with value: 1.5984121680259704 and parameters: {'n_estimators': 295, 'max_depth': 8, 'learning_rate': 0.029882896955595446, 'subsample': 0.653615691352136, 'colsample_bytree': 0.8221626573661827}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:24,490] Trial 13 finished with value: 1.5646091938018798 and parameters: {'n_estimators': 185, 'max_depth': 8, 'learning_rate': 0.10328483221978228, 'subsample': 0.578578733418571, 'colsample_bytree': 0.6937971020319804}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:25,192] Trial 14 finished with value: 1.5576644778251647 and parameters: {'n_estimators': 119, 'max_depth': 8, 'learning_rate': 0.026286438903149127, 'subsample': 0.6870374505837875, 'colsample_bytree': 0.5360030250325667}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:25,698] Trial 15 finished with value: 1.623344361782074 and parameters: {'n_estimators': 58, 'max_depth': 5, 'learning_rate': 0.11263608439461861, 'subsample': 0.6219374804317841, 'colsample_bytree': 0.9031785893066717}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:27,112] Trial 16 finished with value: 1.507414847612381 and parameters: {'n_estimators': 233, 'max_depth': 9, 'learning_rate': 0.0324525654578974, 'subsample': 0.5202925041110055, 'colsample_bytree': 0.6561174679610582}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:29,893] Trial 17 finished with value: 1.7197618246078492 and parameters: {'n_estimators': 293, 'max_depth': 6, 'learning_rate': 0.04218477416131455, 'subsample': 0.8740176822486824, 'colsample_bytree': 0.7720029351996511}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:30,702] Trial 18 finished with value: 1.6959773063659669 and parameters: {'n_estimators': 178, 'max_depth': 2, 'learning_rate': 0.18188215906458088, 'subsample': 0.72029200207761, 'colsample_bytree': 0.6862394600353388}. Best is trial 2 with value: 1.4564182877540588.\n",
            "[I 2025-10-07 11:46:32,000] Trial 19 finished with value: 1.6237003803253174 and parameters: {'n_estimators': 119, 'max_depth': 7, 'learning_rate': 0.017858642546436633, 'subsample': 0.56391216214132, 'colsample_bytree': 0.7649303987051717}. Best is trial 2 with value: 1.4564182877540588.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 4: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}\n",
            "[Fold 4] RMSE: 1.8214, MAE: 1.6051, R: 0.5802\n",
            "\n",
            "--- K-Fold 5/5 ---\n",
            "Fold 5 Epoch 1 Loss: 1.6262\n",
            "Fold 5 Epoch 2 Loss: 1.5129\n",
            "Fold 5 Epoch 3 Loss: 1.4395\n",
            "Fold 5 Epoch 4 Loss: 1.5151\n",
            "Fold 5 Epoch 5 Loss: 1.5274\n",
            "Fold 5 Epoch 6 Loss: 1.4094\n",
            "Fold 5 Epoch 7 Loss: 1.3731\n",
            "Fold 5 Epoch 8 Loss: 1.3843\n",
            "Fold 5 Epoch 9 Loss: 1.4026\n",
            "Fold 5 Epoch 10 Loss: 1.4330\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 11:46:58,445] A new study created in memory with name: no-name-f70c983a-67e9-4a32-8d86-9cc1c75a0f47\n",
            "[I 2025-10-07 11:47:00,047] Trial 0 finished with value: 1.2215837955474853 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.2215837955474853.\n",
            "[I 2025-10-07 11:47:00,642] Trial 1 finished with value: 1.186989462375641 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.186989462375641.\n",
            "[I 2025-10-07 11:47:01,281] Trial 2 finished with value: 1.0646203756332397 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:01,855] Trial 3 finished with value: 1.1807119488716125 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:02,878] Trial 4 finished with value: 1.1963884949684143 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:03,896] Trial 5 finished with value: 1.1393469750881196 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:05,273] Trial 6 finished with value: 1.550169003009796 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:06,955] Trial 7 finished with value: 1.3141855955123902 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:07,581] Trial 8 finished with value: 1.6255678534507751 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:08,884] Trial 9 finished with value: 1.180881094932556 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:09,892] Trial 10 finished with value: 1.5595468282699585 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:11,098] Trial 11 finished with value: 1.2719950914382934 and parameters: {'n_estimators': 277, 'max_depth': 6, 'learning_rate': 0.1234269340201201, 'subsample': 0.5971603596691708, 'colsample_bytree': 0.5020730763700768}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:13,757] Trial 12 finished with value: 1.1018373727798463 and parameters: {'n_estimators': 298, 'max_depth': 8, 'learning_rate': 0.032355470034334255, 'subsample': 0.6290514757037396, 'colsample_bytree': 0.5094194352393875}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:14,892] Trial 13 finished with value: 1.15075626373291 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.03137733444754062, 'subsample': 0.6173223831273252, 'colsample_bytree': 0.5018302274954881}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:16,596] Trial 14 finished with value: 1.2559555351734162 and parameters: {'n_estimators': 281, 'max_depth': 8, 'learning_rate': 0.029992078740169497, 'subsample': 0.5253071179945697, 'colsample_bytree': 0.6738167152482913}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:16,991] Trial 15 finished with value: 1.2649360060691834 and parameters: {'n_estimators': 58, 'max_depth': 8, 'learning_rate': 0.09894162917596783, 'subsample': 0.6274745095999714, 'colsample_bytree': 0.5778918965965928}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:19,420] Trial 16 finished with value: 1.1639909029006958 and parameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.020471587788894952, 'subsample': 0.6695070722352982, 'colsample_bytree': 0.8388676782609714}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:20,454] Trial 17 finished with value: 1.2812476813793183 and parameters: {'n_estimators': 168, 'max_depth': 7, 'learning_rate': 0.08440320783191002, 'subsample': 0.5694064187762937, 'colsample_bytree': 0.5669992073003015}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:21,276] Trial 18 finished with value: 1.2916153490543365 and parameters: {'n_estimators': 117, 'max_depth': 9, 'learning_rate': 0.04104975015072629, 'subsample': 0.5617390605696774, 'colsample_bytree': 0.7895429472174105}. Best is trial 2 with value: 1.0646203756332397.\n",
            "[I 2025-10-07 11:47:22,098] Trial 19 finished with value: 1.283224993944168 and parameters: {'n_estimators': 179, 'max_depth': 9, 'learning_rate': 0.2702272241241035, 'subsample': 0.6652230509575321, 'colsample_bytree': 0.675760829066606}. Best is trial 2 with value: 1.0646203756332397.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 5: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}\n",
            "[Fold 5] RMSE: 2.8136, MAE: 1.8847, R: 0.4903\n",
            "\n",
            "=== Fold Metrics Summary ===\n",
            "      fold      RMSE       MAE        R2\n",
            "0      1.0  1.934621  1.416637  0.526576\n",
            "1      2.0  2.466783  2.198400 -0.466957\n",
            "2      3.0  0.427890  0.305167  0.973965\n",
            "3      4.0  1.821374  1.605066  0.580194\n",
            "4      5.0  2.813626  1.884661  0.490327\n",
            "Mean   3.0  1.892859  1.481986  0.420821\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1kAAAIjCAYAAADxz9EgAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAA+2ZJREFUeJzs3Xd4U9UbwPFvku5NaUsLlAItq+wtG2SVvaeoiFsQ3CIqWxFRGYrgT0Fc7CnIXrL3LHsUKNDSMjroTJP7++PaQumgI2063s/z9KG5eXPum9Pkkjf33HM0iqIoCCGEEEIIIYQwCa25ExBCCCGEEEKIokSKLCGEEEIIIYQwISmyhBBCCCGEEMKEpMgSQgghhBBCCBOSIksIIYQQQgghTEiKLCGEEEIIIYQwISmyhBBCCCGEEMKEpMgSQgghhBBCCBOSIksIIYQQQgghTEiKLCFEsbRgwQI0Gg3Xrl17amz58uUZOnRonudUFFy7dg2NRsOCBQvMnUqBMXToUMqXL5+v+3z48CEeHh789ddf+bpfYX56vR5vb29+/PFHc6ciRLEmRZYQIl8lFzdHjhxJtT0yMpJGjRphY2PDxo0bM31sej+jR4/Oj/Qz9WROTk5OtGrVin/++cfcqRULQ4cOTen3uLi4NPdfunQp5W/zzTffZLv92NhYxo8fz86dO02Qbd6aOXMmjo6ODBw4MGXb+PHjU70+LS0tKV++PCNHjiQiIiJNG+XLl0ej0dCuXbt09/Hzzz+ntPXk+3nPnj106tSJMmXKYGNjQ7ly5ejWrRsLFy5MFZfR+1mj0fDGG2/k6LknP/6VV15J9/5PP/00Jebu3bvpxvTv3x+NRsPHH3+c7v07d+7MNPfFixdnO+8n29TpdHh4eNC3b1/OnTuXKnbHjh00a9aMVq1a4e/vz+TJk1Pus7S05L333uOLL74gPj4+23kIIUzDwtwJCCFEVFQUHTp04NSpU6xatYqAgIBM4ydOnEiFChVSbatRo0Zepphl7du354UXXkBRFK5fv86cOXPo1q0bGzZsoGPHjuZOL8/5+PgQFxeHpaWlWfZvYWFBbGwsa9eupX///qnu++uvv7CxscnxB8/Y2FgmTJgAQOvWrbP8uJ9//hmj0ZijfeaEXq9n5syZvPvuu+h0ujT3z5kzBwcHB2JiYti2bRvff/89x44dY8+ePWlibWxs2LFjB6GhoXh6eqa6L6P+XLZsGQMGDKBOnTqMGjWKEiVKEBQUxK5du/j5558ZPHhwqvjk98yTKleunJOnn5L3ihUr+PHHH7Gyskp136JFizJ9HURFRbF27VrKly/PokWL+Oqrr9BoNOnGjhw5koYNG6bZ3qRJkxznntymXq/n1KlTzJ07l507dxIYGJjyN6hcuTKbN2/G3t6ekJAQKlasSIsWLWjVqhUAL730EqNHj2bhwoUMGzYsx7kIIXJOiiwhhFlFR0fTsWNHTpw4wcqVK+nUqdNTH9OpUycaNGiQD9llX+XKlRkyZEjK7T59+uDv78/MmTPzvciKiYnB3t4+X/ep0WiwsbHJ130+ztrammbNmrFo0aI0RdbChQvp0qULK1asyJdckvs/vwvOdevWER4enub5J+vbty9ubm4AvP766wwcOJAlS5Zw6NAhGjVqlCq2WbNmHD58mCVLljBq1KiU7Tdv3mT37t306tUrTX+OHz8ef39/Dhw4kKbACQsLS5PPk+8ZUwgICODvv/9mw4YN9OjRI2X7vn37CAoKok+fPhm+DlasWIHBYGD+/Pk8++yz7Nq1K6V4eVKLFi3o27evSXN/ss0qVarw5ptv8vvvv/PRRx8BUKZMmZT7NRoNRqMRrfbR4CQXFxc6dOjAggULpMgSwkxkuKAQwmwePnxIQEAAx44dY8WKFXTp0sUk7W7fvp0WLVpgb2+Pi4sLPXr0SDPcJj2KojB58mTKli2LnZ0dbdq04cyZM7nKpVq1ari5uXHlypVU2xMSEhg3bhx+fn5YW1vj7e3NRx99REJCQqq4uLg4Ro4ciZubG46OjnTv3p1bt26h0WgYP358SlzyULCzZ88yePBgSpQoQfPmzVPu//PPP6lfvz62tra4uroycOBAgoODU+3r0qVL9OnTB09PT2xsbChbtiwDBw4kMjIyJWbLli00b94cFxcXHBwcqFKlCmPGjEm5P6NrsrLyN0l+DpcvX2bo0KG4uLjg7OzMSy+9RGxsbJb7fPDgwWzYsCHVELjDhw9z6dKlNGdRkkVERPDOO+/g7e2NtbU1fn5+TJ06NeUM1LVr13B3dwdgwoQJKUO6kv8GQ4cOxcHBgStXrtC5c2ccHR157rnnUu578poso9HIzJkzqVmzJjY2Nri7uxMQEJBq2N3T+jojq1evpnz58vj6+mapv1q0aAGQ5jUK6hmh3r17pxnmt2jRIkqUKJHuFwdXrlyhYcOGaQosAA8PjyzllFtlypShZcuWafL+66+/qFmzZqZnvv/66y/at29PmzZtqFatmtmva8vs75OUlMQLL7xAly5dUuKStW/fnj179nD//v18yVMIkZqcyRJCmEVMTAydOnXi8OHDLF++nK5du2b5sZGRkWmupUj+Zn7r1q106tSJihUrMn78eOLi4vj+++9p1qwZx44dy3QCgrFjxzJ58mQ6d+5M586dOXbsGB06dCAxMTFHzzE51wcPHqT6wGs0GunevTt79uzhtddeo1q1apw+fZrp06dz8eJFVq9enRI7dOhQli5dyvPPP88zzzzDv//+m2kx2q9fPypVqsSXX36JoigAfPHFF3z++ef079+fV155hfDwcL7//ntatmzJ8ePHcXFxITExkY4dO5KQkMDbb7+Np6cnt27dYt26dURERODs7MyZM2fo2rUrtWrVYuLEiVhbW3P58mX27t2baR9k92/Sv39/KlSowJQpUzh27Bi//PILHh4eTJ06NUt93rt3b9544w1WrlyZ8i3+woULqVq1KvXq1UsTHxsbS6tWrbh16xavv/465cqVY9++fXzyySeEhIQwY8YM3N3dmTNnDm+++Sa9evWid+/eANSqVSulnaSkJDp27Ejz5s355ptvsLOzyzDHl19+mQULFtCpUydeeeUVkpKS2L17NwcOHKBBgwY57mtQz9ak9zwzkjz5S4kSJdK9f/DgwXTo0IErV66kvI4XLlxI37590z1L5+Pjw7Zt27h58yZly5Z96v7j4+PTvTbKyckp3UItqwYPHsyoUaN4+PAhDg4OJCUlsWzZMt57770Mhwrevn2bHTt28NtvvwEwaNAgpk+fzg8//JBuLtHR0enmXrJkyQyHGGZXRn8fo9HISy+9xMOHD1m1alWax9WvXx9FUdi3b1+2jq9CCBNRhBAiH/36668KoPj4+CiWlpbK6tWrs/3Y9H6S1alTR/Hw8FDu3buXsu3kyZOKVqtVXnjhhTRtBQUFKYqiKGFhYYqVlZXSpUsXxWg0psSNGTNGAZQXX3zxqfkByssvv6yEh4crYWFhypEjR5SAgAAFUKZNm5YS98cffyharVbZvXt3qsfPnTtXAZS9e/cqiqIoR48eVQDlnXfeSRU3dOhQBVDGjRuXsm3cuHEKoAwaNChV7LVr1xSdTqd88cUXqbafPn1asbCwSNl+/PhxBVCWLVuW4fObPn26Aijh4eEZxgQFBSmA8uuvv6Zsy+rfJPk5DBs2LFWbvXr1UkqWLJnhPpO9+OKLir29vaIoitK3b1+lbdu2iqIoisFgUDw9PZUJEyak5Pf432PSpEmKvb29cvHixVTtjR49WtHpdMqNGzcURVGU8PDwNP3++L4BZfTo0ene5+Pjk3J7+/btCqCMHDkyTWzyay8rfZ0evV6vaDQa5f33309zX3L/XrhwQQkPD1euXbumzJ8/X7G1tVXc3d2VmJiYVPE+Pj5Kly5dlKSkJMXT01OZNGmSoiiKcvbsWQVQ/v3335T30eHDh1MeN2/ePAVQrKyslDZt2iiff/65snv3bsVgMKTJKaP3M6AsWrQoW8/98TaHDx+u3L9/X7GyslL++OMPRVEU5Z9//lE0Go1y7dq1lL54sn+/+eYbxdbWVomKilIURVEuXryoAMqqVatSxe3YsSPT3ENCQrKdd3Kb8+fPV8LDw5Xbt28rGzduVPz8/BSNRqMcOnQoJdZgMCjPP/+80rZtWyU6Ojrd9m7fvq0AytSpU7OdixAi92S4oBDCLO7cuYONjQ3e3t7Zfuzs2bPZsmVLqh+AkJAQTpw4wdChQ3F1dU2Jr1WrFu3bt2f9+vUZtrl161YSExN5++23U30D/c4772Qrt3nz5uHu7o6HhwcNGjRg27ZtfPTRR7z33nspMcuWLaNatWpUrVqVu3fvpvw8++yzgDpzGJAyy+Jbb72Vah9vv/12hvt/cka2lStXYjQa6d+/f6p9eXp6UqlSpZR9OTs7A7Bp06YMh+a5uLgAsGbNmixP5JCTv8mTz6FFixbcu3ePqKioLO0T1LMYO3fuJDQ0lO3btxMaGprhUMFly5bRokULSpQokaqP2rVrh8FgYNeuXVne75tvvvnUmBUrVqDRaBg3blya+5Jfeznpa4D79++jKEqGZ6VAvcbH3d2d8uXLM2zYMPz8/NiwYUOGZ950Oh39+/dn0aJFgDqcztvbO83wtGTDhg1j48aNtG7dmj179jBp0iRatGhBpUqV2LdvX5r4Hj16pHk/b9myhTZt2mT5eaenRIkSBAQEpOS9cOFCmjZtio+PT4aP+euvv+jSpQuOjo4AVKpUifr162c4ZHDs2LHp5v74az27hg0bhru7O6VLlyYgIIDIyEj++OOPVBNszJs3jz/++IPY2Fi6du1K69at05zNSn4NZDSDohAib8lwQSGEWfz000+89957BAQEsHv3bqpUqQKAwWAgPDw8Vayrq2uqoTqNGjVKd+KL69evA6S09bhq1aqxadOmDCeDSH5spUqVUm13d3fP9APrk3r06MGIESNITEzk8OHDfPnll8TGxqa6KP3SpUucO3cu5RqfJyVPDnD9+nW0Wm2amRT9/Pwy3P+TsZcuXUJRlDTPK1nycK8KFSrw3nvv8d133/HXX3/RokULunfvzpAhQ1IKsAEDBvDLL7/wyiuvMHr0aNq2bUvv3r3p27dvquf3uJz8TcqVK5cqLrn/Hzx4gJOTU4bP/XHJ10UtWbKEEydO0LBhQ/z8/NJdF+3SpUucOnXqqX+Pp7GwsMjS8LgrV65QunTpTD+I56SvH6f8N1Q0PStWrMDJyYnw8HBmzZpFUFAQtra2mbY3ePBgZs2axcmTJ1m4cCEDBw7MdDhcx44d6dixI7GxsRw9epQlS5Ywd+5cunbtyvnz51Ndm1W2bNkMp4nPrcGDB/P8889z48YNVq9ezddff51h7Llz5zh+/DgvvPACly9fTtneunVrZs+eTVRUVJrXX82aNU2e+9ixY2nRokXKMMDFixen+Zu/+uqrvPrqq5m2k/waMNWwRSFE9kiRJYQwC39/f9avX0/btm1p3749e/fuxdvbm+Dg4DSFwo4dO7I1ZbY5Pf6BsXPnzri5uTFixAjatGmTch2P0WikZs2afPfdd+m2kZOze8me/LBsNBrRaDRs2LAh3em8HRwcUn7/9ttvGTp0KGvWrGHz5s2MHDmSKVOmcODAAcqWLYutrS27du1ix44d/PPPP2zcuJElS5bw7LPPsnnz5nTbz4mM2smscHiStbU1vXv35rfffuPq1aupJgl5ktFopH379ikztz0pq1OJW1tbZ6kAyoqc9rWrqysajYYHDx5k2HbLli1TrmHs1q0bNWvW5LnnnuPo0aMZ5t+4cWN8fX155513CAoKyvCs4JPs7Oxo0aIFLVq0wM3NjQkTJrBhwwZefPHFLD0+t7p37461tTUvvvgiCQkJGc64COrkMADvvvsu7777bpr7V6xYwUsvvZRnuSZ7vHDr2bMnsbGxvPrqqzRv3jxbx4bk10Dy31oIkb+kyBJCmE2jRo1YvXo1Xbp0oX379uzevRtPT8+U4X/JateunaX2kocBXbhwIc1958+fx83NLcMpzZMfe+nSJSpWrJiyPTw8PNMPrE/z+uuvM336dD777DN69eqFRqPB19eXkydP0rZt20y/Zfbx8cFoNBIUFJTqTNTj37I/ja+vL4qiUKFChSwVCzVr1qRmzZp89tln7Nu3j2bNmjF37tyUxU61Wi1t27albdu2fPfdd3z55Zd8+umn7NixI91v9HPzN8mtwYMHM3/+fLRabapFeZ/k6+vLw4cPn3pGwlRnBHx9fdm0aRP379/P9GxWdvsa1LNpvr6+BAUFZSkXBwcHxo0bx0svvcTSpUsz7adBgwYxefJkqlWrRp06dbLU/uOSzz6HhIRk+7E5ZWtrS8+ePfnzzz/p1KlThgWHoigsXLiQNm3apBmeCzBp0iT++uuvfCmynvTVV1+xatUqvvjiC+bOnZvlxyW/BqpVq5ZXqQkhMiHXZAkhzKpt27YsWrSIy5cvExAQQGJiIu3atUv1k9Xhel5eXtSpU4fffvst1fTdgYGBbN68mc6dO2f42Hbt2mFpacn333+f6ozJjBkzcvrUAPVD7/vvv8+5c+dYs2YNoM6ed+vWLX7++ec08XFxccTExACkTI/9448/por5/vvvs7z/3r17o9PpmDBhQpozQYqicO/ePUBdgDUpKSnV/TVr1kSr1aZMK5/eVNDJH7afnHo+WW7+JrnVpk0bJk2axA8//JBmId3H9e/fn/3797Np06Y090VERKT0S/I1S48/j5zo06cPiqKkLGz8uOS/UU76OlmTJk1STQX/NM899xxly5Z96uyNr7zyCuPGjePbb7/NNG7btm3pbk++/i69oaN56YMPPmDcuHF8/vnnGcbs3buXa9eu8dJLL9G3b980PwMGDGDHjh3cvn07HzNX+fr60qdPHxYsWEBoaGiWH3f06FE0Gk2uFkYWQuScnMkSQphdr169+Pnnnxk2bBjdu3dn48aNOV7Qdtq0aXTq1IkmTZrw8ssvp0wX7uzsnOmQMXd3dz744AOmTJlC165d6dy5M8ePH2fDhg25Hm4zdOhQxo4dy9SpU+nZsyfPP/88S5cu5Y033mDHjh00a9YMg8HA+fPnWbp0KZs2baJBgwbUr1+fPn36MGPGDO7du5cyhfvFixeBrJ1Z8fX1ZfLkyXzyySdcu3aNnj174ujoSFBQEKtWreK1117jgw8+YPv27YwYMYJ+/fpRuXJlkpKS+OOPP9DpdPTp0weAiRMnsmvXLrp06YKPjw9hYWH8+OOPlC1bNtWaXE/K6d8kt7RaLZ999tlT4z788EP+/vtvunbtytChQ6lfvz4xMTGcPn2a5cuXc+3aNdzc3LC1tcXf358lS5ZQuXJlXF1dqVGjRqZrLqWnTZs2PP/888yaNYtLly4REBCA0Whk9+7dtGnThhEjRuS4r0G9LvCPP/7g4sWLWTp7aWlpyahRo/jwww/ZuHEjAQEB6cb5+Phk6e/Vo0cPKlSoQLdu3fD19SUmJoatW7eydu1aGjZsSLdu3VLFX7x4MWWo3uNKlSpF+/btAdi5cydt2rRh3Lhx2X7N1K5d+6lnw//66y90Ol2GyyN0796dTz/9lMWLF6eaxGb37t3pTgdfq1atlOn9x48fz4QJE3I17PnDDz9k6dKlzJgxg6+++ipLj9myZQvNmjWjZMmSOdqnECKXzDOpoRCiuEpvyudk33zzjQIoXbt2VfR6fbYe+7itW7cqzZo1U2xtbRUnJyelW7duytmzZ9NtK3kKd0VRp0WeMGGC4uXlpdja2iqtW7dWAgMDFR8fnyxP4T58+PB07xs/frwCKDt27FAURVESExOVqVOnKtWrV1esra2VEiVKKPXr11cmTJigREZGpjwuJiZGGT58uOLq6qo4ODgoPXv2VC5cuKAAyldffZUSl9GU1MlWrFihNG/eXLG3t1fs7e2VqlWrKsOHD1cuXLigKIqiXL16VRk2bJji6+ur2NjYKK6urkqbNm2UrVu3prSxbds2pUePHkrp0qUVKysrpXTp0sqgQYNSTX2e3hTuipK1v0lGzyG9v1V6Hp/CPSPpTeGuKIoSHR2tfPLJJ4qfn59iZWWluLm5KU2bNlW++eYbJTExMSVu3759Sv369RUrK6tU07lntu8np3BXFEVJSkpSpk2bplStWlWxsrJS3N3dlU6dOilHjx5VFCVrfZ2RhIQExc3NLWXK9WSZvUYiIyMVZ2dnpVWrVinbkqdwz0x678lFixYpAwcOVHx9fRVbW1vFxsZG8ff3Vz799NOUqdGTkck06I/nsnbtWgVQ5s6d+9Tnn9n7MNnjfZGYmKiULFlSadGiRaaPqVChglK3bl1FUZ4+hfvj0/y///77ikajUc6dO5dp+8ltZrSMQuvWrRUnJyclIiIi03YURVEiIiIUKysr5ZdffnlqrBAib2gUJRtXEgshhDC7EydOULduXf7880+ee+45c6cjCqBJkybx66+/cunSJZNNSGJOH330UcqwYmtra3Onky2NGjXCx8eHZcuW5ds+Z8yYwddff82VK1eeOnOkECJvyDVZQghRgMXFxaXZNmPGDLRaLS1btjRDRqIwePfdd3n48CGLFy82dyomsWPHDj7//PNCV2BFRUVx8uRJJk6cmG/71Ov1fPfdd3z22WdSYAlhRnImSwghCrAJEyZw9OhR2rRpg4WFBRs2bGDDhg289tpr/PTTT+ZOTwghhBDpkCJLCCEKsC1btjBhwgTOnj3Lw4cPKVeuHM8//zyffvopFhYyd5EQQghREEmRJYQQQgghhBAmJNdkCSGEEEIIIYQJSZElhBBCCCGEECYkA/qfwmg0cvv2bRwdHbO08KcQQgghhBCiaFIUhejoaEqXLo1Wm/H5KimynuL27dt4e3ubOw0hhBBCCCFEAREcHEzZsmUzvF+KrKdwdHQE1I50cnIyay56vZ7NmzfToUMHLC0tzZpLUST9m7ekf/OW9G/ekv7NW9K/eUv6N29J/+a9gtTHUVFReHt7p9QIGZEi6ymShwg6OTkViCLLzs4OJycns7/AiiLp37wl/Zu3pH/zlvRv3pL+zVvSv3lL+jfvFcQ+ftplRDLxhRBCCCGEEEKYkBRZQgghhBBCCGFCUmQJIYQQQgghhAnJNVkmoCgKSUlJGAyGPN2PXq/HwsKC+Pj4PN9XQaDT6bCwsJCp84UQQgghRKEiRVYuJSYmEhISQmxsbJ7vS1EUPD09CQ4OLjaFh52dHV5eXlhZWZk7FSGEEEIIIbJEiqxcMBqNBAUFodPpKF26NFZWVnla/BiNRh4+fIiDg0Omi58VBYqikJiYSHh4OEFBQVSqVKnIP2chhBBCCFE0SJGVC4mJiRiNRry9vbGzs8vz/RmNRhITE7GxsSkWBYetrS2WlpZcv3495XkLIYQQQghR0BX9T+r5oDgUPOYifSuEEEIIIQob+QQrhBBCCCGEECYkRZYQQgghhBBCmJBck1UAGIwKh4LuExYdj4ejDY0quKLTFo/ZA4UQQgghhChq5EyWmW0MDKH51O0M+vkAoxafYNDPB2g+dTsbA0PydL9Dhw5Fo9Gg0WiwtLSkQoUKfPTRR8THx6fEJN9/4MCBVI9NSEigZMmSaDQadu7cmbL933//5dlnn8XV1RU7OzsqVarEiy++SGJiIgA7d+5MafPJn9DQ0Dx9vkIIIYQQQuQXKbLMaGNgCG/+eYyQyPhU20Mj43nzz2N5XmgFBAQQEhLC1atXmT59Oj/99BPjxo1LFePt7c2vv/6aatuqVatwcHBIte3s2bMEBATQoEEDdu3axenTp/n++++xsrJKs3DyhQsXCAkJSfXj4eGRN09SCCGEEEKIfCZFlgkpikJsYlKWfqLj9Yz7+wxKeu389+/4v88SHa9P9bi4REO67SlKei1lztraGk9PT7y9venZsyft2rVjy5YtqWJefPFFFi9eTFxcXMq2+fPn8+KLL6aK27x5M56ennz99dfUqFEDX19fAgIC+Pnnn7G1tU0V6+HhgaenZ6ofmUVQCCGEEEI8yWBUOBh0n6N3NRwMuo/BmP3PvOYg12SZUJzegP/YTSZpSwFCo+KpOX5zluLPTuyInVXO/5yBgYHs27cPHx+fVNvr169P+fLlWbFiBUOGDOHGjRvs2rWL2bNnM2nSpJQ4T09PQkJC2LVrFy1btsxxHkIIIYQQQoA66mvC2rP/jfrS8fulI3g52zCumz8BNbzMnV6m5PRBMbZu3TocHBywsbGhZs2ahIWF8eGHH6aJGzZsGPPnzwdgwYIFdO7cGXd391Qx/fr1Y9CgQbRq1QovLy969erFDz/8QFRUVJr2ypYti4ODQ8pP9erV8+YJCiGEEEKIQsncl9XklpzJMiFbSx1nJ3bMUuyhoPsM/fXwU+MWvNSQRhVcATAajURHRePo5JhmeJ2tpS7b+bZp04Y5c+YQExPD9OnTsbCwoE+fPmnihgwZwujRo7l69SoLFixg1qxZaWJ0Oh2//vorkydPZvv27Rw8eJAvv/ySqVOncujQIby8Hn3bsHv3bhwdHVNuW1paZjt3IYQQQghRNBmMChPWns3wshoNMGHtWdr7exbYGbnlTJYJaTQa7KwssvTTopI7Xs42ZPSy0ABezja0qOSe6nG2Vrp029Nosv8Cs7e3x8/Pj9q1azN//nwOHjzIvHnz0sSVLFmSrl278vLLLxMfH0+nTp0ybLNMmTI8//zz/PDDD5w5c4b4+Hjmzp2bKqZChQr4+fml/Dw5RFEIIYQQQhRfh4LupzmD9TgFCImM51DQ/fxLKpukyDITnVbDuG7+AGkKreTb47r551t1rtVqGTNmDJ999lmqSS6SDRs2jJ07d/LCCy+g02XtrFmJEiXw8vIiJibG1OkKIYQQQogiKiwq4wIrVVx01uLMQYosMwqo4cWcIfXwdLZJtd3T2YY5Q+rl+wV9/fr1Q6fTMXv27DT3BQQEEB4ezsSJE9N97E8//cSbb77J5s2buXLlCmfOnOHjjz/mzJkzdOvWLVVsWFgYoaGhqX70en2ePCchhBBCCFF4RMXrWXokOEuxHo42Tw8yE7kmy8wCanjR3t+TQ0H3CYuOx8PRhkYVXM0yvtTCwoIRI0bw9ddf8+abb6a6T6PR4ObmluFjGzVqxJ49e3jjjTe4fft2yoQWq1evplWrVqliq1Spkubx+/fv55lnnjHNExFCCCGEEIXOsRsPGLX4OMH3046qepwG9aRE8rwFBZEUWQWATquhiW/JfN3nggUL0t0+evRoRo8eDZDp2lsuLi6p7q9bty5//PFHpvts3bp1jtbzEkIIIYQQRZfBqDBn52Wmb72EwahQtoQtgxqV45tNFwBSTYBhjstqckKKLCGEEEIIIYRZ3I6I490lJzj43yQW3WuXZnKvGjjZWOLrbv/YOlkqz0KyTpYUWUIIIYQQQoh8tzEwlI9XnCIyTo+9lY6JPWrQu16ZlFmzky+r2X85jM27D9KhRWOa+HkU6DNYyaTIEkIIIYQQQuSbuEQDk/45y8KDNwCoVdaZWQPrUt7NPk2sTquhcQVX7p1TaGymeQtyQoosIYQQQgghRL44ezuKkYuPcznsIRoNvN7Sl/faV8bKomhNei5FlhBCCCGEECJPKYrCgn3XmLL+PIkGIx6O1kwfUIdmfhnPXl2YSZElhBBCCCGEyDN3Hybw4bKT7LgQDkC7ah583bc2rvZWZs4s70iRJYQQQgghhMgTuy6G897Sk9x9mICVhZbPulTj+Wd8Uia3KKqkyBJCCCGEEEKYVGKSkWmbzvPz7iAAKpdy4PtB9aji6WjmzPKHFFlCCCGEEEIIk7kS/pBRi48TeCsKgBea+DCmczVsLHVmziz/SJFlThHBEHsv4/vtSoKLd/7lI4QQQgghRA4pisKyIzcZ9/cZ4vQGSthZ8nXf2rT3L2Xu1PJd0ZorsTCJCIYf6sP/WmX880N9NS4PDB06FI1GwxtvvJHmvuHDh6PRaBg6dGiq7fv370en09GlS5c0j7l27RoajSbdnwMHDuTJcxBCCCGEEAVDZJyeEYuO89GKU8TpDTT1LcmGUS2LZYEFUmSZT+w9SErIPCYpIfMzXbnk7e3N4sWLiYuLS9kWHx/PwoULKVeuXJr4efPm8fbbb7Nr1y5u376dbptbt24lJCQk1U/9+vXz7DkIIYQQQgjzOnztPp1n7uafUyFYaDV8HFCVP19ujKezjblTMxsZLmhKigL62KzFJsU9PSY5LjFG/d1oVNtP1IH2ifrY0g6yOUtLvXr1uHLlCitXruS5554DYOXKlZQrV44KFSqkin348CFLlizhyJEjhIaGsmDBAsaMGZOmzZIlS+Lp6ZmtPIQQQgghROGTZDDy/fbLfL/9EkYFfEraMWtgXWp7u5g7NbOTIsuU9LHwZWnTtjk/IOVXLeCSUdyY22Bln+3mhw0bxq+//ppSZM2fP5+XXnqJnTt3popbunQpVatWpUqVKgwZMoR33nmHTz75pMhPvymEEEIIIdK6+SCWdxaf4Mj1BwD0rleGiT1q4GAt5QXIcMFib8iQIezZs4fr169z/fp19u7dy5AhQ9LEzZs3L2V7QEAAkZGR/Pvvv2nimjZtioODQ6ofIYQQQghRdKw7dZtOM3dz5PoDHK0tmDmwDt/1ryMF1mOkJ0zJ0k49o5QVoadSnaXK0LCN4FkLAKPRSFR0NE6OjmjTGy6YA+7u7nTp0oUFCxagKApdunTBzc0tVcyFCxc4dOgQq1atAsDCwoIBAwYwb948WrdunSp2yZIlVKtWLUe5CCGEEEKIgismIYkJa8+w9MhNAOqWc2HWwLp4u+bsc2hRJkWWKWk0WR+yZ2Gb9bjkNo1GsDSot58ssnJh2LBhjBgxAoDZs2enuX/evHkkJSVRuvSjoZCKomBtbc0PP/yAs7NzynZvb2/8/PxMlpsQQgghhDC/wFuRjFx0nKt3Y9BoYEQbP0a2rYSlTgbGpUeKLEFAQACJiYloNBo6duyY6r6kpCR+//13vv32Wzp06JDqvp49e7Jo0aJ0p4EXQgghhBCFn9GoMG9PEF9vOo/eoODlbMP0AXV4pmJJc6dWoEmRZS52JcHCOvNp3C2s1bg8ptPpOHfuXMrvj1u3bh0PHjzg5ZdfTnXGCqBPnz7MmzcvVZF17949QkNDU8W5uLhgY1N8p/AUQgghhCiMwqLjeX/pSXZfugtAQHVPvupTExc7KzNnVvBJkWUuLt4w4mjm62DZlVTj8oGTk1O62+fNm0e7du3SFFigFllff/01p06dSnl8u3bt0sQtWrSIgQMHmjZhIYQQQgiRZ7afv8OHy05xLyYRG0stY7tWZ1Ajb5lZOoukyDInF+98K6KetGDBgkzvX7169VPbaNSoEYqipNx+/HchhBBCCFH4xOsNfLXhPAv2XQOgqqcj3w+qS6VSjuZNrJCRIksIIYQQQgjBpTvRvL3oOOdDowF4qVl5Pg6oio2l7imPFE+SIksIIYQQQohiTFEUFh66waR1Z4nXGylpb8U3/WrTpqqHuVMrtKTIEkIIIYQQoph6EJPI6JWn2HTmDgAtKrnxbf/aeDjKpGW5IUWWEEIIIYQQxdD+K/d4d8kJQqPisdRp+DigKsOaVUCrlcktckuKLCGEEEIIIYoRvcHIjK0X+XHnFRQFKrrZM2tQXWqUSTubtMgZKbKEEEIIIYQoJm7ci2XUkuMcvxEBwIAG3ozt5o+9tZQFpiS9KYQQQgghRDGw5sQtPl0VyMOEJBxtLJjSuyZda5U2d1pFkhRZQgghhBBCFGEPE5IYuzqQlcdvAdDApwQzBtahbAk7M2dWdEmRJYQQQgghRBF1IjiCUYuPc/1eLFoNjGxbiRFt/LDQac2dWpEmRZYQQgghhBBFjNGoMHfXFb7bfJEko0IZF1tmDqxDg/Ku5k6tWJAStoDYf3s/PVb3YP/t/fmyv6FDh6LRaNBoNFhaWlKhQgU++ugj4uPjAZgwYQIdOnSgRo0aDBo0iISEhHzJSwghhBBC5E5oZDxD5h3k640XSDIqdKnlxfpRLaTAykeFpsiaMmUKDRs2xNHREQ8PD3r27MmFCxcyfcyCBQtSConkHxubgrewmqIozDw2k6uRV5l5bCaKouTLfgMCAggJCeHq1atMnz6dn376iXHjxgHwySefsHnzZgIDAzly5AhXr17Nl5yEEEIIIUTObT4TSqeZu9h35R52Vjq+7luLHwbVxdnW0typFSuFZrjgv//+y/Dhw2nYsCFJSUmMGTOGDh06cPbsWezt7TN8nJOTU6piTKPJu8XVFEUhLiku2487cPsAZ+6dAeDMvTPsuLGDZ0o/kybOaDQSlxSHhd4CrTZ1fWxrYZvt52ZtbY2npycA3t7etGvXji1btjB16lSsrKwAGDt2LL1796ZatWrZfl5CCCGEECJ/xOsNfPHPOf44cB2AGmWcmDWwLhXdHcycWfFUaIqsjRs3prq9YMECPDw8OHr0KC1btszwcRqNJqWQyGtxSXE0Xtg41+2M2jkq2485OPggdpY5nyEmMDCQffv24ePjA0BUVBRvvPEGTZo04e23385xu0IIIYQQIm+dD41i5KLjXLzzEIDXWlbkgw5VsLIoNIPWipxCU2Q9KTIyEgBX18zHlj58+BAfHx+MRiP16tXjyy+/pHr16hnGJyQkpLr+KCoqCgC9Xo9er08Vq9frURQFo9GY8mMu2d2/oiisW7cOBwcHkpKSSEhIQKvVMmvWLIxGI0OGDOHgwYNcvXqVv/76i2nTptGsWbM8fAbpMxqNKIqCXq9Hp9Pl6b6S/75P/p1F7hmMCgeuhHP0rgbnS2E84+uOTpt3Z5WLI3n95i3p37wl/Zu3pH/zljn7V1EU/jwYzFebLpKYZMTNwYqv+9SghZ8bKAb0ekO+55QXCtJrOKs5aJT8ugDIhIxGI927dyciIoI9e/ZkGLd//34uXbpErVq1iIyM5JtvvmHXrl2cOXOGsmXLpvuY8ePHM2HChDTbFy5ciJ1d6jNFFhYWeHp64u3tjZWVFYqiEG+Iz/LzUBSFt/e+zeXIyxh5VCBp0eLn7Mf3zb7P8hBAG51NtoYLvvXWW4SEhPDtt98SExPDnDlzsLCwYNasWVluIz8kJiYSHBxMaGgoSUlJ5k5H5MDJexpWXtMSkfjo9elipdC7vJHaJQvd4UcIIYQoEB7qYdEVLYEP1LNV/i5GBvsZcZRLr/JUbGwsgwcPJjIyEicnpwzjCmWR9eabb7Jhwwb27NmTYbGUHr1eT7Vq1Rg0aBCTJk1KNya9M1ne3t7cvXs3TUfGx8cTHBxM+fLlczShxt7be3lr21sZ3v9j2x9pVvrR2SNFUYiOjsbR0THX15a99NJLREREsGrVKkAtXOvWrcvIkSN5+eWXc9W2KcXHx3Pt2jW8vb3zfNISvV7Pli1baN++PZaWcoQyhU1n7vD24pM8eZBJfvV+P7A2HauXyu+0iiR5/eYt6d+8Jf2bt6R/85Y5+nfvlXt8tCKQsOgErCy0fNyxMs839s7TuQfMqSC9hqOionBzc3tqkVXohguOGDGCdevWsWvXrmwVWACWlpbUrVuXy5cvZxhjbW2NtbV1uo998o9qMBjQaDRotdo0E1E8jaIozD4xGw0alDQfQUGDhtknZtO8TPOUN0zycMDkfeZG8myLye1otVrGjBnDe++9x5AhQ7C1tc1V+6ai1WpTppnPrzdVfu6rKDMYFb7YcCGdVzcoqIXWFxsu0KlWGRk6aELy+s1b0r95S/o3b0n/5q386N/EJCPfbrnA/3ZdRVHAz8OB7wfVpZpXxh/2i5KC8BrO6v4LzdVwiqIwYsQIVq1axfbt26lQoUK22zAYDJw+fRovL688yDB79EY9oTGh6RZYAAoKoTGh6I35N/a0X79+6HQ6Zs+enW/7FEXXoaD7hERmPHxWAUIi4zkUdD//khJCCCEKqaC7MfSdu4+f/lULrMGNy7F2RPNiU2AVNoXmTNbw4cNZuHAha9aswdHRkdDQUACcnZ1Tzrq88MILlClThilTpgAwceJEnnnmGfz8/IiIiGDatGlcv36dV155xWzPI5mVzorFXRdzPz7jD5iuNq5Y6azyLScLCwtGjBjB119/zZtvvpnp1PhCZEZRFI5ef5Cl2LDorF/HKIQQQhQ3iqKw4tgtxq4JJDbRgLOtJVP71CSghvlPGoiMFZoia86cOQC0bt061fZff/2VoUOHAnDjxo1Uw+gePHjAq6++SmhoKCVKlKB+/frs27cPf3///Eo7U572nnja58/08k9asGBButtHjx7N6NGj8zcZUWRExCay+vgtlhy5ybmQqCw9xsOx4C0QLoQQQhQEUfF6Pl0VyNqTtwFoXMGVGQPr4OVcMC7rEBkrNEVWVubn2LlzZ6rb06dPZ/r06XmUkRACwGhU2HflHkuOBLPpTCiJSeq1g5ZaDTqdhnh9xksLeDnb0KhC5sswCCGEEMXR0esPGLX4ODcfxKHTanivfWXeaOUr1zEXEoWmyBJCFCy3I+JYduQmy44Gc/NBXMr2al5ODGhQlp51y3Dg6j3e/PMYQLpXH47t6i//WQghhBCPMRgVZu+4zMxtlzAYFbxdbZk5sC71ypUwd2oiG6TIEkJkWUKSga1nw1hyJJjdl8JJPsHsaGNBjzqlGdCgHDXKOKXMiBlQw4s5Q+oxYe3ZdCfBSDSYbwFvIYQQoqC5FRHHu4tPcOiaes1+jzqlmdSzBk42MitkYSNFlhDiqS6ERrPkcDCrT9zifkxiyvZnKroyoKE3AdW9sLXSpfvYgBpetPf3ZP/lMDbvPkiHFo05fD2SmdsuMXbNGZpULImHk1yXJYQQonjbcDqEj1ecIio+CXsrHZN61qB3vewtVyQKDimyTKAQrudcaEjfmk90vJ61J0NYciSYk8ERKdtLOVnTt35Z+tX3prxb1mag1Gk1NK7gyr1zCo0ruNLY151t5+8QeCuKT1ae5pcXGxTZBRSFEEKIzMQmJjFp3VkWHQoGoLa3C7MG1sGnpMzyXJhJkZULyYuRxcbGFpjFe4ua2NhYIOsLv4ncURSFw9cesORwMOtPhxCnNwBgodXQtpoHAxp607KSOxa63C2xZ6nT8m2/OnT7fg/bzoex4tgt+taXb+uEEEIUL2duRzJy0XGuhMeg0cAbrXx5r31lLHP5/6wwPymyckGn0+Hi4kJYWBgAdnZ2efptvNFoJDExkfj4+FRT1RdFiqIQGxtLWFgYLi4u6HTpD0UTphEWHc+Ko7dYdiSYq3djUrb7utszoKE3veqWxd3R2qT7rOLpyDvtK/H1xgtMWHuGZn4lZUpaIYQQxYKiKMzfe42pG86TaDBSysma6f3r0NTPzdypCRORIiuXPD3Vda6SC628pCgKcXFx2NraFpuhVS4uLil9LEwryWBkx4VwlhwOZseFMAxGdWimnZWOrrW8GNDQm3rlSuTpa+21FhXZfOYOJ4Ij+Gj5KX4f1qjYvLaFEEIUT+HRCXy4/CQ7L4QD0K5aKb7uWwtXeyszZyZMSYqsXNJoNHh5eeHh4YFer8/Tfen1enbt2kXLli2LxfA5S0tLOYOVB4LuxrD0SDArjt4kLDohZXu9ci4MaOhNl1qlcbDOn0ODhU7LN/1q02XWbnZfusuiQ8EMblwuX/YthBBC5Ld/L4bz/tKT3H2YgLWFls+6+jOkcTn5grEIkiLLRHQ6XZ4XBDqdjqSkJGxsbIpFkSVMJy7RwPrT6iQWh4Lup2wvaW9F73pl6N/Am0qlHM2Sm5+HAx92rMLkf87xxT9naVHJDW9XO7PkIoQQQuSFhCQD0zZe4Jc9QQBUKeXIrEF1qeJpnv97Rd6TIkuIIkpRFE7djGTJkWDWnrhNdEISAFoNtKrszoCG3jxbtRRWFua/vu+lZhXYdCaUw9ce8NHyU/z1SmO0skixEEKIIuBK+ENGLjrOmdtRALzYxIdPOlfDxlJG6xRlUmQJUcQ8iElk1fFbLD0SzPnQ6JTt5Vzt6N+gLH3qly1wE0zotBqm9a1Np5m72X/1Hn8cuM6LTcubOy0hhBAixxRFYcnhYCasPUuc3kAJO0um9a1NO/9S5k5N5AMpsoQoAoxGhT2X77LkSDBbztwh0WAEwMpCS6cangxo6M0zFUoW6LND5d3sGd2pKuP+PsNXG87TqrJ7ltfhEkIIIQqSyFg9n6w6xfrToQA093Pj2/61KeVkY+bMRH6RIkuIQuzmg1iWHbnJ8qM3uRURl7K9emknBjT0pkftMjjbFZ7r955/xoeNgaHsv3qPD5adZMnrTdAV4MJQCCGEeNKhoPu8s/g4tyPjsdBq+LBjFV5tUbFAf9EpTE+KLCEKmYQkA5vP3GHpkWD2XL6Los68jpONBT3rqpNY1CjjbN4kc0ir1fB131oEzNjFkesP+HVvEK+0qGjutIQQQoinSjIY+X7nRX7YfgmjAuVL2jFrUF1qlXUxd2rCDKTIEqKQOBcSxZLDwaw+cYuI2EfLBTT1LcmAht50rO5ZJC6i9Xa147Ou/nyy8jRfb7pA6yoe+Hk4mDstIYQQIkP34uG5+Uc4diMCgD71yjKhR/V8WxJFFDzylxeiAIuK1/P3idssPRLMqZuRKdu9nG3oW78s/ep7U65k0ZvufGBDbzYEhrLrYjjvLzvJijeaYKEz/yyIQgghxJP+OR3KtFM64gwROFpbMLlXDXrUKWPutISZSZElRAGjKAoHg+6z9HAw6wNDiNerk1hY6jS0q1aK/g29aVnJvUhfq6TRaJjapyYdpu/iZHAE/9t9lbda+5k7LSGEECJFTEIS4/4+w/KjNwENdb2dmTWonqz1KAApsoQoMO5ExbP86E2WHQnm2r3YlO2VPBwY0NCbXnXLUNLB2owZ5i8vZ1vGdavOB8tOMmPLJdpWLSWLNgohhCgQTt2MYNTiEwTdjUGrgfaljcx4uSG2NsXn/2mROSmyhDAjvcHI9vNhLD0czM6L4RiM6iwW9lY6utUuTf+G3tT1dkGjKbpnrTLTp14ZNpwOYdv5MN5beoLVw5thKcMGhRBCmInRqPDz7qt8s/kCeoOCl7MN3/Stwd2zB2RYu0hFiiwhzOBK+EOWHg5mxbFb3H2YkLK9gU8J+jf0pktNL+zlYlk0Gg1Tetek/fRdnLkdxY87rjCqXSVzpyWEEKIYCouK572lJ9lz+S4AAdU9+apPTewtNaw/a+bkRIEjn+KEyCcxCUn8czqEpYeDOXL9Qcp2Nwcr+tQrS78G3jKLXjo8nGyY2KM6oxaf4Pvtl2hbzaPQTlEvhBCicNp27g4fLj/F/ZhEbCy1jOtWnYENvdFoNOj1+qc3IIodKbKEyEOKonA8OIKlh4NZe/I2MYkGALQaaFPFg/4NvXm2qocMgXuK7rVLszEwlA2BoXyw7CRrRjTD2qLwT1cvhBCiYIvXG/hqw3kW7LsGgL+XE7MG1ZUvRcVTSZElRB649zCBVcdvsfRIMBfvPEzZ7lPSjv4NvOlbvyylnGzMmGHhotFomNSzBgeD7nM+NJpZ2y7xYceq5k5LCCFEEXbxTjQjFx3nfGg0AMOaVeDjTlXkSz6RJVJkCWEiBqPCrkvhLD0czNZzd9Ab1EksbCy1dK7hRf+G3jSu4FpsJ7HILTcHa77oWYM3/zrGnJ1X6ODvSW1vF3OnJYQQoohRFIU/D95g8rqzJCQZcXOwYlq/2rSp4mHu1EQhIkWWELkUfD+WpUeCWX70JiGR8Snba5V1pn8Db7rXKY2TjaUZMyw6OtX0onvt0vx98jbvLzvJurebY2Mp3ygKIYQwjQcxiXy84hSbz94BoGVld77tVxt3R5maXWSPFFlC5EC83sCmM6EsORzMviv3Ura72FnSs04Z+jfwxr+0kxkzLLomdK/O/qv3uBz2kO+2XGRM52rmTkkIIUQRsO/KXd5dcoI7UQlY6jR8HFCVYc0qoNXKCBSRfVJkCZENgbciWXokmNXHbxEVnwSARgPN/dzo38Cb9v6l5MzK4yKCIfa/IjQpCefYaxByEiz+O/TYlQQX72w1WcLeiim9avLK70f4efdVOviXokF5V9PmLYQQotjQG4xM33KROf9eQVGgors9swbWlZlsRa5IkSXEU0TG6llz8hZLDgdz5nZUyvYyLrb0rV+WvvXL4u1qZ8YMC6iIYPihPiSp64BZAq0BLjwWY2ENI45mu9Bq51+KPvXKsuLYTT5YdpINo1piayXFrRBCiOy5fi+GkYtPcDI4AoCBDb0Z280fOyv5iCxyR15BQqTDaFQ4EHSPpYeD2RAYSkKSEQBLnYYO/p4MaOhNMz83dDKEIGOx91IKrAwlJahx2SyyAMZ282fv5btcuxfL1I3nGd+9eg4TFUIIURytOn6Tz1ef4WFCEk42FnzVpxada3qZOy1RREiRJcRjIhLgx51XWXH8Njfux6Zsr1LKkf4NvelVtwyu9lZmzFAkc7a1ZGrfWrw4/xAL9l2jY3VPmviWNHdaQgghCrjoeD2frw5k9YnbADQq78r0gXUo42Jr5sxEUSJFlij2EpOMbD9/h0WHbrDrog6FywA4WFvQrXZpBjT0pnZZZ5l6vQBqVdmdQY28WXQomA+Xn2TjOy1xsJbDmhBCiPQdv/GAUYtPcON+LDqthlFtKzG8jZ+MTBEmJ59GRLF1OSyaJYeDWXnsFvdiEv/bqqGBjwsDG/nQuaanjMkuBD7t4s+ui3e5+SCOKevP8UWvmuZOSQghRAFjMCrM/fcK07dcJMmoUMbFllmD6lDfRyZOEnlDPkGKYuVhQhL/nLrNksPBHLsRkbLd3dGaXnW8cI++zNA+jbC0lHWtcu3hnXzZjYO1BdP61WLwzwf56+ANAmp40qKSe77sWwghRMEXGhnPu0tOsP+qOttt11pefNGrJs628n+9yDtSZIkiT1EUjt14wJLDwaw7FUJsogEAnVZDmyoeDGjoTZsq7ihGA+vXXzZztkVAwkPYOxP2zsi3XTb1dePFJj78tv86Hy0/xaZ3W8oC0EIIIdh0JpSPV5wiIlaPnZWOCd2r07d+WbkEQOQ5KbJEkXX3YQIrj91kyeFgroTHpGyv4GZP/wbe9KlXBg8nm5TteqPBHGkWHUYjnFwE2ybCw9B83/3Hnaqy82I41+/FMmntWab1q53vOQghhCgY4hINTP7nLH8dvAFAzTLOzBxYh4ruDmbOTBQXUmSJIiXJYGTXpXCWHA5m27kwkowKALaWOjrX9GJAQ28ali8h32CZ2vV9sPETCDmh3i5RHpqOgk2jM5/GXWupLkhsAnZWFnzTrzb9f9rPsqM36VTTk2erljJJ20IIIQqPcyFRjFx0nEthDwF4vWVF3u9QBSsLrZkzE8WJFFmiSLh+L4alR4JZfvQmd6Iefaiv7e3CgAbedKvthaMMHzO9B9dgy1g4u0a9beUIrT6Exm+oCw1Xaq+ugwXok5LYu3cvzZo1w/LE73DsNzXGmGSydBqWd+XlZhX4ZU8Qo1ecZvO7JXCxkyn3hRCiOFAUhd/2XePLDedJTDLi7mjNd/1ry3W6wiykyBKFVrzewIbAEJYcDubA1fsp20vYWdKrblkGNPSmiqejGTMswuKjYPe3cOBHMCSCRgv1XoQ2n4LDY/+ZuXg/WmhYryfS7hZ41YYy30H4BQg+ACtegWEbQWeaIviDjlXYfiGMq+ExjP/7DDMG1jVJu0IIIQquew8T+HD5KbafDwPg2aoeTOtbi5IO1mbOTBRXUmSJQkVRFAJvRbHkyA3WnLhNdLx6FkSjgRaV3BnQwJt2/h5YW+jMnGkRZTTA8T9g+2SICVe3VWwNHb+EUtWz3o7OAvr8DHObw60jsONLaDfOJCnaWOr4tl9t+szZx+oTtwmo4UVADU+TtC2EEKLg2X0pnPeWniQ8OgErCy1jOlXlxabl5dIAYVZSZIlCISI2kdXHb7HkyE3OhUSlbC/jYkv/Bt70bVBWVmrPa1f/hU1j4E6gerukH3T4Aip3VKvc7HIpB91mwbIXYc90qNhKLdhMoG65Erzeypc5O6/w6arTNCxfQr7NFEKIIiYxycg3my/wv11XAajk4cCsQXWp5uVk5syEkCJLFGBGo8K+K/dYciSYTWdCSUwyAmCl09KxhicDGnjT1LckWlmlPW/duwKbP4ML69XbNi7QejQ0eBkscnm9U/WecHUoHF0AK1+HN/eCvVvu2vzPO+0qsf1cGBfuRDN2zRlmP1fPJO0KIYQwv6vhDxm1+ASnb0UC8FzjcnzWxR9bKxnJIgoGKbJEgXM7Io5lR26y7GgwNx/EpWyv5uXEgAZl6Vm3jExmkB/iHsC/0+DQ/8CoB40OGr6iFlh2rqbbT8cpcH0/3L0Aq9+CwUtydmbsCdYWOr7tX5ses/fyz+kQAk7eplvt0iZIWAghhLkoisKyozcZ//cZYhMNuNhZMrVPLTpWl2HhomCRIksUCAlJBraeDWPJkWB2XwpHUWdex9HGgh51SjOgQTlqlHGS8dX5wZAER39Vr5OK+29CkUodoMNkcK9i+v1Z2UHf+fDzs3BpExz8CZ55wyRN1yjjzPA2fszadonP1wTSuKIrHo42T3+gEEKIAicyTs+nq06z7lQIAM9UdGX6gDp4OcvlAqLgkSJLmNWF0GiWHA5m1fGbPIjVp2x/pqIrAxp6E1DdS07956dLW2HzpxB+Xr3tXhU6fgF+7fJ2v5411P2s/wC2fA4+TcGrlkmaHtHGj61n73A2JIpPVwXyv+frS7EuhBCFzJFr9xm1+AS3IuLQaTW8174yb7TyRSeXDIgCSooske+i4/WsPRnCkiPBnAyOSNnu4WhN3/pl6d/Am/Ju9uZLsDgKvwCbPoXLW9Tbtq7w7KdQb6g6E2B+aPgKXNmuXvu1fBi8/i9Y5f51YGWh5dv+ten+wx62nL3DquO36F2vrAkSFkIIkdeSDEZm77jCzG0XMSpQztWOmQPrULdcCXOnJkSmpMgS+UJRFA5fe8CSw8GsPx1CnN4AgIVWw7NVPRjQ0JtWld2x0Mlq7Pkq9j7snAKH54FiAK0lNH4dWn4Iti75m4tGAz1mw5xmcO8SbPgYevxgkqareTkxqm0lvtl8kXF/n6GprxuezjJsUAghCrJbEXG8s/g4h689AKBX3TJM7FEdRxvTrKsoRF6SIkvkqbDoeFYcvcWyI8FcvRuTsr2iuz0DGnjTu15Z3B1lau18l5QIh3+Gf6dCvDozE1W7QvuJUNLXfHnZuULv/8Fv3dT1uHzbQI0+Jmn6jVa+bDl7h5M3Ixm98hS/Dm0owwaFEKKAWn86hNErThEVn4SDtQWTelanV10ZhSAKDymyhMklGYzsuBDOksPB7LgQhsGozmJhZ6WjS00vBjT0pr5PCfmAaw6KAhc3qkMD719Rt5WqCQFfQoWW5s0tWYUW0PID2DUN1r4DZepDifK5btZCpw4b7DxrDzsvhLP0SDADGpbLdbtCCCFMJzYxiQl/n2XJkWAAanu7MGtgHXxKymUEonCRIkuYTNDdGJYcDmbFsZuERyekbK9bzoUBDbzpWrs0DtbykjOb0EB1MeGgf9Xb9h7w7GdQdwhoC9jkIq1GQ9AuCD4IK16BlzaALvfDQ/w8HPmgQ2W+XH+eSevO0czPjbIl7EyQsBBCiNwKvBXJyEXHuXo3Bo0G3mrtyzvtKmMplxKIQkg+8YpciU1MYv3pUJYeDubQtfsp213trehdtwwDGnpTqZSjGTMUPAyHHZPh2O+gGEFnDU3egubvgY2TubNLn84C+vwCc5rDzcOw8yto+7lJmn65eUU2nbnD0esP+HjFKf4Y1lgWtBZCCDMyGhXm7w1i6sbz6A0Knk42fDegNk19TbM4vRDmIEWWyDZFUTh1M5IlR4L5+8RtHiYkAaDVQMvK7gxo4E3baqWwspBvnswqKQEOzIFd30BitLrNvye0n2CS4Xd5zqUcdJ8Jy4bC7m+hYiuTDGnUaTV80682nWbuYu/le/x16AbPP+OT+3yFEEJkW1h0PB8sO8Wui+EAdPAvxdQ+tShhb2XmzITIHSmyRJY9iElk1fFbLD0SzPnQ6JTt3q629K/vTd8GZWVBwIJAUeDsGtgyFiKuq9tK14WOU8CniXlzy67qveDKDjj2G6x8Dd7YC/Ylc91sBTd7Pg6oyoS1Z5my/hytKrlTrqQMGxRCiPy040IYHy47yd2HiVhbaPm8qz/PNS4n12yLIkGKLJEpo1Fhz+W7LDkSzJYzd0g0GAF17aFONTwZ0MCbZyqWlOFWBcXt47BxDNzYp9529IK246DWANAW0jOLAV/BjQNw9wKsGQ6DFqnTvefSi03KszEwlINB9/lg+UkWv/qMvI6FECIfJCQZmLrhAvP3BgFQ1dORWYPqUlkuLxBFSKH51DVlyhQaNmyIo6MjHh4e9OzZkwsXLjz1ccuWLaNq1arY2NhQs2ZN1q9fnw/ZFn43H8QyfctFWny9gxfmH+KfUyEkGoxUL+3ExB7VOTymHTMH1qWpn5t8MC0IokJg9VvwvzZqgWVhC60+hrePQp1BhbfAArCyg77z1GvJLm6AQz+bpFmtVsO0vrWxs9JxKOg+v+67ZpJ2hRBCZOxyWDQ9Z+9LKbCGNi3P6uHNpMASRU6hOZP177//Mnz4cBo2bEhSUhJjxoyhQ4cOnD17Fnv79Kf13LdvH4MGDWLKlCl07dqVhQsX0rNnT44dO0aNGjXy+RkUfAlJBjafucPSI8HsuXwXRZ15HScbC3rWLUP/Bt7UKONs3iRFavo42PcD7JkO+v/WIavZH9qNA+citJ6IZ03oMBk2fAibP1OHPXrWzHWz5UraMaZzNT5bHcjXG8/Tpoo7Fd0dTJCwEEKIxymKwuLDwUxYe4Z4vRFXeyum9a1F22qlzJ2aEHmi0BRZGzduTHV7wYIFeHh4cPToUVq2TP9i+JkzZxIQEMCHH34IwKRJk9iyZQs//PADc+fOzfOcC4tzIVEsORzM6hO3iIjVp2xv6luSAQ296VjdExvLAjbFd3GnKBC4AraMg6ib6rayDdWhdWUbmDe3vNLoVbiyXT2btXwYvLYTrHK/bspzjcux6Uwouy/d5YNlJ1n2RlN0cnZWCCFMJiI2kU9WnmZDYCgAzf3c+K5/bTycbMycmRB5p9AUWU+KjIwEwNXVNcOY/fv3895776Xa1rFjR1avXp3hYxISEkhIeLTGU1RUFAB6vR69Xp/Rw/JF8v5NkUd0vJ61p0JZfuwWp29FpWwv5WRNn7pl6FOvNOVckycCMKLXG3O9z4LOlP2blzS3jqDd8hnaW0cAUJzKYHh2HIp/L/VapQKav0n6t8sMLG4fR3P3Isb1H2PoMt0kuX3Rw5/O3+/j2I0I5u68xGstKpik3fxUWF6/hZX0b96S/s1b5uzfQ9fu8/6y04RGJWCh1fBeez9ebloerVZTZP7e8vrNewWpj7Oag0ZRkgeFFR5Go5Hu3bsTERHBnj17MoyzsrLit99+Y9CgQSnbfvzxRyZMmMCdO3fSfcz48eOZMGFCmu0LFy7Ezq5wzz6mKHAlCg6EaTlxX4PeqH5br9Mo1Cih8IyHQlUXBfkSv2CySbyH/+2leD/YD0CS1ppLpbpx2SMAo7b4THXrFn2WppenokHhcPkR3C7RyCTtHgjTsOiKDp1G4cNaBrwK99tdCCHMyqDAxmAtW25pUNDgbqPwQiUD5WREtijkYmNjGTx4MJGRkTg5ZbzeaKE8kzV8+HACAwMzLbBy6pNPPkl19isqKgpvb286dOiQaUfmB71ez5YtW2jfvj2WlpZZftydqHhWHb/N8mO3uX4/NmW7n7s9/eqXoUed0pSU9Shy3L95LvEh2v3foz39I5qkOBQ0KLUGobQeg5+jJ37mzi+LTNe/nTHuTES39zsahPxOUpdh6ppaudRJUbj953H+vXiXdeGuLH2tEZa6wjNhSIF9/RYR0r95S/o3b+V3/wY/iOX9Zac5fksdddSnXmk+71wVe+tC+bHzqeT1m/cKUh8nj3J7mkL3ah8xYgTr1q1j165dlC2b+YX9np6eac5Y3blzB09PzwwfY21tjbW1dZrtlpaWZv+jJstKLnqDke3nw1h6OJidF8MxGNUTlvZWOrrVLk3/ht7U9XaRtSjSUWD+1kYjnFoM2yZCdIi6zacZmo5foildp/BMDfoEk/Tvs2Pg+h40Nw9hueYNeGkD6HJ/OPu6b206TN9F4O0o5u29wdttK+W6zfxWYF6/RZT0b96S/s1b+dG/a07c4rNVgUQnJOFobcEXvWvSvXbpPN1nQSGv37xXEPo4q/svNEWWoii8/fbbrFq1ip07d1KhwtOvmWjSpAnbtm3jnXfeSdm2ZcsWmjQpZAuyAgajwsGg+xy9q6Fk0H2a+Hmke3H+lfCHLD0czIpjt7j78NG1ZQ18StC/oTddanoV2W+SipTr+2HjaAg5od528VFn16vWzSRrRBV6Okvo8wvMbQ43D8G/X8Gzn+W62VJONkzoXp13lpxg1vZLtK1WCv/S5j2DLYQQhcHDhCTGrTnDimPqZEz1fUowY0AdvF1l7LUongrNp+3hw4ezcOFC1qxZg6OjI6Gh6gw1zs7O2NraAvDCCy9QpkwZpkyZAsCoUaNo1aoV3377LV26dGHx4sUcOXKE//3vf2Z7HjmxMTCECWvPEhIZD+j4/dIRvJxtGNfNn4AaXsQkJPHP6RCWHg7myPUHKY9zc7Cid72y9G/gjZ+HDIIuFB5cgy1j4ewa9baVI7T6EBq/ARZpz7AWayV8oNsMdabBXd9AhVZQoUWum+1RpzTrT4ew+ewd3lt6gr9HNMfKorCeNxRCiLx3MjiCUYuPc+1eLFoNjHi2EiOf9cOiEA25FsLUCk2RNWfOHABat26davuvv/7K0KFDAbhx4wbaxxZdbdq0KQsXLuSzzz5jzJgxVKpUidWrVxeqNbI2Bobw5p/HeHJ2ktDIeN748xjNfEtyIjiCmEQDAFoNtK7iQf8G3rSt5lGorikp1uKjYPe3cOBHMCSCRgv1XoQ2n4KDu7mzK7hq9IErO+D4H7DyVXhjL9iXzFWTGo2GL3rV5PC1+5wPjeaH7Zd4r0MVEyUshBBFh9Go8L/dV/lm0wWSjAqlnW2YMbAujSpkPPOzEMVFoSmysjIJ4s6dO9Ns69evH/369cuDjPKewagwYe3ZNAUWkLJt75V7APiUtKN/A2/61CuLp7OsO1FoGA1qgbB9MsSEq9sqtoaOX0Kp6mZNrdDoNBVuHIB7l+DvETBwYa6HVLo7WjO5Z02GLzzG7J1XaOdfilplXUyTrxBCFAF3ouJ5b+kJ9l5WP4d0runJlF61cLaTa5KEgEJUZBVHh4Lu/zdEMHOfd6nGS80qoJW51wuXq//CpjFwJ1C9XdIPOnwBlTvKdVfZYWUPfefDL23hwno4/Iu6cHEudanlxYZAL9adCuH9pSdZ+3ZzWZRbCCGArWfv8OHykzyI1WNrqWN8d3/6N/CWybSEeIyMJSvAwqKfXmABuDlaS4FVmNy7AosGw+/d1QLLxhk6ToE390OVACmwcsKrFrSfpP6+6VMIDTRJsxN71MDNwYpLYQ+ZvvWiSdoUQojCKl5vYOyaQF75/QgPYvX4ezmx9u3mDGhYTgosIZ4gRVYB5uGYtWF/WY0TZhYXoRYAsxvDhX9Ao4NGr8PIE9DkLbCQtcpypfHrUKkjGBLUyTASY5/+mKdwtbfiy141Afh511WOPjaxjBBCFCcXQqPp8cNeft9/HYBXmldg1fCmMrGWEBmQIqsAa1TBFS9nGzL6bkgDeDnbyAWmBZ0hCQ79DLPqwv4fwKiHSh3grf3Q+Wuwk7+fSWg00PNHcPCEuxdg0ycmabZDdU961y2DUYEPl50k7r9JZoQQojhQFIU/9l+j+w97uHAnGjcHKxa81JDPuvpjbSFDqEUeigiG2yfUn5CTOMdeg5CTj7ZFBJs1vaeRa7IKMJ1Ww7hu/rz55zE0kGoCjOTCa1w3/3TXyxIFxOWt6tmr8PPqbfeq0PEL8Gtn3ryKKns36P0T/N4Tji6Aim2ges9cNzuuW3X2XrnL1bsxTNt0gbHd/HPdphBCFHT3YxL5aPkptp67A0DrKu5M61sbd0dZUkTksYhg+KE+JKlrvloCrQEuPBZjYQ0jjoKLd/7nlwVSZGVRYqL68yStFiwsUsdlRKOBxxeJzkpsQA0v5gypx7jV5wh9bBKMUk42fNa1Gs9W9kKvT92uXg8ZTcb4ZA6ZxQJYWeUsNikJjEbTxFpaPrpMKa9iDQb176HXa0hMTPs8n4w1ZHIyw8ICtPcuwKZPMVzchkHRgU0paP2xOi27zgISH4vVZqPdHMQajWpfZESnU3/yOjaz/n08VlHU11pW2k03tmxraPw+7JuJdvW7WJSpBy7lntru4+/lJ2NtLSyZ1K0Wr/5+hPm7r9G2SimaVX40VXxm7+W8PkYk0+sz7t/svO/lGJF+rMGQcf+mF5sX7+WifozIrH9Neox4TGbve1PFQsE4RiQlZdy/6b3v9126ywfLTxIWnYCVTscHHavwYpPy6HSpv9iVY4T6e/LniIyenxwjshkbcR9dop7klYiMioYk4xNli8EIEffBzjtfjxGZve8eJ0VWFn37LVin88VNpUrw3HOPbk+blvEfq3x5+G9JLwBmzIDYDC4bKV0aXntN/T2ghhdnt3gSeCeGK9dv4etThrJ29hxdo+HoGnB3h+HDHz32f/+D8PD023VxgXfeeXT711/h9u30Y+3s4KOPHt3+6y+4di39WEtL+PTTR7eXLIFLl9KPBRg//tHvK1fC2bMZx44Z8+hgum4dnDiRceyHH4K9vfr7pk1w+HDGse+8o/YHwLZtsHu3lkuXKnPypDblzZfsrbfAw0P9ffduSGe1AJU+jlerfEOZoOmgGDhwuxlb4t8Hn2awxQa2pA4fOlR9XQAcPQrr12ec7+DBULmy+vvp07B6dcax/fpB9f9mgD93DpYtyzi2Z0+oU0f9/fJlWLgw49jOnaFRI/X3GzdgwYKMY9u3h2bN1N9DQmDu3Iz7t3Vr9QfU1+6PP2bcbtOm0KGD+ntkpPo+SsM4Bk6Uo6HjOrqseBWG/kNsvAXTpmXcbp06al+A+h7+8ssnIzwoe70+Z25H8sqtEI7Md8beWj2Epo19JD+OEQBz5mg5fDj9/pVjxCM5PUZs365h+fL0+xeycYwAXn0VypRRfz9wALZsyTi2OB0jMutfkx8j/tOwIXTpov4eG0sujxGP+PtD//6PbheEY8T69RU4cSL9/n38GKE3GOk7Kox/T0UB3pSws6J9TU9C/rXhq3/lGPG4x48Rp065c/x4+v0LcoxIluVjRHQp2j9oSLNyBwEIifbk52Mvpo2LLQWO+XuMSEjIOP5xck1WIaHVaCjjYouXnUIZF1u0MotPwWI0QPBhODgXTvwFigGqdIHeP4NfW7CUyUnylVYH/j3A0g6CD8Cur03SbMvKbjhaW3D3YSJfbThvkjaFEKKguHY3hr5z9rHn8l0AapRxZnDjcjLBlsgfRiM8DFOvu7q+z9zZ5JpGycoqv8VYVFQUzs7OhIdH4uTklOb+/BwKlJioZ8OGDXTq1AnLx+6UoUC5jzUYID4+/f5NLzblFLuiwMVNsGUsPLgKgIVnVbSdvoCKrYrvaf50YuPiMu7fPDvNf2YFFmuGgUaL8sJa9GWaZxybxaFAey/dZehvh9BoFf56pTHN/NwKxFCg2Fg969en379yjMh9bHy8nnXr0u/fJ2MLwvu+sB0jEhL0rF2bcf/KcEFVTo8Rer2ev//eQEBA+v0LCmtP32LsmkBiEg04WloyuVdNOtXwemoOcoxQ+3ft2vV07Ng5g/6VY0SaWKMBY9glkoL/m8gi5ASEngbDo0tjdBoDOq36B0h3uCDAy9ugdO18PUZERUXh7u5MZGT6tUEyGS6YRVZWqd/QmcVlp82sSn7PWloqWFmlPsBlFJuddk0da5GNV1ZBiNXp+K9fn96/KW/O0EB1MeGgf9U7nNzh2c+h7hD1TAqp38hZySEvYrXarL/W8jo2K/2r0WS93afG1u0D17fDiT/RrHoNqzf2ZGk2x8zabVPdjeebevPngRt8tPwUG99pgaNN1t8ceXmMyEr/Jsdmp928iC0I7/vsHiOy2r8F4X1fGI8RWe1fkx4j8iEWCkashUX6/RsVr+fz1YGsOaGO+WtUwZUZA+pQ2sU2S+3KMUL16HNE1mLN/b7P12OE0Qj3r8Lt449+Qk6i1ceQplk7J/CqDU5l4NTiR+1qFKx06VRDVgpPNpLX7/usxkuRJUR2PQyHHZPh2O+gGEFnBU2GQ/P3wCbjbzSEmXSaqg4ZvHcZ/n4bBvyZ6wWfP+lUjV0X73Ljfixf/HOOr/rUMlGyQgiRf47deMCoxccJvh+HTqthVNtKDG/jJ7MWi5xTFHhwLU1BRUJU2lhLe7WgKl330Y9rRbVSu30iVZFVGEmRJURWJSXAgTmw+9tHBwv/ntB+ApQob87MRGasHaDvfPilHZxfB0fmQcNXctWkvbUF0/rWYsD/DrD4cDAda3jSpoqHiRIWQgjTMRgVDgbd5+hdDSWD7tPETz1Wzdl5melbL2EwKpQtYcvMgXWo7yPrNopsUBSIvJm6oLp9HOIj0sZa2IBnrdQFlVullJE/RZEUWUI8jaLAub//u+7qmrrNqw4ETAGfpubMTGSVV21oN0FdoHjjGCjXBEpVz1WTjSuW5KVm5fl17zVGrzjF5nda4WyXjbEwQgiRxzYGhjBh7VlCIuMBHb9fOoKHozXOtpZcCnsIQLfapfmiVw2csjHsWRRTUSFpC6rYu2njdFZQqkbqgsq9qrqETVbZlVTXwUrKZCo/C2s1roCSIkuIzNw+oV53dX2vetvRC9qOg1oDHl0RKgqHZ96Eqzvg0mZY/jK8uh2s7HLV5Ecdq7LzQjhBd2OYsO4M3/WvY5pchRAilzYGhvDmn8d4cp6JsOgEde0rCy1f9qpJn3pl0MiMxeJJD8PTFlQPQ9PGaS3AoxqUrveooPLwB4tsXDCYHhdvdaHh2HsA6JOS2Lt3L82aNcMy+WI5u5IFdiFikCJLiPRFh8K2Sep07ChgYQvNRkKzUWBlb+7sRE5oNNDjR5jbDMLPweZPoev0XDVpa6Xjm3616Td3HyuP3aJTDS/a+5cyUcJCCJEzBqPChLVn0xRYj3O2taRXXSmwBBB7/4mC6gRE3Uwbp9GqZ6QeP0NVqjpYZm2SlGxz8X5UROn1RNrdUkemZGcGFTOSIkuIx2iNiWj3fAf7ZoI+Rt1Ysz+0GwfOZc2bnMg9B3fo9RP80QuOzIeKbcC/e66arO9TgldbVuSnf6/yycrTNPApQQn7XH6DJ4QQuXAo6P5/QwQzFh6dwKGg+zTxLbjDrUQeiI9Ui6jHi6qI6+kEatRrph4vqDxryhfN2SBFlhAAioLmzEranv0EnV49NU3ZhhDwFZRtYN7chGn5tlHPSO6dAX+PUP/jyOVwg3fbVWb7uTAuhT3k8zWB/DC4nmlyFUKIHAiLzrzAym6cKKQSHkLoqdQF1b3L6ce6VnyioKolMybnkhRZQtw8Ahs/weLmISwAxakMmvYToUafXE/1LQqoZz+Da7vh1lFY+Rq8uDZ7F+Q+wcZSx7f9a9Prx32sOxVCpxohdKmVdhFPIYTIDx6ONiaNE4VAYizcCUxdUIVfgPQGjbqUS11QedUG2xL5nnJRJ0WWKL4ib8LWCXB6KQCKpT3n3QLwe34Glnby7U2RprOEPr/A3JZwYx/s/gZaj85Vk7XKuvBWa1++336Zz9cE0riiK24O1iZKWAghsq5WWWesdFoSDcZ079cAns42NKogU7YXSkkJTxRUJyDsHCiGtLFOZf4rpur8V1DVBXsZIpofpMgSxU9iDOydCXtnQVIcoIE6z5HU8mMu7j6OX15dwCkKFteK6sQXK1+Bf6dChZa5npL/7WcrseXsHc6HRvPpqtPMHVJfLioXQuSreL2BN/48mmmBBTCum78sOlwYGPQQdjb1Gao7Z8GoTxtr7wFlHpvlz6sOOMpkTOYiRZYoPoxGOLUEtk2A6BB1m08z6Pil+g2PXg8cN2eGIr/V6gdXtsPJhbDiVXhjN9jl/JtdKwst3/avTY8f9rLpzB3WnLhNz7plTJiwEEJkLCFJLbB2X7qLraWON1v7sujQjVSTYHg62zCumz8BNWRIc4FjSIK7F1IXVKGBYEhnrShb19QFVem66jIz8sVegSFFligeru9XF6K9/V8R5eIDHSZBte5yQCruOn8NwQfh/hVYOxL6/5Gr10T10s6MbFuJ77ZcZNzfZ2jiW5JSTnLdgxAibyUkGXjzz2PsvBCOjaWW+UMb0sS3JMPb+LH/chibdx+kQ4vGNPHzkDNYBYHRoE5C8XhBFXLqvxE2T7BxTl1Mla4Lzt7y+aWAkyJLFG0PrsGWcXB2tXrbyhFafgCN3wBL+eArAGtH6DsPfmkP59bC0V+hwbBcNflma1+2nL3D6VuRfLLyNPNebCDDBoUQeSYxycjwv46x/XwY1hZa5r/YMGVqdp1WQ+MKrtw7p9C4gqsUWOagKHD/6hMF1UlIfJg21spBHeZX5rGCqkQFKagKISmyRNEUHwV7voP9P6qn2TVaqPcCtPkUHDzMnZ0oaErXhfYTYNMY2PgJlGuirmCfQ5Y6ddhg11l72H4+jGVHb9K/QcFdlV4IUXjpDUZGLDzG1nNqgTXvxYY09XMzd1rFl6JAxI20i/smRKaNtbRTp0p//AxVST/QavM9bWF6UmSJosVogON/wvbJEBOmbqvQSr3uyrOGeXMTBVvjN+HKDri8BZYPg1e352oV+8qlHHm3fWWmbjzPpLVnae7nRmkXmVRFCGE6eoORkYuOs/nsHawstPzvhQY0ryQFVr5RFIi6jSb4CFVvL0e3aAGEnIC4+2ljddbqYr6PF1RulXO1fIgo2OQvK4qOoF2wcQzcOa3edvWFjl9A5QA5zS6eTquFnnNgTlN1JqfNn0GXb3PV5GstK7L5bCjHb0Tw8YpT/D6skQwbFEKYRJLByDuLT7AhMBQrnZafnq9Pq8ru5k6raIu+88QZquMQE4YFUOXxOK0llKqeuqDyqKYuHyKKDSmyROF37wpsGQvn16m3bZyh1Who+ApYWJk3N1G4OLhD75/gj15w+Beo2Aaqdc1xczqthm/61abzzN3svnSXhYdu8FxjHxMmLIQojpIMRt5depJ/TodgqdMwZ0g92lSRofAmFXNXHeb3eEEVfTttnEaH4l6NG4aSlG3YFZ13A7XAspB1Eos7KbJE4RUXAbumwcGf1PUiNDpo+DK0/iRX03CLYs73WWg2Sl1Lbc1wdXp/57I5b87dgY8CqjJp3Vm++OccLSu54+1qZ7p8hRDFisGo8MGyk6w9eRsLrYYfn6tP22qyFlKuxD14oqA6AZE30sZptOBWJfUZKs8aJGHBifXrKV2/MzpLOVslVFJkicLHkATHFsCOLyH2nrrNrz10mAweVc2amigi2nwGQbvh9jFY+Rq8uBa0uhw391LT8mwKDOXQtft8uPwkC195Bq3M8CWEyCaDUeHD5SdZfUItsH4YXI/2/lJgZUt8lDqz3+NnqB4EpR9bstITBVVNsHZIG6dPZ2FgUexJkSUKl8tbYdNnEH5Ove1WRZ3UolI78+YlihYLK3Va97kt4fpe2PUNtP44x81ptRqm9atFwIzdHLh6n9/3X2NoswomTFgIUdQZjQofrzjFymO30Gk1fD+oLgE1PM2dVsGWGKOuPfV4QXXvUvqxJSqkLqi8aqmXHwiRQ1JkicIh/CJs/hQubVZv27pCmzFQ/yWZmUfkDdeK0PU7WPkq/PsVVGgJPk1y3JxPSXvGdK7K52vO8NXG87Sq4kEFN3sTJiyEKKqMRoUxq06z/OhNdFoNMwfWoVNNL3OnVbDo4+DOGbWQunVM/ffuBVCMaWOdy6lDwVMKqtpymYEwOfl0Kgq22Puw8yt1EgLFoM7Y0/h1dUFh2xLmzk4UdbX6w5XtcHIRrHgF3tyTq9fdc4192HgmlL2X7/HBspMsfb2JLAwqhMiU0ajw2ZpAFh8ORquB7/rXpmut0uZOy7ySEiHsTOozVGHnwJiUNtaxdOozVKXrgL1Mcy/ynhRZomAy6NXCaudXEB+hbqvSBTpMgpK+Zk1NFDOdp0HwQbh/Ff4eCf1/z/GSAFqthql91GGDR68/YP6eIF5tWdHECQshigpFURj7dyALD95Ao4Fv+9emR50y5k4rfxn0EH4+dUF15wwYEtPG2rs/UVDVBUcZUinMQ4osUbAoClzcpA4NvHdZ3VaqhnrdVcVW5s1NFE/WjtB3PvzSHs79DUcXQIOXctxc2RJ2fNalGqNXnmba5gu0qeqOn4ej6fIVQhQJiqIw/u8z/HlALbC+6VubXnVzPtNpoWA0wN2LqQuq0NOQFJ821rZE2oLKqYysiykKDCmyRMFx5wxsGgNXd6q37d3h2c+h7pBczewmRK6VrgvtxqkLFG8cDeWeUReWzKEBDb3ZEBjKvxfDeX/pSVa82RQLndaECQshCjNFUZi47iy/7b8OwNQ+tehTv4gVWEYj3L+SuqAKOQn62LSx1s5QunbqgsrFRwoqUaBJkSXM72E47PgCjv2mXqCqs4Jn3oIW74ONk7mzE0L1zHC4sgOubIPlL8Or28DSNkdNaTQavupTkw7Td3HyZiQ/7brK8DZ+Jk5YCFEYKYrCl+vP8eveawB81bsm/Rt4mzep3FIUdZr0x9ehun0CEqPTxlo5qBNRPF5QlagAWvkiShQuUmQJ80lKgINz1emxE6LUbf49of0EKFHenJkJkZZWC73mwpym6gXXmz+HLt/kuDkvZ1vGd6vO+8tOMmPrRdpW86Cqp3ypIERxpigKX208z8+71XWbvuhVg4GNypk5q2xSFIgMTn2G6vaJR9dXP87CVp0q/fGCqqSfjF4RRYIUWSL/KQqcWwtbPocH19RtXnUgYAr4NDVnZkJkzsFDLbT+7AOHfwbfNlC1S46b612vDBsCQ9l67g7vLz3J6uHNsJRhg0IUS4qiMG3TBX769yoAk3pU57nGPmbOKguibj9RUB2H2Htp43RW6mK+jxdUblVkGRZRZMkrW+Sv2ydg06dwfY9628FTvdal1kAZCiAKB7920PRt2Pc9rBmufkHgnLPZvjQaDV/2rsGR6fc5czuKH7Zf5t32lU2brxCiUJi+5SI/7rwCwPhu/jzfpHzuGowIflTsJCXhHHtNvebJ4r+PfnYlwSWbwxAfhv031O+xguphaNo4rQWUqp66oHKvpi70LkQxIUWWyB/RobBtEpz4C1DAwgaajoRmo8DawdzZCZE9z46FoN0QcgJWvgYv/p3j4S0ejjZM7FGDkYuOM3vHZdr7l6JGGWfT5iuEKNBmbL3IrO3qjLqfd/VnaLMKuWswIhh+qK8OywcsgdYAFx6LsbCGEUczLrRi76cd8hd1M22cRqdOBPT44r4e1cHSJnfPQYhCTooskbf0cbD/B9g9HfQx6raa/aDtuOx/gyZEQWFhpU7r/lNL9azs7u+g1Yc5bq5bLS82Boaw/nQo7y89yd9vN8PaQq5JEKI4+H7bJWZsvQTAp52r8XLzXBZYoJ7B+q/AylBSghrn4g1xEepZrseLqojr6TxIA+5VUp+hKlUDrOxyn7MQRYwUWSJvKAoEroCt49ULYAHKNoSOU8C7oVlTE8IkSvpCl29h1euwcwpUaKFO7Z4DGo2GST1qcPDqfS7ciWbm1kt8FFDVxAkLIQqa2Tsu8+2WiwCM7lQ1/xcn3zZRvTb6/pX073f1TV1QedVS1w4UQjyVFFnC9G4ehU2fQPBB9bZTWXXGwBp9ZE0LUbTUHghXtsOpJbDiFXhjD9i65Kipkg7WfNGrJm/8eZS5/16hvX8p6pYrYdp8hRAFxtx/rzBtkzp+78OOVXijlW/+J3Fl26PfXXyeKKhq5/h4JoSQIkuYUuQt2DZB/cAJYGkPzd+FJsNlKIEoujp/A8GH1DVg1o6Cfgty/GVCQA1PetQpzZoTt3l/2UnWj2yBjaUMGxSiqPll91W+2nAegPfaVzbfOnkNX4EqndWiys7VPDkIUUTJdG4i9xJjYMeX8H39RwVWnefg7aPqdSpSYImizMYJ+s5TZ9M6uxqO/Z6r5iZ0r46HozVXw2P4dvOFpz9ACFGozNsTxOR/zgEwqm0lRratZL5k6j4Pfm2lwBIiD0iRJXLOaISTi9Xi6t+pkBQH5ZrCazuh54/g5GXuDIXIH2XqQ9ux6u8bPobwnBdHLnZWfNWnJgC/7Ani8LX7pshQCFEALNgbxKR1ZwF4+1k/3mlnxgJLCJGnpMgSOXPjAPzyrHrRf3SIOpa7/+/w0np12IEQxU2Tt6FiG/XLhuXDQB+f46aerVqKfvXLoijwwbKTxCYmmTBRIYQ5/LH/GuPXqgXWW619ea99ZTR5dZ3ypS15064QIsukyBLZ8+A6LBsK8zuqU7xaOUK7CTD8EPj3kIktRPGl1UKvn8DODe4EwpaxuWru827+eDnbcP1eLFP/u3ZDCFE4LTx4g8/XnAHg9ZYV+bBjlbwrsI7Mhx2Tnx5nYa0uSCyEyBMy8YXImoRodS2g/bPBkAAaLdR7Adp8Cg4e5s5OiILBsRT0mgt/9YVDP4FvG6jSKUdNOdlYMrVPLV6Yf4jf9l+nYw1Pmvq6mThhIUReW3zoBmNWnQbgleYVGN2pat4VWHtnPvqCp9YgaPw6aDTok5LYu3cvzZo1w9Liv49+diVlvUoh8pCcyRKZMxrUC/ln1YM936kFVoVW8Ppu6DZTCiwhnlSpPTQZof6++i2Iup3jplpWdmdw43IAfLT8FA8TZNigEIXJ0iPBfPJfgfVSs/J82qVa3hRYigLbJz8qsJq/B73mQJm6ULoOeNUm0q68Oi176TrqjxRYQuQpKbJExoJ2wf9awd9vQ0yYuijhwEXwwhrwrGHu7IQouNqOVT/MxN2Hla+pX1bk0JjO1ShbwpabD+L44r8ZyYQQBd+Kozf5eMUpFAVebOLD2K7+eVNgGY2wcTTsmqbebjsO2o2T4ftCmJkUWSKte1dg8XPwWzcIPQ02ztBxCrx1AKp2lgO3EE9jYQ195qtrxV3bDXum57gpB2sLvu5bC4BFh26w62K4qbIUQuSR1cdv8cHykygKDHmmHOO7V8+jAsugfhF6cK56u/M30OI90+9HCJFtUmSJR+IiYNOnMLsxnF8HGh00eg3ePg5N3gILK3NnKETh4eYHXb5Rf9/xpbpgcQ419XVjaNPyAHy84hSRcXoTJCiEyAtrTtzivaUnUBQY1KgcE7vXyJsCKylRncn0xJ/qddI950KjV02/HyFEjhSqImvXrl1069aN0qVLo9FoWL16dabxO3fuRKPRpPkJDQ3Nn4QLC0MSHP4Fvq8H+38Aox782sOb+6DzNLCX2YeEyJHag6BmP1AMsPxl9YuMHPoooArlS9oREhmfss6OEKJgWXfqNu8uOYFRgYENvfmiZw202jwosBJjYfFgdQF0rSX0+w3qDDL9foQQOVaoiqyYmBhq167N7Nmzs/W4CxcuEBISkvLj4SGTNaS4vA3mNod/3ofYe+BWBZ5bAUOWg0dVc2cnROGm0UCX79R15CJvwLp31QvUc8DOyoJv+tVGo4HlR2+y7dwdEycrhMiNDadDGLVYLbD61i/Ll71q5k2BFR+lzmB6eQtY2MLgxeDf3fT7EULkSqGawr1Tp0506pT96ZA9PDxwcXExfUKFWfhF2PwZXNqk3rZ1hTZjoP5LoCtULwshCjYbJ+g7X11b7sxK8H0W6j2fo6YalHflleYV+Hl3EKNXnmbLuyVwsZNhvEKY28bAUN5edByDUaF33TJM7VMrbwqs2PvwZx+4fQysnWDwUvBpYvr9CCFyrVh8mq5Tpw4JCQnUqFGD8ePH06xZswxjExISSEhISLkdFRUFgF6vR68373UQyfvPVR6x99Hunob22K9ojEkoWguMDV7B2PwDsHUBo6IOFyyGTNK/IkPFun9L1Ubbagy6HRNRNnxEkld9cKuUo6ZGtqnItnNhXL0bw+erT/NdP3VSjGLdv/lA+jdvFeb+3XYujBGLT5JkVOhey4sve/pjNCTlZlLR9EWHYrGoH5rwcyi2riQNWqbOYpqFPivM/VsYSP/mvYLUx1nNQaMoORy7AsTHx2NjY5PTh+eKRqNh1apV9OzZM8OYCxcusHPnTho0aEBCQgK//PILf/zxBwcPHqRevXrpPmb8+PFMmDAhzfZFf/2GnZ1dOo/Qomh0j/JSMut4DYrGIoexSUBGf6qnx2qUJCrc3Unl0DVYGWIBCHGuy1mvfsTYlMowC0VjmcUcnow1AEbTxGKRMqNhXsWiGNCYLFanXoRcYGKNaMj4f3sFrTrJSYGJVdCQ8XpQ2YlN9f7Mq1ie9l7+L1Yx0uTKNDyiA4m09WZ3pc8wai2fiM3aMeLGQ5gZaIEeS4ZVNlC7pJLrY0TOYuUYkbNYOUbkLrbgHCPORGj47aIWg6KhrquRQX5GdJonYsniMSKTWNvEezS58g0OCXeIs3Rlv+9HRNuWydfPETmLlWNEzmLlGJG72Lw9RsTGxjLouReJjIzEyckpw0dmu8gyGo188cUXzJ07lzt37nDx4kUqVqzI559/Tvny5Xn55Zez01yOZaXISk+rVq0oV64cf/zxR7r3p3cmy9vbm/u7P8TJwTpNvOLgh+IzOOW29tyUDM8EKfblUcq/8Cj2/DfwX8GTJta2NErFVx7FXpyFMf4+V65ewbeiL1rdY5fTWbtj9HvzUezlOZDw3zTPCmjuX0ETtANN3H0waFCs6mFoNwmlQks0V39BE5fBYqk6O4xVP0i5qbn2O5qYa+nHai0xVvvkUez1hWgeXk4/FjBWH/soNngZmqiM1/8xVhsNWnVIlObWGjQRJzOOrfI+WNirsSHr0dw/knFspZFg5aLGhm5BCdubfv8CRt83wEa9lk8T9i+a8H8zbrfiy2BbRo29uw/Nna0Zx5Z/AezLq7H3D6MJ2ZBxbLmB4FhZvRFxAu2tvzOOLdsHnKurNyLPoL25IuPYMt3BpY56I/oi2huLM4xVvDqhuDZUb8RcQ3vt94xjS7VDcWuq3oi7BZd+zrB/FfdWKB6t1BvxYWivzM243ZJNUDzbqzcSI9BempVxrGsDFK/O6o2kGLQXvs041qU2Spke6g1jItpzX2Uc61QNxbtfym3tmYkZxz5+jIgOxWJJbTRJMRhLN0DxfTZ1bDaOEVuCdLxxsCGu9pasf7sZrjdmc/XCsXT7N9NjxJMsXTBWHplyU44RLgAYbm3g2uGF6fcvcoxIltNjRFLUda5v/yzD/i2Ix4jgPWNZdzoUo6Lg5+5AR3+PlCGCOT5GkM7niNj7aE8vQZMYjWLpQVLv7VCivBqbxc8Rer2eK+vfpmoFz3T7V44Rj8Xm4Bih1+s5seFrGpbXp9+/yDEiWVE4RkQ9TMC1xbSnFlnZHi44efJkfvvtN77++mteffXRVKE1atRgxowZ+VZk5VSjRo3Ys2dPhvdbW1tjbZ22mNJpdei0urQP0FmA5WPfRmt1ZPith06XOlanAyWdNjOK/e9FpdVpU+ei06F7Mlarg5hwuLINHgSp2y3toWoAmrZ/YJH8+OTYDHJIt930aJ+Mtcg4FrIXa2EJOsust2uRjdjkPCwsMGTUv+nEPjXfnMRmpR9S2rU0S2yq1/vTYi0ei9VbZtq/qWIN2WhXyUa+mmzEGpSsx0LWY129oVp3OL0I7e0j4FoBSvo9Fpv1Y0Rbf0+qXnfkfGg0E9ad5/u6mfRvdt7LeRVbyI8Rmv+uVU23f5FjxGPBOTtGWGbevwXtGLHrfBgnA/8rsDwc6FTDC93j07Tn9BiREvvf54iHd+DUYtDHgl1JNM+8h6XHY0ONs/M5gkz6V44RqdvNyecIMunfJ2PlGJG12AJ6jMjob/ykbJ/J8vPz46effqJt27Y4Ojpy8uRJKlasyPnz52nSpAkPHjzITnM5ltMzWe3bt8fR0ZGVK1dmKT4qKgpnZ2ci74enX61qtKB9rFY1JGaWtDrVak5ijXr0+kQ2bNhAp06dsLSwzDg2OgR2ToFjv4NiBJ0VNH4Dmr+jLiz8RLuZznams8phbJK6b1PEai0fnWLPs1gDen18+v2bTixKJoPttRaPTrEXhFjFqPZFRjSP/aeXh7H6xLiM+zdV7FOuC8xW7GPvz7yKhae8l9OJ3fyZunioXUl4bSc4ev0Xm71jRGBILD1n7yXJqDCjb1W0N49n0L9pjycZvpezEwvF5hihT4xnw/p16fdvmnYLwPu+kB0j9IkJbFi/NuP+LUDHiD2X7vLyb4cwGhJpV60UswbWxfLJb9Zze4wAuHkYFg2E+EjwrAnPLQN79xx9jtDr9Wz45286dQrIoH/lGJGbWL1ez/p/1tK5U8f0+zdNuwXgfS/HiCzGpj1GREVF4ezqbvozWbdu3cLPzy/NdqPRmOcXoz18+JDLlx+dOg4KCuLEiRO4urpSrlw5PvnkE27dusXvv6unHmfMmEGFChWoXr068fHx/PLLL2zfvp3Nmzdnf+c6q9Rv6MzistNmVmktQfvfGGSt1aNvZB6XlAAHf4Jd0yBBnbAD/57QboL6jXlG7WYnhyzHZuOlVSBidaC1yrx/H48la99iFIhYjTbrr7W8jM1q/2o02Wi3AMRC9mPbT4Lr+yH0FKwZAc+vTv+btKe0W6OMMyOe9WPG1kuM/+cK7/tnoX8hD9/3RfgYodFl7fULBeN9XwiPEVnuXzO+7/ddvsvLvx0mIUmhXbWyzBxcD0uLLKyGk91jxNWdsGgw6GPAuwkMXqJOTpWLdhWNRdb6F+QYkZNYjS4b/VsA3vdyjMh5bBbjs71Olr+/P7t3706zffny5dStWze7zWXLkSNHqFu3bsp+3nvvPerWrcvYseq43JCQEG7cuJESn5iYyPvvv0/NmjVp1aoVJ0+eZOvWrbRt2zZP88x3igLn1sLsxrDlc7XA8qoNQ9dD/98zLrCEEOZhYQ19f1WH8Abtgr0zc9zU8DZ+VC/tREScniVXteRiLiMhRCYOXL3HsN8Ok5Bk5NmqHsx+ri5WWSmwsuvCBvirv1pgVWwDz69Mv8ASQhRo2T6TNXbsWF588UVu3bqF0Whk5cqVXLhwgd9//51169blRY4pWrdunekHiAULFqS6/dFHH/HRRx/laU55LiJYXSQYICkJ59hrEHJSHU8K8DBM/YB2/b/rzBw8od04qDUQtIVqrWkhihc3P+g8Dda8BdsnQ/kW4N0w281Y6rR827823b7fQ+ADLatPhNC/kU8eJCxE8XUo6D4v/XqYeL2RVpXd+fG5elhbZPEsQHacXg4rX1OHcVXtqq6xZ5H2OnEhRMGX7SKrR48erF27lokTJ2Jvb8/YsWOpV68ea9eupX379nmRY/EVEQw/1FeHAQKWQGuAC+nEWthA05HQbBRYO+RfjkKInKszGK5sh8DlsGIYvLFHvW4ym6p6OjGyjS/fbr3MpPXnaVHFAy9n2zxIWIji58i1+wz99RBxegMtKrnx0/P1sbHMgwLryK+w7l1AgVoDoMeP6kX3QohCKUenOlq0aMGWLVsICwsjNjaWPXv20KFDB1PnJmLvpRRYmfJrByOOwLOfSoElRGGi0UDX78DFByJuwNp3Mr8oPBOvNC+Pj4NCdHwSH684LcMGhTCBo9cf8OL8Q8QmGmju58bPLzTImwJr3/ew7h1AgQYvQ8+5UmAJUcjJeLKi4NnPwcXb3FkIIXLCxlkdEqS1gDMr4cRfOWrGQqdlsK8BKwstuy6Gs/hwsIkTFaJ4OX5DLbBiEg00qVgybwosRYEdX6ozjgI0ewe6fCvD/YUoArL9LtZqteh0ugx/hBBCZFPZBtDmU/X39R/C3Us5asbTDt5rp87+OnndWW4+SH+RUiFE5k7djOCF+Yd4mJBE4wquzBvaAFurPCiwNo2Bf6eqt9uOhfYTHk3zLYQo1LJ9LnrVqlWpbuv1eo4fP85vv/3GhAkTTJaYEEIUK83eUadtDvoXlr8Er2zL0QXvQ5v4sPVcOEeuP+Cj5af48+XGaLXyoU2IrAq8FcmQXw4SHZ9Eo/KuzB/aEDsrEw/dMxpg7Sg4/od6u9M0aPyaafchhDCrHE188aS+fftSvXp1lixZwssvv2ySxIQQoljRaqHXTzC3GYSehq3jIWBKtpvRaTV80682ATN3se/KPf48eJ0XmpQ3ebpCFEVnbkfy3C8HiYpPooFPCea/1BB7axMXWEmJsOo1OLNKXSeox2x1EhwhRJFiskG/zzzzDNu2bTNVc0IIUfw4eUHPOervB36Ei5ty1Ex5N3tGB1QFYMr681y7G2OqDIUoss6FRDHkl4NExumpW86FX19qiIOpCyx9HCx5Ti2wtJbQb4EUWEIUUSYpsuLi4pg1axZlypQxRXNCCFF8Ve4Ijd9Uf1/9JkSH5qiZF5qUp0nFksTpDXy4/CRGo8w2KERGLoRG89wvB3kQq6e2twu/DWuEo42laXeSEA1/9oVLm8HCFgYtBv+0o4OEEEVDtousEiVK4OrqmvJTokQJHB0dmT9/PtOmTcuLHIsvu5JPvybDwlqNE0IUHe0ngGdNdRmHla+B0ZjtJrRaDV/3rYW9lY7D1x4wf29QHiQqROF38U40g38+wP2YRGqVdeb3YY1wMnWBFXsffu8B1/eAlSM8vxIqtTPtPoQQBUq2z4NPnz4dzWMz32i1Wtzd3WncuDElSpQwaXLFnos3jDiqftAC9ElJ7N27l2bNmmFp8d+fzq6kTN8uRFFjYQ195sP/WqkTYeybCc3fzXYz3q52fNrFnzGrTjNt0wXaVPXA113W0hMi2eUwtcC6F5NIjTJO/DGsMc62Ji6wou/AH70g7AzYusKQFVCmnmn3IYQocLJdZA0dOjQP0hAZcvF+VETp9UTa3QKv2mBp4v8EhBAFi3tl6PQ1/D0Ctk+G8i3Uqd6zaVAjbzYEhrD70l3eX3qS5W80wUIna/AIcSX8IYN+Psjdh4n4eznx58uNcbYz8f+tEcHqGaz7V8DBE15YDR7VTLsPIUSBlKUi69SpU1lusFatWjlORgghxGPqDoEr29VFipcPgzd2q4sXZ4NGo2Fqn1p0nL6LE8ER/Lw7iDdb++ZRwkIUDkF3Yxj0vwOERydQ1dORP19pjIudlWl3cveyWmBF3QSXcvDCGnCtaNp9CCEKrCwVWXXq1EGj0aAomV84rdFoMBgMJklMCCGKPY0Guk6HW0cg4jqsew/6/JLtxUpLu9gytps/Hy4/xfQtF3m2qgdVPB3zKGkhCrZr/xVYYdEJVCnlyF+vNMbV3sQFVmgg/NETYsLBrTI8vxqcZXIwIYqTLBVZQUFywbQQQpiFrQv0mQfzAyBwOfi1zdGUz33rl2VjYCjbzofx/rITrHqrGZYybFAUMzfuxTLo5wOERsVTycOBv15tTEmH7C/6nambR+DP3hAfqU5gM2QVOLibdh9CiAIvS0WWj49PXuchhBAiI96NoM0Y2D4J/vkAyjYCN79sNaHRaJjSuybtp+8i8FYUc3ZeYWTbSnmUsBAFT/B9tcAKiYzH192eha8+g5upC6ygXbBwIOhj1Pfpc8vUL0qEEMVOjlfZO3v2LDdu3CAxMTHV9u7du+c6KSGEEE9o/i5c3QnXdsPyl+CVrU9f4uEJHk42TOxRnVGLTzBr2yXaVvOgeunsXeMlRGF084FaYN2KiKOimz2LXn0Gd0cTF1gXNsLSF8CQABVawcCFYC2zeQpRXGW7yLp69Sq9evXi9OnTqa7TSp7WXa7JEkKIPKDVQe//wZxmEHoKtk2Ejl9ku5nutUuz4XQoG8+E8v7Sk/w9ojlWFjJsUBRdtyPiGPTzAW4+iKN8STsWvvoMHk42pt1J4Ir/1rRLgipdoO98sDTxPoQQhUq2/2cdNWoUFSpUICwsDDs7O86cOcOuXbto0KABO3fuzIMUhRBCAOBUGnr+qP6+/we4tCXbTWg0Gib3qoGrvRXnQ6OZte2SiZMUouAIjYxn0M8HCL4fh09JOxa99gyeziYufo7+BstfVgusmv2g/29SYAkhsl9k7d+/n4kTJ+Lm5oZWq0Wr1dK8eXOmTJnCyJEj8yJHIYQQyap0gkavq7+vekNd6DSb3BysmdyzBgBz/r3CyeAIEyYoRMFwJ0otsK7fi8Xb1ZZFrz6Dl7OtaXeyfzasHQkoUP8l6PU/0Mk6lkKIHBRZBoMBR0d16l83Nzdu374NqJNjXLhwwbTZCSGESKv9RChVA2LvwqrXwWjMdhOda3rRrXZpDEaF95edJF4vQ71F0RH2X4EVdDeGMi5qgVXaxYQFlqLAzq9g0xj1dtOR6nILWhl6K4RQZftoUKNGDU6ePAlA48aN+frrr9m7dy8TJ06kYkVZZE8IIfKcpY16zYeFLVzdAfu/z1EzE7tXx93RmsthD5m+5aKJkxTCPMKjExj08wGuhqsF1uLXnqFsCTvT7UBRYPNnsHOKevvZz9QvPrK5fp0QomjLdpH12WefYfzvW9OJEycSFBREixYtWL9+PbNmzTJ5gkIIIdLhXgU6TVV/3zYRze1j2W6ihL0VX/aqCcD/dl/l6PX7psxQiHx392ECg38+wJXwGLycbVj06jN4u5qwwDIa1OGB+39QbwdMhZYfSoElhEgjy0VWgwYNmDt3Lk2aNKF3794A+Pn5cf78ee7evUtYWBjPPvtsniUqhBDiCfVeAP+eYExCt/p1LAxx2W6ivX8petcrg6LAB8tOEZcowwZF4XQ/JpEhvxzkUthDSjlZs+jVZyhX0oQFlkEPK16BY7+DRgs9ZsMzb5iufSFEkZLlIqt27dp89NFHeHl58cILL6SaSdDV1TVlCnchhBD5RKOBbjPBuRyaB0HUCv4tR82M61YdTycbgu7G8PWm8yZOUoi89yAmkcE/H+B8aDQejmqBVd7N3nQ70MfBkiFwZiVoLdXhunWHmK59IUSRk+Uia968eYSGhjJ79mxu3LhB27Zt8fPz48svv+TWrVt5maMQQoiM2LpAn19QNDq8H+xDc3pptptwtrXkqz7qsMFf917jwNV7Jk5SiLwTEZvIc78c5HxoNG4O1ix89RkquptwEeCEaPirH1zcCBY2MGgRVO9luvaFEEVStq7JsrOzY+jQoezcuZOLFy8ycOBAfvrpJ8qXL0+XLl1YuXJlXuUphBAiI+UaY2z5EQC6jR/CvSvZbqJ1FQ8GNfIG4MPlJ4lJSDJpikLkhchYPUPmHeRsSBRuDlYsfq0xfh4mLLBi78PvPeHabrByhCEroFJ707UvhCiycjzXqK+vL5MnT+batWssWrSIAwcO0K9fP1PmJoQQIouMTd/hrkNVNIkxsHwYJCVmu40xnatRxsWW4PtxTNlwLg+yFMJ0IuP0vDD/IIG3onC1t2Lhq8/g5+Fouh08DIPfusGtI/9v777Do6jeNo5/t6RCEggtIZTQq/QiHaSKDUVQVBClCaJiAUH9qdgLiIgoCCIqIEhTkCIovYVepdcACaGml0123j9W84oQSMJuNgn357pykd2cmb33ZJjkyZk5B3wKw5O/Qmhz5+1fRPK1W1rQYdWqVfTu3ZvevXuTlpZGv379nJVLRESywmxhW9lnMHwKQ8ROWPFOlnfh5+3Bpw/XAmDaplOsO3zBySFFnCM2ycaTUzaz63Q0hX09mNGvMZVLOLHAuhIO390N5/ZCwRLQezGE1Hfe/kUk38tykXX69Gnee+89KlasyF133cWJEyf46quviIiIYMKECa7IKCIimZDkGUjaPWMdDzaMgyN/ZHkfTSsWpVeTsgAMm7OLmCSbMyOK3LK45FSenLKZneFXKOTrwfS+d1I1yN95L3DxqKPAungEAsrAU0ugRHXn7V9EbguZLrJ+/vlnOnXqRLly5fj666/p3r07hw4dYvXq1fTq1QsfHyeupC4iItliVOkMDf++qmD+M45LnrLo1U5VKRPoy9noJN777S8nJxTJvvjkVHpP2cz2U1cI8PFgWp/GVC/pxALr3D6Y0gmiw6FIRXh6CRSp4Lz9i8htI9NF1hNPPIGPjw/z588nPDycDz74gIoVK7oym4iIZEeHd6F4DYg/7yi0/l5APrMKeFkZ1a02JhP8vPU0Kw9kvVATcbaElFSemrqFrScv4+dtZVqfxtQMCXDeC5zeBt91hvgoKHEHPLUUAko5b/8iclvJdJF1+vRp5s+fz7333ovZfEu3comIiCt5+DjW8bH6wNE/YdP4LO+iUblAnm5WDoDh83YTnaDLBsV9ElPSeHrqFjYfv4Sfl6PAuqOUEwus42vhh/sh6QqUagi9F0LBYs7bv4jcdjJdLRUvXtyVOURExJmKV4VOHzo+/2MknNme5V0M7ViF8kULcC4mmbcX7nNyQJHMSUxJo8/3W9h07BIFvaz80KcRtUsXct4LHFoG0x+GlDgo1xJ6/uKYTVBE5BZoSEpEJL+q3xuq3Q92G8zt41hUNQu8PSyM6l4bswnm7zjD7/siXZNTJANJtjT6/bCVDUcvUsDTwvdPN6RuGScWQHvnwcwekJoEle+Gx2aDlxPX2RKR25aKLBGR/Mpkgvu/gIDScOkYLB6a5V3UK1OY/i0dN/6/Pn8Pl+Kzvv6WSHYk2dLo/+M21h25gK+nhalPN6J+2UDnvcD2Hx1/fLCnQs2H4ZEfwcPbefsXkduaiiwRkfzMpzB0nQwmM+z6CXbNyvIuXmxficolCnIhLoX//brXBSFFrpacmsbAadtYc+g8Ph4WvuvdkIahTiywNn4FCwaDYYd6T8JD34DFw3n7F5HbXpaLrC1bthAWFnbN82FhYWzdutUpoURExInK3AmtRzg+X/SSYx2gLPCyWhjdrQ4Ws4lFuyP4bfdZF4QUcUhOTWPQtO2sPHgebw8zU3o3pHH5Is7ZuWHA6k/g97//PzQZDPeNBbPFOfsXEflblousZ599lvDw8GueP3PmDM8++6xTQomIiJO1eBnKNnPc3D+3D6Rm7bK/O0oF8Gxrx2WD//tlL+djk12RUm5zKal2Bs/YwZ8HovCymvn2yYY0qeDEAmv5/2Dl+47HbV6HDu85LqsVEXGyLBdZf/31F/Xq1bvm+bp16/LXX1q0UkQkVzJb4KFJjssHz+6AFe9meReD76pE9WB/LifYeH3+HgzDcEFQuV2l2WHIz7tZ/tc5PK1mJj/ZgGYVizpn5/Y0+G0IbBjneNzxQ2g1TAWWiLhMlossLy8vzp07d83zERERWK1Wp4QSEREXCAiB+790fL7hCzjyZ5Y297SaGd29Nh4WE8v+OscvO8+4IKTcjmxpdn44bGb5/ig8LWa+6VmfFpWctE5Vmg3m9YdtUwET3D8Omgxyzr5FRDKQ5SKrQ4cOjBgxgujo6PTnrly5wmuvvUb79u2dGk5ERJys2r3QsK/j8/nPQFxU1jYP9ueFtpUAeOvXfURGJzk7odxmUtPsvDJnDzsvmfGwmJjYsz6tqzhpbU5bEszqCXvngNkKD38L9Xo5Z98iIjeQ5SJr1KhRhIeHU7ZsWdq0aUObNm0oV64ckZGRjB492hUZRUTEmTq8B8WrQ3wU/DIQ7PYsbf5MqwrUKhVATFIqI+bt1mWDkm1pdoOXft7F4r3nsJgMvuxRhzZVnVRgJcfBjG5waAlYveHRGVCzq3P2LSJyE1kuskJCQti9ezeffPIJ1atXp379+owdO5Y9e/ZQunRpV2QUERFn8vCBh6c4fvE88gds+ipLm1stZkZ3q42n1czKg+eZvfW0i4JKfpZmN3hl9i4W7DqL1Wziqcp27qripEsEEy/Dj13g+BrwLAiPz4HKHZ2zbxGRTMjWTVQFChSgf//+zs4iIiI5pXg16PQh/PYi/PE2hDaDknUzvXmlEn683L4yHy45wDu//UWzSkUJKeTjurySr6TZDYbN2c38HWewmE183r0WaSe3OWfncefhxwfh3B7wLgRPzINS9Z2zbxGRTMpUkbVgwQLuvvtuPDw8WLBgwQ3b3n///U4JJiIiLlb/KTi6AvYvhDlPw4A14OWX6c37tijP7/si2X7qCq/O2c2PfRph0mxtchN2u8GIebuZu/00FrOJcT3q0r5qURafdMLOo0/DDw/AxSNQoDj0+gVK1HDCjkVEsiZTRVaXLl2IjIykePHidOnSJcN2JpOJtLQ0Z2UTERFXMpngvi/gzA64dAwWD4MHv8705haziVHdatP5i7WsO3KB6WGneOLOsi4MLHmd3W7w+i97+Hnracwm+PyROnS+IxibzXbrO794FH7oAtGnIKA09PoVilS49f2KiGRDpu7JstvtFC9ePP3zjD5UYImI5DG+gdB1EpjMsGsG7P45S5uXL1aQYR2rAvDB4v2cupjgipSSDxiGwf9+3ctPm8Mxm2DMI3W4r3ZJ5+z83F/w3d2OAiuwAjy1RAWWiLhVlia+sNlstG3blsOHD7sqj4iI5LSyTaHVq47Pf3vJMaqVBb2bhtK4XCAJKWkMnbMLu12zDcrVDMPgzV/3MT3sFCYTjO5emwfqhDhn52e2wdTOEHcOStSEp5dCIU3EJSLulaUiy8PDg927d7sqi4iIuEuLV6BMU0iJhTl9IDUl05uazSY+fbg2vp4Wwo5fYuqGE67LKXmOYRiMXPgXP246ickEnz5cmwfrlnLOzk+sh+8fcMwmGNIAev8GBZ00BbyIyC3I8hTuTzzxBN9++60rsoiIiLtYrPDQN47Z2M5uh5XvZ2nzMkV8GdG5GgCf/H6AY+fjXBBS8hrDMHhv0f70wvvjh2rxcH0nFViHl8O0hxx/GAht4Zjkwqewc/YtInKLsjyFe2pqKlOmTOGPP/6gfv36FChQ4Kqvf/bZZ04LJyIiOahQabh/HPzcE9Z/DuVbQYW7Mr35E43L8PveSNYducArs3cx+5mmWMyabfB2ZRgGHy45wLfrjgPw4UN30L2hky7j2zcf5vYDuw0qdYTu3zvWfxMRySWyPJK1d+9e6tWrh5+fH4cOHWLHjh1XfYiISB5W/X5o8LTj8/nPONYcyiSTycTHD9fCz8vK9lNXmLw2a/d2Sf5hGAYfLz3IN2scx8B7XWrSo1EZ5+x8xzTHkgN2G9R4CB6drgJLRHKdLI9krVy50hU5REQkt+j4AZzcCOf3wy8D4bGfwZy5v8mFFPLhf/dWZ9jc3Yxefoi7qhanUonMr70leZ9hGIxadpAJq48C8M4DNZw3tf+mCbD070la6vWCez8Hs8U5+xYRcaIsj2Q9/fTTxMbGXvN8fHw8Tz/9tFNCZWTNmjXcd999lCxZEpPJxC+//HLTbVatWkW9evXw8vKiYsWKTJ061aUZRUTyPA8feHgKWL3hyHIIm5Clzbs1KEWbKsVISbXz8uxdpKbZXRRUcqMxfxxm/EpHgfX2fdXp1ST01ndqGLDm0/8vsO581rHGmwosEcmlslxkff/99yQmJl7zfGJiIj/88INTQmUkPj6e2rVrM378+Ey1P378OPfccw9t2rRh586dDBkyhL59+/L777+7NKeISJ5Xojp0/Hvyi+Vvwtmdmd7UZDLxUdda+Htb2X06On1EQ/K/sX8c5os/Hcu8vHFPNXo3K3frOzUMxzG44j3H49YjHMemSff7iUjulenLBWNiYjAMA8MwiI2NxdvbO/1raWlpLF68OH3BYle5++67ufvuuzPdfsKECZQrV47Ro0cDUK1aNdatW8eYMWPo2LGjq2KKiOQPDfrA0ZVw4DeY2wf6rwavgpnatIS/NyMfqMGLs3Yx9s/D3FW1BNVL+rs4sLjTlysOM+aPQwC81rkqfVuUv/Wd2u2w+GXYOsXxuMP70HTwre9XRMTFMl1kFSpUCJPJhMlkonLlytd83WQyMXLkSKeGu1UbN26kXbt2Vz3XsWNHhgwZkuE2ycnJJCcnpz+OiYkBHAsx22w2l+TMrH9e39058iv1r2upf13LZf3beQzWM9sxXTyCfdFQ0u77ItOb3lOjOIuqFuOPA+d56eedzB3QGE9rli+gyBXWn17P2JixBJwOoFmpZu6Ok+tMXHOcUcsdI1ivtK/EU03KZOlYvO7xm2bD8ttzmPfOwcBEWufRGHV7gc4hWabzr2upf10vN/VxZjOYDMMwMtNw9erVGIbBXXfdxdy5cwkMDEz/mqenJ2XLlqVkyZLZS5sNJpOJ+fPn06VLlwzbVK5cmaeeeooRI0akP7d48WLuueceEhIS8PG5djait99++7rF4owZM/D19XVKdhGRvKRI7AGaHfkQEwZbyw7kTGCTTG8bkwIf7bIQn2qiYyk7nUvnvfuzDMNgQtwEzqSdIcQSwjMFn8GkS9XSrThr4teTjnuj7imdRodSmfq14obM9hQanPiK4Ojt2LGwPXQAZwrfecv7FRG5VQkJCTz22GNER0fj75/xFRqZHslq1aoV4LjPqUyZMvn2B8yIESN46aWX0h/HxMRQunRpOnTocMOOzAk2m43ly5fTvn17PDw83JolP1L/upb617Vc27+dsa9OwbJuFPUjp1H7nj5QODTTW/tXiOSFn3fzx1kLz9zblJoheeuywQ1nN3Bm1RkAzqSdoXDdwjQt2dTNqXKH7zac5NeNBwF44a4KDG5TIVv7uer4NVKwzOmFOXo7hsULe9cp1K7UkdrODH6b0fnXtdS/rpeb+vifq9xuJstTuJctW5a1a9cyceJEjh07xuzZswkJCeHHH3+kXLlyNG/ePMthXSUoKIhz585d9dy5c+fw9/e/7igWgJeXF15eXtc87+Hh4fZv6j9yU5b8SP3rWupf13JZ/7YZASfXYQrfhMevz8DTS8GSudd5oF5plh04z6LdEbw6fy8Ln2uOlzVvzApnGAZf7/kaEyYMDEyY+GzHZzQv3RzLbT6z3ZR1x/lgiaPAer5tJV5sf+2tBFnlkZaAx8+PQXgYeBTA9NhMrOVa3vJ+xUHnX9dS/7pebujjzL5+li+Onzt3Lh07dsTHx4ft27en378UHR3NBx98kNXduVSTJk34888/r3pu+fLlNGmS+UtdREQEsFih6yTwDoAzW2Fl1s737z5Qk6IFPTl0Lo4xf9+7kxdsOLuBfRf3YeC4BM7A4Fj0MZrPbM7r615nyfElRCdHuzllzvth4wne+e0vAAa3qciL7Srd8j49bTFYp3VxFFjeAdDrV1CBJSJ5VJaLrPfee48JEyYwadKkqyq5Zs2asX37dqeG+6+4uDh27tzJzp07Acelizt37uTUqVOA41K/Xr16pbd/5plnOHbsGMOGDePAgQN89dVX/Pzzz7z44osuzSkiki8VKuNYmwhg3Rg4tirTmwYW8OT9B+8A4Js1R9l+6rILAjqXYRi8u+nd634tzhbHgqMLGLZmGC1ntaTn4p5M3DWRfRf3YTfy3n1nWTFt00ne/HUfAANbV+DlDpVv/RaCmLM0P/w+pnN7oEAx6L0YSjd0QloREffIcpF18OBBWra89i9LAQEBXLlyxRmZMrR161bq1q1L3bp1AXjppZeoW7cub775JgARERHpBRdAuXLlWLRoEcuXL6d27dqMHj2ayZMna/p2EZHsqtEF6vcGDJg3AOIvZHrTjjWCeLBuCHYDXpm9iyRbmqtSOsW4HeM4E3cmw693KNuBioUqYjfs7Dy/ky93fsmjvz3KXT/fxevrXmfp8aX5bpRrRtgp3vhlLwADWpZnWMcqt15gXTqG9Yd78UuOwPAPgaeWQlBNJ6QVEXGfLN+TFRQUxJEjRwgNDb3q+XXr1lG+vBPWxLiB1q1bc6PJEKdOnXrdbXbs2OHCVCIit5mOH8LJjXDhIPwyCB6blemFYd++rwbrj1zg2Pl4Pv39IP+7t7qLw2bPqlOrmLRnUoZfN2HiTNwZ5t0/j3MJ51h7Zi3rTq9jU8QmLiZdZMHRBSw4ugCzyUytorVoUaoFzUOaUzWwKmZT3pzGftaWU7w2fw8AfZqXY/jdVW+9wIraDz90wRQXSZxXCbx6LcKjqBMWMBYRcbMsn+n79evHCy+8QFhYGCaTibNnzzJ9+nReeeUVBg4c6IqMIiKSm3j6wsNTwOIFh3+HsImZ3jTA14OPu9YCYMr642w+fslVKbNtU8QmXlr10g3bGBhExkdis9sIKhBEt8rdGHvXWNY9uo5vO3xL7xq9rxrlGrdjHI/89gh3/XwXb6x7g6Un8tYo1+yt4Qyf5yiwnmoWyhv3VLv1AuvsDviuM8RFYhSvzrpKr0NAKSekFRFxvyyPZA0fPhy73U7btm1JSEigZcuWeHl58corr/Dcc8+5IqOIiOQ2QTWh4/uw+BVY/j8o2xSCa2Vq0zZVi9O9QSl+3nqaoXN2seSFFvh6ZvnHkUvsiNrB8yuex2bYaBLchMF1B2M1W0lNTWX9uvU0a94Mq9WRNdA7EE+L51Xbe1g8aBTciEbBjXi5wctExEU4RrnO/P8o169Hf+XXo79iMVmoVawWLUL+f5QrNy6PMm/7aYbN3Y1hwJNNyvLmvdVvPefJDTC9O6TEQkh9Uh+ZSfLKjc4JLCKSC2T5p5rJZOL1119n6NChHDlyhLi4OKpXr07BggVdkU9ERHKrhn3h6Ao4uBjmPA0DVoNngUxt+sa91Vl3+AInLybw0ZIDvPOA++/B2XdxH4P+GERiaiJNSzZl3F3j0osom83GcetxqgVWy9L0wcEFg+lepTvdq3QnJS2F7VHbWXd6HevOrONo9FF2RO1gR9QOvtjxBUV9itI8pDnNQ5rTpGQT/D3dv57YrzvP8MrsXRgGPHFnGd6+v8atF1iH/4BZT0BqIoS2gB4/gdnbOYFFRHKJbP/p0NPTk+rVc+e19CIikgNMJnhgPHzdDC4ehiWvwgNfZmpTf28PPnm4Nk98G8YPG0/SsUYQzSoWdXHgjB2+fJgBywcQZ4ujXvF6fN7m82tGqW6Vp8WTO4Pv5M7gO3ml4SucjTvLujPrWHtmLWERYVxIvMAvR37hlyO/YDFZqF2sdvq9XFUKO2GCiSxasOssL87aid2AHo1K8879NW89w1+/wpw+YLdBpQ7Q/Qfw8AGbzTmhRURyiUwXWU8//XSm2k2ZMiXbYUREJI/xDYSHvoHv74MdP0KFNlCza6Y2bV6pKI83LsP0sFMMm7ObpUNa4Oed84tMnow5Sb9l/YhOjuaOoncwvu14fKzXX7DemUoWLHnNKNfa045LC49FH2N71Ha2R21n7PaxFPMpRrOQZrQIacGdJe90+SjXot0R6QXWIw1K836XOzCbb7HA2jkDfn0WDDtU7wIPTQKrcwtZEZHcItNF1tSpUylbtix169a94Qx/IiJymynXAlq+Ams+hYVDIKQ+FA7N1Kavda7GmsPnCb+UyAeL9/PhQ5m7r8tZzsadpe+yvlxMukjlwpX5ut3XFPTM+cvf/z3KNbThUM7EnWH9mfWsPb2WsMgwzieev+4oV4uQFlQu7IR1qv5l6d4Inp+5gzS7wcP1S/HhQ04osMK+gSVDHZ/XfcKx3prZcuthRURyqUwXWQMHDuSnn37i+PHjPPXUUzzxxBMEBga6MpuIiOQVrYbDsdVwejPM7QtPLQHLzUelCnhZ+fTh2jz6zSZ+2hxOxxpBtK5SPAcCQ1RCFH2X9SUyPpJQ/1C+af8NAV4BOfLaNxNSMOSqUa5t57alX1p4PPr4VaNcxX2K0yykWfq9XH6eftl+3d/3RTJ4hqPAeqhuCB93rXXrBdba0fDnO47PGw+Ejh+AOW9OYy8iklmZPsuNHz+eiIgIhg0bxsKFCyldujTdu3fn999/18iWiMjtzmKFrpPBKwBOb4FVH2Z60zvLF+GpZqEADJ+7h+hE19+fcynpEv2X9Sc8NpyQgiFM7jCZIj5FXP662eFp8aRJySYMbTiUBV0WsLTrUt5o/AatSrXCx+pDVGIU84/M5+XVL9NyZkt6L+3N5D2TOXjpYJZ+Pv/x1zkGz9hOqt3ggTol+bRbbSy3UmAZBix/6/8LrFavQqcPVWCJyG0hS2c6Ly8vevTowfLly/nrr7+oUaMGgwYNIjQ0lLi4OFdlFBGRvKBwWbh/rOPztZ85RrYyaVjHqpQrWoDImCTeWfiXiwI6xKTE8MzyZzgafZTivsWZ3GEyJQqUcOlrOlNIwRAeqfoIX7b9krWPrmVi+4n0rN6TUP9QUo1Utp3bxtjtY3l44cO0m9OOtza8xR8n/yAuJeOf0ysOnGPg9G3Y0gzuq12S0bdaYNntsOhlWP+543H7d6HNa5letFpEJK/L9uyCZrMZk8mEYRikpaU5M5OIiORVNR50TOu+/QeY1x8GboACNx8h8vG0MKpbLbpN2Mjc7afpVDOI9tWdX/gk2BIY+MdA9l/aT6B3IJM7TKaUX95dANfL4kXTkk1pWrIpwxoO43TsadadcUwRHxYRRlRCFPMOz2Pe4XlYTVbqFK+TPk38P/dyrToYxTM/bseWZnDPHcGM6V4bq+UWRpvSUh0TXOyeCZjg3jHQ4CmnvWcRkbwgS0VWcnIy8+bNY8qUKaxbt457772XL7/8kk6dOmHW8L+IiAB0+hhOhcGFg/DrIOgxM1MjGPXLBtKvRXkmrjnGiHl7aFC2MIULOG/2uaTUJJ5b8Ry7z+/G39Ofb9p/Q7mAck7bf25Qyq8Uj1Z9lEerPkpyWjLbIrelL4Z8IuYEW89tZeu5rXy+/XOK+xanYsEGrN5ZhBSjAp1qhPL5o3VurcBKTXasmXbgNzBZHDNP3vGw896giEgekekia9CgQcycOZPSpUvz9NNP89NPP1G0qPvWNBERkVzK0xce/hYmtYVDS2HzN9B4QKY2fbF9Zf48EMWRqDjeXLCPcT3qOiWSLc3GS6teYnPkZgp4FGBCuwlUCazilH3nVl4WL5qGNKVpSFNe5VXCY8PTR7k2R2wmKiGKqITFeJQETywkFavLj/sd63JVKlQp6zMWpsTDzMfh2EqweEG3qVC1s0vem4hIbpfpImvChAmUKVOG8uXLs3r1alavvv619vPmzXNaOBERyaOC7oAO7zmm7V72BpRt6njuJrw9LIzuVpuHvt7Awl1nubtmEJ3vCL6lKKn2VF5d+yprz6zF2+LNl3d9yR3Fbp4lvyntV5oeVXvQo2oPVh8+yzNzfsbufQD/wCMkcY5t57ay7dxWxmwbQwnfEjQPaZ6+LlcBjwI33nniFZjxCIRvAo8C0GMGlG+dE29LRCRXynSR1atXrxxfbV5ERPKwRv0c92cdWuK4hKz/KvC8yS/rQO3ShRjYqgJfrjzCG7/spVG5QIoW9MpWBLth5831b7L85HI8zB6MbTOWBkENsrWv/GLTsYs888NuEm2VuCu4GV8/Xo+ohLPplxVuidzCuYRzzD08l7mH52I1W6lXvF76vVwVC1W8+veB+Avw44MQuRu8A+DxOVC6kfveoIhILpClxYhFREQyzWSCB8bDhGZw4RAsHQ73j8vUps+3rcQf+89xIDKWN+bv5esn6mX5D32GYfDepvdYeGwhFpOFUa1G0TSkaXbeSb6x+fglnp66hURbGq0qF+Orx+vhZbVQ2r80j/k/xmPVHiMpNYmt57amX1p4MuYkmyM3szlyM59t+4ygAkHpBdedBcpS4Kceju+vb1HoOR+Cc3ZBaRGR3CjbswuKiIjcVIEijskPvr/fMeNg+TZQ86GbbuZpNTO6e20e+HI9S/dFsmDXWR6oE5LplzUMg1FbRzH70GxMmPig+QfcVeauW3kned7WE5d46rvNJKSk0aJSUSb2rI+3h+Wadt5W7/QiCuBUzKmrRrki4yOZc2gOcw7NwWpAfWsSzYuXovndX1Ah6A50zYuISBbXyRIREcmyci2hxcuOzxcOgcsnM7VZjZIBPHdXJQDe/HUfUTFJmX7Jr3Z9xQ9//QDAyKYj6Vz+9p6AYfupy/T+bgvxKWk0q1iESb0aXLfAup4y/mV4vNrjfN3ua9Y9uo6v2n7FY2XvpkyqQaoJwny8GV3AzINrhtBxbkfe2fgOK06tIMGW4OJ3JSKSe6nIEhER12s9HEo1guRomNvXsZZSJgxqU4E7QgKITrQxYt4eDMO46TZT9k5hwq4JAAxvNJwHKz14S9Hzup3hV3jy283EJafSpHwRJvdqmOkC67+8rd60MPsxYvMcFoWH81uCL8NrDaJZSDO8LF5ExEcw+9BsXlj5As1mNqPvsr58v+97jl45mqnvnYhIfqHLBUVExPUsHtB1MkxoDqc3w+qP4K43brqZh8XMqG61uW/cOv48EMWcbafp1qB0hu1/OvATY7aNAeCFei/weLXHnfYW8qLdp6/Q89swYpNTaVQukG97N8DHM3sFFgAnN8KM7pAcAyXrUfaJuZT1DeRxIDE1ka2RW9MvLQyPDScsIoywiDBGbR1FyQIl0y9DbBzcGF8PX6e9TxGR3EZFloiI5IzCZeG+zx0zDa4ZBeVaQbkWN92sSpAfQ9pX4pOlB3ln4V80q1iUkoV8rmn3y5Ff+CDsAwD63dGPvnf0dfY7yFP2nonmiclhxCal0jC0MN/1boiv5y382D/yp2MdrNREKNvMsci0t3/6l32sPrQo1YIWpRzf05MxJ1l7+v/v5Tobf5afD/3Mz4d+xsPsQb0S9WgR0oIWIS0o5VvqVt+uiEiuoiJLRERyTs2ucHQl7PgR5vWDZ9Y7Jse4if4tyrNs3zl2hl/h1bm7+eHpRlfNNrj0+FLe2vAWAE9Ue4Ln6j7nsreQF/x1NoYnvg0jJimV+mUL891TjSjgdQs/8vcvdBTHaSlQsT10/8Gx6PQNlPUvS9nqZXmi+hMkpiayJXJLetF1Ou70VaNcwQWCKW0rTYHTBWhaqqlGuUQkz1ORJSIiOevuj+HUJrh4GBYMhkdnOKZ7vwGrxTHbYOexa1l7+AI/bQ7nscZlAFgVvooRa0dgN+x0rdSVYQ2H3dbrOu6PiOHxyZu4kmCjTulCTH2qIQVvpcDaNRN+GQRGGlR/AB6aDFbPLO3Cx+pDy1ItaVmqJYZhcDLmJOvOrGPtmbVsjdxKRHwEEUSwec1mPMwe1C9RnxYhLWheqjnl/Mvd1t9PEcmbVGSJiEjO8iwAD0+ByW3h4GLYMtmxcPFNVChWkKEdq/Deov28v+gvWlQqyumkXby86mVSjVQ6l+vM/+783239C/nByFgenxzG5QQbtUsF8EOfRvh5e2R/h5snweJXHJ/XeRzu+wIst/arg8lkIjQglNCAUJ6o/gQJtgQ2ndnEjI0zOO1xmjPxZ9gUsYlNEZv4dOunhBQMoXlIc1qEtKBhUEONcolInqAiS0REcl5wLWj/Lix9FX5/Hco0gaCaN93sqWbl+H1fJFtOXObZuXOJ8PmCFHsKbcu05f3m72Mx38KkDnnc4XOxPDZpE5fiU7gjJIAf+jTG/1YKrLWfwZ8jHZ83fgY6fghm509K7OvhS4uQFsT6xnL33XdzJvGMY5Tr9Fq2ntvKmbgzzDo4i1kHZ+Fp9nSMcpVqQfOQ5oT6h97WRbWI5F4qskRExD0aD4CjK+Dw7477ffqvuul9PhaziVHdatPp6xkcs0zElJZMs5LN+KTlJ1jNt++PtCNRcfSYFMbF+BRqlPTnxz6NCPDJZoFlGPDnO7DuM8fjlkOhzes3vaTTGUwmE+UCylEuoBw9q/ckwZbguJfr7xkLz8SdYWPERjZGbOSTLZ+kj3K1LNWShkEN8bFeOyGKiIg73L4/kURExL1MJujyFXzdDC4chN9HwH1jb7pZsvkMBct+R5I9GSOxPENqvY+nJWv3COUnx87H8dikTVyIS6ZasD/T+jSmkG82+8NuhyXDYMskx+N2I6H5EKdlzSpfD19alW5Fq9KtMAyD4zHHWXfacS/XtnPbrhnlahDUIP3SwrL+ZTXKJSJuoyJLRETcp0BReGgi/NAFtk2F8m2gRpcMm5+IPkH/Zf1JssfiYy9H1KkneWPeQWYNaILFfPv9Qn38Qjw9Jm0iKjaZqkF+TO/bmMIFsllgpaU6JiLZ9RNggntGQ8M+Ts17K0wmE+UDylM+oDy9avQiwZbA5sjN6ZcWno0/y4azG9hwdgOfbPmEUgVLOQquUi00yiUiOU5FloiIuFf51tD8RcflaQufh5B6UKjMNc3OxJ2h77K+XEy6SJXCVXin0Zd0O76TrScv89364/RtUT7ns7vRyYvx9PhmE+dikqlcoiDT+zYmMLsFVmoyzO3jmKrdZIEHJ0Ct7s4N7GS+Hr60Lt2a1qVbO0a5oo+nX1a49dxWTsedZubBmcw8OBNPsycNgxqmF11l/cu6O76I5HMqskRExP3avAbH18CZrTC3H/RedNUsdlEJUfT9vS/nEs5RLqAcE9tPpIhPEd64tzoj5u3hk98P0rpKcSoWL+jGN5Fzwi8l0OObTUTGJFGxeEGm972TIgW9srezlASY9QQc/RMsntBtKlS9x6l5Xc1kMlG+UHnKFyrPkzWeJMGWQFhEGOvOrGPdmXWcjT/L+rPrWX92PR9v+ZjSfqVpHtKc5iHNNcolIi6hIktERNzP4gEPfwsTWkD4JljziaPwAi4lXaLfsn6cjjtNqYKlmNR+EkV8HAsYP9qwNEv3RrL60Hlenr2Luc80wWpx/gx4uUn4pQQe/WYTZ6OTKF+sADP6NaaYXzYLrKRomPEInNoIHr6ONcsqtHFuYDfw9fClTZk2tCnTBsMwOBZ9LH1drm3nthEeG85PB37ipwM/4WXxokFQA8e6XCHNNcolIk6hIktERHKHwqFw7xjHZWtrPoVyLYkOvoMBywdwLPoYJXxLMLnjZEoUKJG+iclk4qOud9BhzBp2hV9h4ppjPNumovveg4uduZJIj0mbOHMlkXJFC/BTvzsp7uedvZ3FX4RpD0LELvAKgMdnQ5nGzg2cC5hMJioUqkCFQhV4ssaTxNvirxrlioiPYP2Z9aw/sx6AMn5lrhrl8rZms39F5LamIktERHKPOx6Goyth5zTi5/VnUKU7OHDpAIHegUzqMImQgiHXbBIc4MNb99Xgldm7+PyPQ7StVpyqQf5uCO9aEdGJ9PhmE6cvJxJaxJef+t1JCf9sFgAxEfBjFzh/AHyLQM/5EFzbqXlzqwIeBbirzF3cVeYuDMPg6JWj6QXXtqhtnIo9xYwDM5hxYAZeFq//v5crpAVl/K+9V1BE5HpUZImISO5y98ckhW/kOY9Ydl/6C39Pf75p/w3lAspluEnXeiEs3RvBH/ujePnnXfzybDM88tFlg5HRSfT4ZhOnLiVQJtCXn/rfSVBANgusyyfghwcc//qVhF6/QrHKzoybZ5hMJioWrkjFwhXpXbM38bZ4NkVsSi+6IuMj0z//iI8o6182fZSrQYkGGuUSkQypyBIRkVwlxerJkHLV2HJxNwXsdiaG3E2VwCo33MZkMvHBQ3ewdcwa9p2NYfzKIwxplz8Kh6iYJB6btIkTFxMoVdiHn/rfSXBANidqOH/QUWDFRjguz+y1AArrHqR/FPAoQNsybWlbpm36KNc/MxZuP7edkzEnORlzkun7p+Nt8b5qlKu0f2l3xxeRXERFloiI5Bqp9lReXfMq6y/uxttkZXzkGWqe/gKqPAglatxw2+J+3rzzQE2e/2kHX644QrtqJagZEpBDyV0jKjaJRydt4tiFeEIK+fBTvzsJKZTNAitiF/z4ICRchGJVoecv4B/s1Lz5yb9HuZ6q+RRxKXGERYSlF13nEs6x9sxa1p5Zy4d8SKh/6P+PcgU1wMuSzclIRCRfUJElIiK5gt2w88b6N/jj1B94mD0Ye9eX1F81Bg4vgzl9oN8K8PS94T7uqxXMkj0RLNkbySuzd/Hr4GZ4WS059A6c60JcMo9NCuPY+XhKBngzs/+dlA688fvP0KlNML07JEdDcB14Yh4UKOLUvPldQc+CtC3blrZlHaNcR64cSS+4dpzbwYmYE5yIOcG0/dPwtnjTKLhRetFV2k+jXCK3GxVZIiLidoZh8O6md1l0bBFWk5XRrUbTNKQpPFAJJjSD8/th2euO2QdvwGQy8V6Xmmw+fokDkbF88edhhnasmkPvwnkuxiXz2KRNHImKIzjAm59upcA6ugJmPg62BCjTFB6bBd75b2KQnGQymahUuBKVClfi6ZpPXzXKtfbMWqISolhzeg1rTq8BSB/lahHSgvpB9TXKJXIbUJElIiJuZRgGn279lDmH5mDCxIctPqRNmb/XaipYDB6c6LjMbesUKN8Gqt9/w/0VKejF+w/W5Jlp2/l61VHaVw+iTulCrn8jTnIpPoXHJ4dx6FwcJfy9mNHvTsoWKZC9ne3/DeY8BWkpULEddP/xpqOBknX/HeU6fOUwa087Rrl2Ru28apTLx+pDw6CG6etylfIr5e74IuICKrJERMStxu8cz49//QjAyKYj6VSu09UNKrSBZi/A+s9hwWAoWRcK3fjyq041g3mgTkl+3XmWl3/eyaLnW+DtkfsvG7z8d4F1IDKWYn6OAqtc0WwWWLtmwS8DwUiDavdD12/B6uncwHINk8lE5cKVqVy4Mn3u6ENsSuz/38t1eh1RideOcrUo1SJ9xkJPi75HIvmBiiwREXGbb/d8y8TdEwEY0WgED1Z68PoN73oDTqyFM9tgXn94ciFYbvwjbOT9Ndhw9CJHz8fz2fJDvNa5mrPjO1V0go0nvg1jf0QMRQt68VO/O6lQrGD2drblW1j0MmBA7cfg/nE37S9xDT9PP9qVbUe7su0wDINDlw+x7sw61p5Z+/+jXH+d4Me/fsTH6kOjoEaOUa5Sza+7LpyI5A0644qIiFvM2D+Dz7d/DsCQekN4rNpjGTe2eEDXyTChJZzaAGtHQevhN9x/IV9PPnroDvp8v5VJa4/RoXoJGoQGOvEdOE90oqPA2nc2hqIFPfmpX2MqFs9mgbXuc/jjLcfnjfpDp4/BnH/WDMvLTCYTVQKrUCWwSvoo1z/rcq09vZbziedZfXo1q0+vhjAoF1Au/bLC+iXqa5RLJA9RkSUiIjlu/uH5fLj5QwD61+pPnzv63HyjwPKOiS/m9YXVH0O5llC26Q03aVutBA/XL8Wcbad5ZfYuFr/QAl/P3PWjLybJRq9vw9hzJprAAp5M73snlUr4ZX1HhgEr3nMUoAAtXoa7/gcmk3MDi9P4efrRvmx72pdtnz7KtfbMWtaeXsuu87s4Hn2c49HH+eGvH/Cx+tA4qHH6pYUlC5Z0d3wRuYHc9ZNGRETyvSXHl/DWBsdIS8/qPRlcZ3DmN67VzTFb3q4ZMLcfPLMWfG88OvW/e6uz/sgFTlxM4JOlB3n7/huvt5WTYpNsPDllM7tOR1PY14PpfRtTJSgbBZbdDkuHw2bHpZe0exuav+jUrOJa/x7l6ntHX2JSYth01jHKte7MOs4nnmfV6VWsOr0KgPIB5R0zFpZqQb3i9TTKJZLLqMgSEZEcs/LUSl5b+xoGBg9XfpihDYZiyupIS+dPIDwMLh2FBc/BI9NuOFoT4OPBR11r8eSUzUzdcIKONYJoUsH9a0TFJafS+7st7Dh1hUK+Hkzr25hqwdmYWj0tFRY+DzunOx53HgWN+jk3rOQ4f09/OoR2oENoBwzD4ODlg+mXFe46v4tj0cc4Fn3s/0e5ghunX1r431GusMgwxsaMpUhkEZqXbu6mdyRye1GRJSIiOWLD2Q28vPplUo1U7i1/L/+7839ZL7AAvPzg4W9hcns48JtjaveGN77csFXlYvRoVIafNp9i6JxdLB3SkoJe7vsRGJ+cylPfbWbbycv4e1uZ1qcxNUoGZH1HqSmOyyf/+hVMFujyFdR+1PmBxa1MJhNVA6tSNbBq+ijXxrMb00e5LiReYFX4KlaFrwKgQkCF9FGuusXqMm7nOM7bzzNu5zialWqWvf93IpIlKrJERMTltp3bxgsrXsBmt9G2TFvebfYuZtMtTMZQsq7jkrhlr8Pvr0GZJlCi+g03ef2eaqw5dJ7TlxP5YPF+Pnjwjuy//i1ISEnlqalb2HLiMn7eVqb1bUzNkGwUWCkJ8HNPOPIHWDzh4SlQ7T7nB5Zcx9/Tn46hHekY2hG7YefgpYPpBdeu87s4Gn2Uo9FH+f6v7/Eye5FsTwbgr0t/MXXfVJqWbIqfpx/+nv4U8CigokvEBVRkiYiIS+29sJdn/3yWpLQkmoU045OWn2A1O+HHz52D4NhKR5Ex52novxI8fDJsXtDLyqfdavHYpDBmhJ2iU40gWlYudus5siAxJY0+U7ey+fgl/Lys/NinMbVKFcr6jpJiYMYjjpkWrT7w6HSo2NbpeSX3M5vMVCtSjWpFqtGvVj+ik6PZGLGRdacdRdfFpItXtf9s22d8tu2z9McWkwU/T7/0osvf0x9/L/+rHvt5+uHv5f//X//X8xZz7l9/TsQdVGSJiIjLHLx0kAHLBxBvi6dhUEM+b/25827QN5uhy9fwdTM4vx9+fx3u/eyGmzStUJQnm5Tl+40neXXubn5/sSX+3h7OyXMTSbY0+v6whY3HLlLQy8r3fRpRp3ShrO8o4RJMewjO7gAvf3jsZyjbxOl5JW8K8AqgU2gnOoV2Yu3ptQz6c9A1bfw8/EhKS8Jmt5FmpHEl+QpXkq9k6/UKeBS4ujjz+P+CLL1Q+2+B9ncR52XxusV3K5J7qcgSERGXOBF9gv7L+xOTEkOtYrUYd9c4vK3ezn2RgsXhoYnw44Ow9Vuo0Oaml8y9endVVh86z4mLCby78C8+7VbbuZmuI8mWRr8ftrL+yEUKeFr4/umG1CtTOOs7io2EH7o4ikrfIvDEPChZx9lxJR8wDIPxO8djNpmxG/b0580mM2X8y/DTPT+RnJZMTEoMMckxxNpiiUmOcTz+5+Pvx7EpsenPxaY42iWkJgAQb4sn3hZPRHxEljN6WbyuO1p2o1G1f57ztfrqMkfJ1VRkiYiI052JO0PfZX25lHSJqoFV+artVxTwKOCaF6twFzR7AdaPhV8HO+7XCiiVYXNfTyufdqtN94kbmb3tNJ1qBtG2WgnXZMNRYA34cRtrD1/A19PC1KcbUb9sNhZFvnwSfngALh8Hv2Do9SsUq+L8wJIvbDi7gX0X913zvN2ws+/iPjac3UCzkGZ4W70p7ls8y/u32W3EpsSmF11XFWH/KtD+XbD987XYlFgMDJLTkjmfeJ7zieez/PpWk/XqyxyvM3r232LN38Pxb0GPgrrMUVxORVYeoilYRSQvOBd/jr6/9+VcwjnKB5RnYvuJBHhlY2KHrGjzBhxfC2e3O9bP6v0b3OCXqIahgfRpVo7J644zYt4elr1YmEK+zl9nKDk1jYHTtrH60Hl8PCxM6d2QhqHZKLDOH3IUWLFnoVBZeHIBFA51el7JHwzDYNyOcZgwYWBc83UTJsbtGEfTkk2zPRrkYfYg0DuQQO+sH892w068Lf76o2X/Lc7+NXr2z+NUeyqpRiqXky9zOflytvIX9Ch4/eLsOveg/fex1iSTzFCRlUcYhqEpWEUk17uYeJF+y/txOu40pQqWYlKHSdn6JSzLrJ6Oad0ntHRMBrFmFLR+9YabvNKxCisPRnH0fDxvL9jH54/WdWqklFQ7z07fzsqD5/H2MPNt7wbcWT4b63NF7IIfH4KEC1C0CvT6BfxL3nQzuX3Z7DYi4yOvW2ABGBhExkdis9vcUjCYTeb0UaiQgiFZ2tYwDJLSkq57KeO/R8uuV7DFpsSSmJoIQJwtjjhbHGfjz2Y5v7fF+6rRsoLWgsTGx7J3614K+RS6ujj7zz1pPlYf/f52m1CRlUdsOLuBvy79BTimYP1nmF9EJLeITo5mwPIBHI8+TgnfEkzuODlblyFlW2B5x8QX8/rB6o+gXMsbTgjh7WFhVLfadP16A7/sPEunmsF0qhnklCi2NDuDZ2znj/1ReFnNfPtkQ5pWKJr1HYVvhmkPQ3I0BNeGJ+ZDAfcvpCy5m6fFk5n3zuRS0iUAUlNTWb9uPc2aN8NqdfzqF+gdmCdHZEwmEz5WH3ysPpQokPXLfG1ptgwvbbzevWf/LtLiUuIwcBR5SYlJRCVGXbXvnYd23vT1rSbr9WdvzOCSx38/9vP0u7WlLyRH5bkia/z48Xz66adERkZSu3Ztxo0bR6NGja7bdurUqTz11FNXPefl5UVSUlJORHUawzD4ZMsnVz33+bbPb2mYX0TEmeJt8Qz6YxAHLx+kiHcRJneYnOW/UDtFre5wdAXs+gnm9oWB68An4wkm6pYpzDOtKvDVqqO8Pn8PDUMLU6Tgrc14Zkuz89yMHSz76xyeVjOTejWgWcVsFFjHVsFPj4Et3rEO2GOzwNvFl11KvhFUIIigAo4/GthsNo5bj1MtsBoeHjkzm2Zu5WHxoIhPEYr4ZP2PFWn2NOJscdcUaFcSr7B592ZKli9JfGr8tcXa3+1SDcdljpeSLqUXwFlhwuS4zDGDSxlvNmGIh+X2/t7ntDxVZM2aNYuXXnqJCRMm0LhxYz7//HM6duzIwYMHKV78+n8t9ff35+DBg+mP82JRsuHsBo5FH7vquQOXDzB131SeqvlUBluJiOSMxNREnv3zWXZf2E2AVwDfdPiG0IBQ9wXq/CmEh8GlY7Dgeej+A9zg3P9Cu0r8uT+Kg+diefPXfYx/vF62Xzo1zc6QmTtZui8ST4uZb3rWz95aXAcWw+wnIS3FMbHHI9PB0zfbuUTk1lnMFgK8Aq65x9Rms+F1yIvOdTpnWMQahkFiamKmZm683uOktCQMDGJtscTaYjnDmSzn97H6XHemxmuKs+tMw+/uyxzz4rwEearI+uyzz+jXr1/66NSECRNYtGgRU6ZMYfjw4dfdxmQyERTknMs/3OGfm1f/OwUrOBYUjEmOYVDdQXiY9dcJEcl5KWkpvLjqRbad20ZBj4JMbDeRyoUruzeUlx88PAUmt4f9C2DbVGiQ8R+kvKwWRnevTZfx61m0J4JOu85yX+2s3/OUmmbnxZ93sWhPBB4WExN61qN1lWxcLrl7NswfAEaaYzr6rt+CVesJieRlJpMJXw9ffD1800cYsyIlLeWG955lNHoWkxJDnC0OcPxBLDE1kaiEqJu82rWsZuvVC1F7+aXP1njDUbW/Z3O8lcsc8+q8BHmmyEpJSWHbtm2MGDEi/Tmz2Uy7du3YuHFjhtvFxcVRtmxZ7HY79erV44MPPqBGjRoZtk9OTiY5OTn9cUxMDOD4K4XNZnPCO8majKZg/cfkvZMJiwjjg2YfuOfSnHzkn++vO77PtwP1r2u5o39T7am8uu5V1p9Zj7fFm7GtxlI5oHLu+B4Xq4m5zRtY/nwLY+lwUks2gGJVM2xepbgvA1uVY9zKY/zvl73UL+1PMb//L2xu1r9pdoOhc/ewcHckHhYT4x6tTYsKgVnuC9P277EseQUTBvY7upN27xdgmCE39KkL6fzgWupf18qJ/jVhIsAaQIA1ALI4qH3VZY622GsKsowe/9M2zUgj1e6Eyxz/c3/ZP/+mF2Qe/3ns6U9Bz4Jsidxy1bwEa06toWnJplnO4SyZ/T6bDMO4/tQzuczZs2cJCQlhw4YNNGny/zcyDxs2jNWrVxMWFnbNNhs3buTw4cPUqlWL6OhoRo0axZo1a9i3bx+lSl1/DZW3336bkSNHXvP8jBkz8PXN2Us1DMNgQtwEzqadzXCGoH+mZ/XCi/t976e2p+sX1RQRsRt25iTMYbdtN1asPFHgCSp6VHR3rKsZdu48OpoSsXuI9i7NmipvYTdnfKN/mh1G77FwJsFEzcJ2+lax3+gqw3R2A2YcMbPlghmzyeCpynZqBWb9R2uFc4upeXYmAMeLtmV3qZ6gm9xFxI0MwyCFFBKNRJKMJBKNRBLtiVc9/u+///48ldRbzvDvpQhMmChpKckzBZ9x22hWQkICjz32GNHR0fj7+2fYLs+MZGVHkyZNrirImjZtSrVq1Zg4cSLvvvvudbcZMWIEL730UvrjmJgYSpcuTYcOHW7Yka6QkpbCmF/HYKRl/MO6kFchSvuVZveF3cxOmE1iiURebfCq6xb9zMdsNhvLly+nffv2t/2Nwa6g/nWtnOxfwzB4b/N77D66G6vJyqctPqVVqVYufc1si2uEMbkVAfHhdLZuxN7p4xs2r9wglgcnbGLvZTO2kFp0qeO4bDCj/rXbDUb8so8tF85iMZv4vHttOtXI4oxnhoF5zUdY/i6w0pq+QKnWb1AqD1wO4yw6P7iW+te11L8ZS05LvmoR6uv++6/RtX//G58aD3DVQIOBwZm0MxSuW9hto1n/XOV2M3mmyCpatCgWi4Vz585d9fy5c+cyfc+Vh4cHdevW5ciRIxm28fLywsvr2mvfPTw8cvw/joeHB7PunXXTKViL+hTlm93fMHH3RH47/hu7L+zm45YfU7NozRzNm1+443t9O1H/upar+/ef2U7nH52P2WTmw5Yf0i60ncte75YVDoEHJ8C0rli2fYulUluoek+GzWuWDmRIu8p8+vtB3ll0gBaVSxAU4J3+9X/3r91u8Mave5i3w1FgffFoXe6pFZy1fHY7/P4ahH3teNz2TSwtXibjZZTzN50fXEv961rq32t5eHhQ0LsgwWTx3Ihjuv3HFj3GoSuHrpqXwGwy8/Wer2lZpqVbRrMy+z3OM9cheHp6Ur9+ff7888/05+x2O3/++edVo1U3kpaWxp49ewgOzvo32l2CCgRRvUh1qhepTrXAapS0lqRaYLX054IKBGE1WxlUZxBTOk4hqEAQp2JP0XNxT6bsnXLNZBkiIrdi3I5xTNs/DYCRTUfSKbSTmxNlQsV20PQ5x+e/PgvRN56Va0DL8tQuXYjYpFRenbub611Vb7cbvP7LXmZtDcdsgs8fqZONAisNFj73/wVW51HQ4uWs7UNEJJ/aHLmZA5cPXPO7rN2ws+/iPjac3eCmZJmTZ4osgJdeeolJkybx/fffs3//fgYOHEh8fHz6bIO9evW6amKMd955h2XLlnHs2DG2b9/OE088wcmTJ+nbt6+73oJL1S9Rnzn3zaF92fakGqmM2TaGAcsHcD7hvLujiUg+MHnPZCbtmQTAa41fo0vFLu4NlBV3vQnBdSDxMszr7yhwMmC1mBndrRaeVjOrD53np82nCDt+iW0XTIQdv0Rqmp03F+zlp82nMJtgzCN1sj4bYWoKzHkadkxz3HfV5Wto1O/W3qOISD7xz+zaJq4/UmXCxLgd4677R7DcIs9cLgjwyCOPcP78ed58800iIyOpU6cOS5cupUQJx/Xvp06dwmz+/7rx8uXL9OvXj8jISAoXLkz9+vXZsGED1atXd9dbcLkArwBGtxrN/CPz+WjzR2yK2ETXBV15t9m7tCqdS++ZEJFcb/r+6YzdPhaAF+u/SI+qPdycKIusno5p3Se2hJPrYO1n0Gpohs0rFvfjlQ6V+WDxAV6fv/fvOwIs/HB4K76eFhJS0jCZYFS32jxQJ4szu9oSYVZPOLIczB7w8LdQ/YFbeXciIvmKzW4jMj4yw4nfDAwi4yOx2W14WjKe0Mid8lSRBTB48GAGDx583a+tWrXqqsdjxoxhzJgxOZAqdzGZTDxU6SHqFK/Dq2te5cClAwxeMZjHqj7GSw1ewsui9VZEJPPmH3b80QbgmdrP8HTNp92cKJuKVIB7RjvWoFr1IZRrAWXuzLB5qUKOGWX/+yM+IcUxCtbzzrI8VO/6M9VmKCkGfurhKPSsPvDoNMfljCIiks7T4snMe2fedF6C3FpgQR67XFCypnxAeaZ3nk7P6j0BmHFgBj0W9eDolaNuTiYiecXiY4t5a8NbAPSq3otBtQe5OdEtqv0o1HrEsdDv3L6QeOW6zdLsBu8u+uuGu1r+1znS7Fm4VCXhEvzwgKPA8vKHnvNUYImIZCAz8xLkZnluJMtdUtJSSElLueZ5s8mM1Wy9ql1GTJjwsHhkq60tzUZKWgo2u+Nfw2zcsO2/h1eH1BtCwxINeXPDmxy6dIhHfnuEYQ2H0a1yN1LtqRkOxQJX/YXgv/u9UdtUe+oNJ93ISlsPs0f67DGuaptmT8uwf6/XNs24wf0cZmv6yua5oa3dsJNqz3idCovJgsVscXnbG/Xvv9sahoHNnvFCf1lp++//n65qCzf+v5yT54iM+vdm54iM2q44tYIRa0eQZqTxcKWHeb7u89f0S548R3QeBeGbSb18DPuCwdD1W/67INamoxc5G52Yfj+AQRpw9X7PRttYfySSOysUufk5IjYSpneD8/uxegdi7jUfStbVOeJfbTM6fv/bVueI7J0jUo3UDPs3u+eIm7WFPHqOyM7vEUZahv17Tdtc8P9e54jsnyNu9P/u31RkZdLoDaPxKnDtZXaVAivxeK3H0x9/uv7TDL9ZoYVC6V2nd/rjzzd9ToIt4bptS/qVpH/9/umPx28Zz8X4ixw+d5hd63dhsfz/BL/FfIvxbKNn0x9/s+2b60520aRIE9adXUdsWizvbnqXDWc3EOoTypWkK9fN4Ovhy7Bmw9IfT98znRNXTly3rYfZg9dbvp7+eNbeWRy+dPi6bQHebv12+ufz9s/jr/MZ/8X4tRavpZ9Mfzv0Gzsjd2bYdmjToRTwdKwR9vuR39lydkuGbYfcOYRC3oUA+PP4n6w9sfa6/QswqOEgihcoDsDaU2tZdWJVhvvtV68fIf6OezQ2nd7E8mPLM2zbu05vQguFArAtYhuLDy/OsO1jdzxG5SKVAdgTtYdfDvySYdtu1btRo3gNAPaf38/sv2Zn2LZL1S7UCaoDwJFLR5ixZ0aGbTtX6kyjkEYAnIo+xdSdUzNs2758e5qVaQZARGwEE7ZMyLB/W4e2pnVoawDOJ5znqy1fZbjfpqWb0qFCBwCik6P5fNPnGbZtWLIh91R2TNedYEvg0w2fZti2TlAdulTtAjiuBf9g7QcZtq1erDrda3RPf3yjtjl1jvh629dsObfluv2b2XMEQCHvQgy5cwgbzmzgldWvcCH2AuX9ykMKfLjuw6va5tlzhLc/PPwtv09uzZb9s+HnOChZ56p2ByJjMGiMiYIAJJl3kWzef83+vtq+hhVn/W98jkiKhl0/OSbd8CxIv+7fE1KyLqBzRPo5Ii6COefmXPf4BZ0j/nEr54jF5xezc/3O6/Zvds4R//hu53ecjT173bZ59hyRjd8jdsfuZsf6HdftX9DvEf/ID+eI5PjkDNv/my4XvI34evjSrXI3hjYYitVs5c9TfzJh9wTOxN14OmMRub1sjdzKCytfwGa3US2wGm3KtHHLWiQuFVIf6vw9eceRPyD+wlVfLuCZub9B3rRdwkXHDIKJl8E7AOo+AUUrZiexiIjkISYjN899mAvExMQQEBDA+Uvn8ff3v+brOXq5oC2FJUuWcPfdd1+1EFp2hvn3X9zPsDXDOB59HICnaz7NgNoD8DBfvcDabTPMb08jKSXpuv17vbbuHrrPi8P8icmJGfavLgVyuJVzREJSAouXLL5u/2blHLH3/F4GrRhEvC2e5iHNGd1yNFZLxoVEnj5HpKZgn/4wHF8FxWvA07+Dh2Ph4TS7QZtP13IuJhmDay8XNAEl/L1ZNbQNFrPp+ueIyL0w7WFIvABFKsETc8G/pM4R12mbnJLMwkULr3v8/retzhFZP0fYbDYWLFpAp06drtu/ulzw1trabDYWLlpIx04dM1yoVr9H3Frb3HSOiImJoVhgMaKjo69bG/xDlwtmkqfFM1MzmGRllpOstPWweIDd8Z/U0+J51Qnuum1volqRasy6dxYfb/mYeYfn8d2+79gWtY2PW3xMKb/rz5aVmf3+498/MPJCW4vZ4ujXTPSvxWzBwvUvB8iNbc0mc6aPNVe3zUz/mkymTO83N7QF1/2/z+o5IjP9+0/b6zl46SDPrXyOeFs8jYIaMab1GLyt3lnKkFm54f+91eoJD02Cr5tC1F+w4j3o/InjixZ4+/4aDJy2/e+7six/f5C+asvI+2vj43HtZeQWswVL+DaY/rDjUsGg2tBzPhQoev22OkdgNpkzffzmhv/3efEcYTVZM9W/kLX/y65qmyvOEVn5PcJkyXT/5ob/9zpHZL9tpjNnqpXkS74evoxsOpJPW32Kn4cfu8/vptvCbiw6tsjd0UQkhx2LPkb/5f2JSYmhdrHajLtrXJYKrDzLrwQ8OMHx+eaJcHBJ+pc61Qzm6yfqERRwdT8EBXjz9RP16FQz+Pr7PLYKfujiKLBK3wlPLrxugSUiIvmXiiyhU2gn5tw/h7rF6xJni2P42uG8vu514m3x7o4mIjngdOxp+i3rx6WkS1QLrMZX7b7C18PX3bFyTqX20OTv9Rd/GQQx/38Tf6eawax79S6mPd2AXpXSmPZ0A9a9elfGBdbBJTC9O9jioXwbxzTtPoVc/x5ERCRXUZElAJQsWJIpHacwqPYgzCYzC44uoPvC7uy9sNfd0UTEhSLjI+m7rC9RCVFUCKjAxPYT8ffM+BrzfKvtmxBcGxIvwbz+YP//exUsZhONywVSv6hB43KBWMwZTAKyZw7MegLSkqHqvfDYLPh7ljIREbm9qMiSdFazlYF1BvJdx+8ILhDMqdhT9Fzckyl7p9zwxk8RyZsuJl6k37J+nIk7Q2m/0kzqMInC3oXdHcs9rF7QdQp4FIATa2HdmKxtv22qY3Fje6pjseNu3zv2KSIityUVWXKNeiXqMfu+2XQo24FUI5Ux28YwYPmADNfMEJG8Jzo5mv7L+3Mi5gRBBYKY3GEyxXyLuTuWexWtCPeMcny+8gMI35y57TZ8CQtfAAxo8DR0mQA3mJFRRETyPxVZcl0BXgGMajWKkU1H4mP1YVPEJrou6Mrq8NXujiYityguJY6Bfwzk0OVDFPEuwuQOkylZsKS7Y+UOtXvAHd3ASIM5fSDxSsZtDQNWfgjL/l5AtdkLcM9nYNaPVhGR251+EkiGTCYTD1V6iJn3zqRqYFUuJ19m8IrBfBj2IclpmVvtWkRyl8TURAavGMyeC3so5FWISR0mUda/rLtj5R4mk6NQKlQWok/Bby86iqn/Mgz4/XVY/ZHj8V3/g3YjHduLiMhtT0WW3FT5gPJM7zydntV7AjDjwAx6LOrB0StH3ZxMRLIiJS2FISuHsO3cNgp6FGRC+wlUKlzJ3bFyH29/eHgKmCywb55jtCpiFwEJJyBiF5zZDrMeh03jHe3v/gRavqICS0RE0umicckUT4snwxoOo2nJpry+7nUOXz7MI789wrCGw+hWuVv6KuYikjvZ7DZeWf0KG85uwMfqw1ftvqJGkRrujpV7FSzhKJoMYM3HeKz5mNYAB//Trv170HhAjscTEZHcTSNZkiXNQ5oz9/65NAtpRnJaMu9uepchK4dwJemKu6OJSAbS7Gm8vu51VoavxNPsyRd3fUHd4nXdHSt3S7jomCnwZsq1cH0WERHJc1RkSZYV9SnKV22/YmiDoVjNVlaEr6Drwq5sidzi7mgi8h92w847m95hyfElWE1WxrQZw53Bd7o7loiISL6mIkuyxWwy06tGL2Z0nkGofyhRCVH0+b0PX2z/Apvd5u54IgIYhsEnWz5h3uF5mE1mPmr5ES1LtXR3LBERkXxPRZbckmpFqjHr3lk8VOkhDAwm7ZlE76W9CY8Nd3c0kdveuB3jmL5/OgDvNH2HjqEd3ZxIRETk9qAiS26Zr4cvI5uOZFSrUfh5+LH7/G66LezGomOL3B1N5LY1afckJu2ZBMDrjV/ngYoPuDmRiIjI7UNFljhNx9COzLl/DnWL1yXeFs/wtcN5fd3rxNvi3R1N5LYy7a9pfLHjCwBerv8yj1Z91M2JREREbi8qssSpShYsyZSOUxhUexBmk5kFRxfQfWF39l7Y6+5oIreF+Ufm8/GWjwEYWHsgvWv2dm8gERGR25CKLHE6q9nKwDoD+a7jdwQXCOZU7Cl6Lu7JlL1TsBt2d8cTybd2pezivc3vAdC7Rm8G1h7o5kR5mG8RsHrduI3Vy9FORETkP7QYsbhMvRL1mH3fbN7Z+A7LTi5jzLYxbDi7gQ+af0Bx3+LujieSr6wMX8nchLkYGDxS5RFeqv+SFgm/FYVKw+BtjvWyAFtqKuvXr6dZs2Z4WP/+0elbxNFORETkPzSSJS4V4BXAqFajGNl0JD5WH8Iiwnh4wcOsCl/l7mgi+cb6M+sZvn44duzcW+5eXmv8mgosZyhUGkrWcXwE1ybaNxSCa///cyqwREQkAyqyxOVMJhMPVXqIWffOompgVS4nX+a5Fc/xQdgHJKcluzueSJ62NXIrQ1YOwWa3UcOjBm82fhOzSad2ERERd9JPYskx5QLKMb3zdHpV7wXATwd+oseiHhy5fMTNyUTypj3n9/Dsn8+SlJZE85LN6ebbDatZV4GLiIi4m4osyVGeFk+GNhzK1+2+JtA7kMOXD/Pookf5+eDPGIbh7ngiecbBSwcZ8McAElITaBzUmE+af4LVpAJLREQkN1CRJW7RPKQ5c++fS7OQZiSnJfPupncZsnIIV5KuuDuaSK53LPoY/Zf3JzYlljrF6vDFXV/gbfV2dywRERH5m4oscZuiPkX5qu1XDG0wFKvZyorwFXRd2JUtkVvcHU0k1wqPDaff7/24lHSJaoHVGN9uPL4evu6OJSIiIv+iIkvcymwy06tGL2Z0nkGofyhRCVH0+b0PX2z/Apvd5u54IrlKZHwk/Zb1IyoxioqFKjKx/UT8Pf3dHUtERET+Q0WW5ArVilRj1r2zeKjSQxgYTNozid5LehMeG+7uaCK5woXEC/Rb1o8zcWco41eGb9p/Q2Hvwu6OJSIiItehIktyDV8PX0Y2HcmoVqPw8/Bj94XddFvYjUXHFrk7mohbRSdH0395f07EnCC4QDCTO0ymmG8xd8cSERGRDKjIklynY2hH5tw/h7rF6xJvi2f42uG8vu514m3x7o4mkuPiUuJ4ZvkzHL58mKI+RZncYTLBBYPdHUtERERuQEWW5EolC5ZkSscpDKo9CLPJzIKjC+i2sBt7L+x1dzSRHJNgS+DZP59l78W9FPIqxKT2kyjjX8bdsUREROQmVGRJrmU1WxlYZyDfdfyO4ALBhMeG03NxT77d8y12w+7ueCIulZKWwpCVQ9getR0/Dz8mtp9IxcIV3R1LREREMkFFluR69UrUY/Z9s+lQtgOpRiqfb/+c/sv7E5UQ5e5oIi5hs9t4ZfUrbIzYiI/Vh6/afUX1ItXdHUtEREQySUWW5AkBXgGMajWKkU1H4mP1ISwijK4LurIqfJW7o4k4VZo9jdfXvs7K8JV4mj0Zd9c46hSv4+5YIiIikgUqsiTPMJlMPFTpIWbdO4uqgVW5knyF51Y8xwdhH5CUmuTueCK3zG7YGblxJEtOLMFqtjKmzRgaBzd2dywRERHJIhVZkueUCyjH9M7T6VW9FwA/HfiJHot6cOTyETcnE8k+wzD4ePPHzD8yH7PJzMctPqZlqZbujiUiIiLZoCJL8iRPiydDGw7l63ZfE+gdyJErR3h00aPMOjALwzDcHU8ky77Y8QUzDswA4N1m79IhtIObE4mIiEh2qciSPK15SHPm3j+XZiHNSE5L5r2w9xiycghXkq64O5pIpn2z+xsm75kMwP/u/B/3V7jfzYlERETkVqjIkjyvqE9Rvmr7FUMbDMVqtrIifAVdF3Zlc8Rmd0cTuakf//qRcTvGAfBKg1foXqW7mxOJiIjIrVKRJfmC2WSmV41ezOg8g1D/UKISoui7rC9fbP8Cm93m7ngi1zXn0Bw+2fIJAIPqDOLJGk+6OZGIiIg4g4osyVeqFanGrHtn0bVSVwwMJu2ZRO8lvQmPDXd3NJGr/HbsN97Z+A4AT9V4imdqPePmRCIiIuIsKrIk3/H18OXtpm8zqtUo/Dz92H1hN90WduO3Y7+5O5oIAH+e/JM31r2BgcEjVR7hxfovYjKZ3B1LREREnERFluRbHUM7Mve+udQrXo94Wzwj1o7gtbWvEW+Ld3c0uY2tO7OOV9a8QpqRxv0V7ue1xq+pwBIREclnVGRJvhZcMJhvO37LoNqDMJvMLDy2kG4Lu7H3wl53R5Pb0JbILQxZOYRUeyodynZgZNORmE06DYuIiOQ3+uku+Z7VbGVgnYF81/E7ggsEEx4bTs/FPfl2z7fYDbu748ltYtf5XQz+czDJacm0KtWKj1p8hNVsdXcsERERcQEVWXLbqFeiHrPvm02Hsh1INVL5fPvn9F/en6iEKHdHk3zuwKUDDPxjIAmpCTQObszo1qPxsHi4O5aIiIi4iIosua0EeAUwqtUo3mn6Dj5WH8Iiwui6oCurwle5O5rkU8euHGPA8gHEpsRSt3hdvmjzBV4WL3fHEhERERdSkSW3HZPJxIOVHmTWvbOoFliNK8lXeG7Fc3y05SNshtbUEucJjwmn37J+XEq6RPUi1Rnfdjy+Hr7ujiUiIiIupiJLblvlAsoxrfM0elXvBcDPh39mQuwEjl456uZkkh9ExkfSd1lfohKjqFioIhPbTcTP08/dsURERCQH5Lkia/z48YSGhuLt7U3jxo3ZvHnzDdvPnj2bqlWr4u3tzR133MHixYtzKKnkBZ4WT4Y2HMrX7b4m0DuQc/ZzPPH7E8w6MAvDMNwdT/KoC4kX6LesH2fjz1LGrwyTOkyikHchd8cSERGRHJKniqxZs2bx0ksv8dZbb7F9+3Zq165Nx44diYq6/sQFGzZsoEePHvTp04cdO3bQpUsXunTpwt69mr5brtY8pDmz7p5FJWslktOSeS/sPYasHMKVpCvujiZ5zJWkK/Rb1o8TMScILhDM5A6TKepT1N2xREREJAflqSLrs88+o1+/fjz11FNUr16dCRMm4Ovry5QpU67bfuzYsXTq1ImhQ4dSrVo13n33XerVq8eXX36Zw8klLyjiU4SeBXrycr2X8TB7sCJ8BV0XdGVzxI1HS0X+EZsSyzN/PMORK0co5lOMyR0mE1ww2N2xREREJIflmUVaUlJS2LZtGyNGjEh/zmw2065dOzZu3HjdbTZu3MhLL7101XMdO3bkl19+yfB1kpOTSU5OTn8cExMDgM1mw2Zz76QI/7y+u3PkVzabDbPJTPcK3alfvD6vbXiNEzEn6LusL0/VeIoBdwzAw6xpt7Mrvx+/iamJPLvyWfZd3Echr0J8fdfXBPsE59j7ze/9627qX9dS/7qW+te11L+ul5v6OLMZTEYeufHk7NmzhISEsGHDBpo0aZL+/LBhw1i9ejVhYWHXbOPp6cn3339Pjx490p/76quvGDlyJOfOnbvu67z99tuMHDnymudnzJiBr69mBbudpBgpLEpcxLaUbQCUspSiu293Ai2Bbk4muY3NsDEtfhpHU4/ijTdPF3yaktaS7o4lIiIiTpaQkMBjjz1GdHQ0/v7+GbbLMyNZOWXEiBFXjX7FxMRQunRpOnTocMOOzAk2m43ly5fTvn17PDw0ouJs1+vfLnThj1N/8G7Yu5y2nWZi4kRGNBxB53Kd3Zw278mvx6/NbmPo2qEcjT6Kj9WHr9p8Re1itXM+Rz7t39xC/eta6l/XUv+6lvrX9XJTH/9zldvN5Jkiq2jRolgslmtGoM6dO0dQUNB1twkKCspSewAvLy+8vK5dKNTDw8Pt39R/5KYs+dF/+/fuCndTp0Qdhq8dzvao7byx8Q3CzoXx+p2vU8CjgBuT5k356fhNs6fx1oa3WHNmDV4WL8a3HU+DoAZuzZSf+jc3Uv+6lvrXtdS/rqX+db3c0MeZff08M/GFp6cn9evX588//0x/zm638+eff151+eC/NWnS5Kr2AMuXL8+wvUhGggsG823HbxlUZxBmk5mFxxbSbWE39pzf4+5o4iZ2w87bG99m6YmlWM1WPmv9GQ2DGro7loiIiOQCeabIAnjppZeYNGkS33//Pfv372fgwIHEx8fz1FNPAdCrV6+rJsZ44YUXWLp0KaNHj+bAgQO8/fbbbN26lcGDB7vrLUgeZjVbGVh7IFM7TSW4QDDhseH0WtKLyXsmYzfs7o4nOcgwDD7a/BG/HPkFs8nMJy0/oWWplu6OJSIiIrlEniqyHnnkEUaNGsWbb75JnTp12LlzJ0uXLqVEiRIAnDp1ioiIiPT2TZs2ZcaMGXzzzTfUrl2bOXPm8Msvv1CzZk13vQXJB+oWr8uc++fQMbQjqUYqY7ePpf+y/kQlXH+9NslfDMPg8+2f89OBnzBh4r1m79G+bHt3xxIREZFcJM/ck/WPwYMHZzgStWrVqmue69atG926dXNxKrnd+Hv682nLT2lWshkfbv6QsMgwui7oyrvN3qV16dbujicu9M3ub5iy17E23xt3vsF9Fe5zcyIRERHJbfLUSJZIbmIymXiw0oPMuncW1QKrcSX5Cs+teI73N71PUmqSu+OJC/yw7we+3OlYzHxog6F0r9LdzYlEREQkN1KRJXKLygWUY1rnaTxZ/UkAZh6cSY9FPThy+Yibk4kzzT40m0+3fgrAs3WepVeNXm5OJCIiIrmViiwRJ/C0ePJKw1eY0G4CRbyLcOTKER5d9CizDswij6z3LTew8OhC3t34LgBP13yaAbUGuDmRiIiI5GYqskScqFlIM+bcP4fmIc1JTkvmvbD3eGHlC1xJuuLuaJJNf5z8g/+t/x8GBj2q9mBIvSGYTCZ3xxIREZFcTEWWiJMV9SnK+LbjGdZwGB5mD1aGr6Trgq5sjtjs7miSRWtPr2XomqGkGWl0qdiF4Y2Gq8ASERGRm1KRJeICZpOZntV7Mr3zdEL9Q4lKjKLvsr6M3T4Wm93m7niSCVsit/DiqhdJtafSKbQTbzd5G7NJp0wRERG5Of3GIOJC1YpUY9a9s+haqSsGBpP3TKb3kt6Ex4a7O5rcwK7zu3j2z2dJTkumdanWfNDiAyxmi7tjiYiISB6hIkvExXw9fHm76duMbjUaP08/dl/YTbeF3fjt2G/ujibXsf/ifgYuH0hiaiJ3Bt/JqNaj8DB7uDuWiIiI5CEqskRySIfQDsy9by71itcj3hbPiLUjeG3ta8SlxLk7mvzt6JWjDFg+gFhbLPWK12Nsm7F4WbzcHUtERETyGBVZIjkouGAw33b8lkF1BmE2mVl4bCHdFnZjz/k97o522wuPCaffsn5cTr5MjSI1+LLtl/h6+Lo7loiIiORBKrJEcpjVbGVg7YFM7TSV4ALBnI47Ta8lvZi8ZzJ2w+7ueLeliLgI+i7ry/nE81QsVJEJ7Sbg5+nn7lgiIiKSR6nIEnGTusXrMuf+OXQM7UiqkcrY7WPpv6w/UQlR7o52W7mQeIF+y/txNv4sof6hTOowiULehdwdS0RERPIwFVkibuTv6c+nLT/lnabv4GP1ISwyjK4LurLy1Ep3R7stXEm6Qr9l/TgZc5KSBUoyqcMkivoUdXcsERERyeNUZIm4mclk4sFKDzLr3llUC6zGleQrPL/yed7f9D5JqUnujpdvxabEMuCPARy5coTiPsWZ3GEyQQWC3B1LRERE8gEVWSK5RLmAckzrPI0nqz8JwMyDM+mxqAdHLh9xc7L8J8GWwLN/PstfF/+isFdhJnWYRGn/0u6OJSIiIvmEiiyRXMTT4skrDV9hQrsJFPEuwpErR3h00aPMOjALwzDcHS9fSE5L5oWVL7Ajagd+nn580+Ebyhcq7+5YIiIiko+oyBLJhZqFNGPu/XNpHtKc5LRk3gt7jxdWvsCVpCvujpan2ew2Xl71MpsiNuFr9eXrdl9TNbCqu2OJiIhIPqMiSySXKuJThPFtxzOs4TA8zB6sDF9J1wVd2Ryx2d3R8qQ0exoj1o5g9enVeFm8+LLtl9QuVtvdsURERCQfUpElkouZTWZ6Vu/JjHtmEOofSlRiFH2X9WXs9rHY7DZ3x8sz7Iadtza8xe8nfsdqtvJ5m89pGNTQ3bFEREQkn1KRJZIHVA2syqx7Z9G1UlcMDCbvmUzvJb0Jjw13d7RczzAMPgz7kF+P/orFZGFUy1E0D2nu7lgiIiKSj6nIEskjfD18ebvp24xuNRo/Tz92X9hNt4Xd+O3Yb+6OlmsZhsGY7WOYeXAmJky81/w92pZt6+5YIiIiks+pyBLJYzqEdmDufXOpV7we8bZ4RqwdwWtrXyMuJc7d0XKdibsn8t3e7wB4s8mb3Fv+XjcnEhERkduBiiyRPCi4YDDfdvyWQXUGYTaZWXhsId0WdmPP+T3ujpZrfL/ve8bvHA/AsIbDeLjyw25OJCIiIrcLFVkieZTVbGVg7YFM7TSV4ALBnI47Ta8lvZi8ZzJ2w+7ueG7188GfGbV1FADP1X2OntV7ujmRiIiI3E5UZInkcXWL12XO/XPoGNqRVCOVsdvH0n9Zf6ISotwdzS0WHl3Ie5veA6BPzT70u6OfmxOJiIjI7UZFlkg+4O/pz6ctP+Wdpu/gY/UhLDKMrgu6svLUSndHy1HLTy7njfVvYGDwWNXHeKHeC5hMJnfHEhERkduMiiyRfMJkMvFgpQeZde8sqgVW40ryFZ5f+Tzvb3qfpNQkd8dzuTWn1zBszTDshp0HKz7Iq41eVYElIiIibqEiSySfKRdQjmmdp/Fk9ScBmHlwJj0W9eDw5cNuTuY6myM289Kql0i1p3J36N281eQtzCad3kRERMQ99FuISD7kafHklYavMKHdBIp4F+HIlSP0WNSDmQdmYhiGu+M51c6onQxeMZjktGRal27N+y3ex2K2uDuWiIiI3MZUZInkY81CmjH3/rk0D2lOcloy74e9zwsrX+By0mV3R3OKvy7+xaA/BpGYmkiT4CaMajUKD7OHu2OJiIjIbU5Flkg+V8SnCOPbjmdYw2F4mD1YGb6Shxc8zOaIze6OdkuOXD7CgOUDiLXFUq94PcbeNRYvi5e7Y4mIiIioyBK5HZhNZnpW78mMe2YQ6h9KVGIUfZf1Zez2sdjsNnfHy7JTMafov7w/V5KvULNITca3HY+P1cfdsUREREQAFVkit5WqgVWZde8sulbqioHB5D2TeXLJk4THhLs7WqZFxEXQd1lfzieep1LhSkxoP4GCngXdHUtEREQknYoskduMr4cvbzd9m9GtRuPn6ceeC3vo9ls3Fh5d6O5oN3U+4Tx9l/UlIj6CUP9Qvmn/DQFeAe6OJSIiInIVFVkit6kOoR2Ye99c6hWvR7wtntfWvcaItSOIS4lzd7Trupx0mf7L+3Mq9hQhBUOY1GESRX2KujuWiIiIyDVUZIncxoILBjOl4xQG1RmE2WTmt2O/0W1hN/ac3+PuaFeJTYllwPIBHLlyhOI+xZnUYRJBBYLcHUtERETkulRkidzmLGYLA2sPZGqnqZQsUJLTcafptaQXk/dMxm7Y3R2PBFsCg/4YxP5L+wn0DmRSx0mU9ivt7lgiIiIiGVKRJSIA1C1el9n3z6ZTaCdSjVTGbh9L/2X9ORd/zm2ZktOSeX7F8+w8vxM/Tz++af8N5QPKuy2PiIiISGaoyBKRdP6e/nzS8hPeafoOPlYfwiLDeHjhw6w8tTLHs9jSbLy06iXCIsPwtfoyod0EqgRWyfEcIiIiIlmlIktErmIymXiw0oP8fO/PVAusxpXkKzy/8nne2/QeSalJOZIh1Z7K8LXDWXN6Dd4Wb8a3HU+tYrVy5LVFREREbpWKLBG5rtCAUKZ1nsaT1Z8EYNbBWfRY1IPDlw+79HXthp23NrzFspPLsJqtfN7mcxoENXDpa4qIiIg4k4osEcmQp8WTVxq+woR2EyjiXYQjV47QY1EPZh6YiWEYTn89wzD4IOwDFhxdgMVkYVSrUTQLaeb01xERERFxJRVZInJTzUKaMff+uTQPaU5yWjLvh73P8yuf53LSZae9hmEYfLbtM2YdnIUJE+83f5+2Zdo6bf8iIiIiOUVFlohkShGfInzV9itebfgqHmYPVoWv4uEFDxMWEeaU/U/YNYGp+6YC8FaTt7in/D1O2a+IiIhITlORJSKZZjKZeKL6E8y4ZwblAsoRlRhFv2X9+Hzb59jstmzvd+reqXy16ysAXm34Kl0rd3VWZBEREZEcpyJLRLKsamBVZt4zk66VumJg8O3eb3lyyZOEx4RneV+zDsxi9LbRADxf93meqP6Es+OKiIiI5CgVWSKSLb4evrzd9G1GtxqNn6cfey7sodtv3Vh4dGGm97Hg6ALeC3sPgH539KNfrX6uiisiIiKSY1Rkicgt6RDagbn3zaVe8XrE2+J5bd1rjFg7griUuBtut+zEMv63/n8APF7tcZ6r+1xOxBURERFxORVZInLLggsGM6XjFJ6t8ywWk4Xfjv1Gt4Xd2H1+93Xbrzm9hlfXvIrdsPNQpYcY1nAYJpMph1OLiIiIuIaKLBFxCovZwjO1n2Fqp6mULFCS03GneXLJk0zeM5k0exphkWGMjRnL9399z4srXyTVSOXucnfz5p1vYjbpVCQiIiL5h9XdAUQkf6lTvA6z75/NuxvfZemJpYzdPpYNZzYQnRzNeft5vtj5BQYGbUq34f3m72MxW9wdWURERMSp9OdjEXE6f09/Pmn5Ce80fQcfqw9bzm3h0JVDABgYVAusxqhWo/Awe7g5qYiIiIjzqcgSEZcwmUw8WOlBZt0zC2+L9zVfV4ElIiIi+VWeKbIuXbrE448/jr+/P4UKFaJPnz7Exd149rLWrVtjMpmu+njmmWdyKLGIAJyNP0tSWtJVz+2/tJ8NZze4KZGIiIiIa+WZIuvxxx9n3759LF++nN9++401a9bQv3//m27Xr18/IiIi0j8++eSTHEgrIgCGYTBux7hrJrYwm8yM2zEOwzDclExERETEdfLExBf79+9n6dKlbNmyhQYNGgAwbtw4OnfuzKhRoyhZsmSG2/r6+hIUFJRTUUXkXzac3cC+i/uued5u2Nl3cR8bzm6gWUgzNyQTERERcZ08UWRt3LiRQoUKpRdYAO3atcNsNhMWFsaDDz6Y4bbTp09n2rRpBAUFcd999/G///0PX1/fDNsnJyeTnJyc/jgmJgYAm82GzWZzwrvJvn9e39058iv1r3MZhsEX27/AhAmDa0esTJj4YvsXNCzWUGtkOYGOX9dS/7qW+te11L+upf51vdzUx5nNkCeKrMjISIoXL37Vc1arlcDAQCIjIzPc7rHHHqNs2bKULFmS3bt38+qrr3Lw4EHmzZuX4TYffvghI0eOvOb5ZcuW3bA4y0nLly93d4R8Tf3rHKlGKqdiTl23wALHLIOnLp9i4eKFWE154lSUJ+j4dS31r2upf11L/eta6l/Xyw19nJCQkKl2bv3NZvjw4Xz88cc3bLN///5s7//f92zdcccdBAcH07ZtW44ePUqFChWuu82IESN46aWX0h/HxMRQunRpOnTogL+/f7azOIPNZmP58uW0b98eDw/NzOZs6l/naxTfiMvJlwFITU0lbFMYje9sjNXqOPUEegdSwreEOyPmGzp+XUv961rqX9dS/7qW+tf1clMf/3OV2824tch6+eWX6d279w3blC9fnqCgIKKioq56PjU1lUuXLmXpfqvGjRsDcOTIkQyLLC8vL7y8vK553sPDw+3f1H/kpiz5kfrXeUoXKk1pSgOOE2S4NZw7it+h/nUhHb+upf51LfWva6l/XUv963q5oY8z+/puLbKKFStGsWLFbtquSZMmXLlyhW3btlG/fn0AVqxYgd1uTy+cMmPnzp0ABAcHZyuviIiIiIjIzeSJKdyrVatGp06d6NevH5s3b2b9+vUMHjyYRx99NH1mwTNnzlC1alU2b94MwNGjR3n33XfZtm0bJ06cYMGCBfTq1YuWLVtSq1Ytd74dERERERHJx/JEkQWOWQKrVq1K27Zt6dy5M82bN+ebb75J/7rNZuPgwYPpN6N5enryxx9/0KFDB6pWrcrLL79M165dWbhwobvegoiIiIiI3AbyzJRegYGBzJgxI8Ovh4aGXrWwaenSpVm9enVORBMREREREUmXZ0ayRERERERE8gIVWSIiIiIiIk6kIktERERERMSJVGSJiIiIiIg4kYosERERERERJ1KRJSIiIiIi4kQqskRERERERJxIRZaIiIiIiIgTqcgSERERERFxIqu7A+R2hmEAEBMT4+YkYLPZSEhIICYmBg8PD3fHyXfUv66l/nUt9a9rqX9dS/3rWupf11L/ul5u6uN/aoJ/aoSMqMi6idjYWABKly7t5iQiIiIiIpIbxMbGEhAQkOHXTcbNyrDbnN1u5+zZs/j5+WEymdyaJSYmhtKlSxMeHo6/v79bs+RH6l/XUv+6lvrXtdS/rqX+dS31r2upf10vN/WxYRjExsZSsmRJzOaM77zSSNZNmM1mSpUq5e4YV/H393f7AZafqX9dS/3rWupf11L/upb617XUv66l/nW93NLHNxrB+ocmvhAREREREXEiFVkiIiIiIiJOpCIrD/Hy8uKtt97Cy8vL3VHyJfWva6l/XUv961rqX9dS/7qW+te11L+ulxf7WBNfiIiIiIiIOJFGskRERERERJxIRZaIiIiIiIgTqcgSERERERFxIhVZIiIiIiIiTqQiKxdZs2YN9913HyVLlsRkMvHLL7/cdJtVq1ZRr149vLy8qFixIlOnTnV5zrwqq/27atUqTCbTNR+RkZE5EzgP+fDDD2nYsCF+fn4UL16cLl26cPDgwZtuN3v2bKpWrYq3tzd33HEHixcvzoG0eU92+nfq1KnXHLve3t45lDhv+frrr6lVq1b6IpdNmjRhyZIlN9xGx27mZbV/dezemo8++giTycSQIUNu2E7HcPZkpn91DGfN22+/fU1/Va1a9Ybb5IXjV0VWLhIfH0/t2rUZP358ptofP36ce+65hzZt2rBz506GDBlC3759+f33312cNG/Kav/+4+DBg0RERKR/FC9e3EUJ867Vq1fz7LPPsmnTJpYvX47NZqNDhw7Ex8dnuM2GDRvo0aMHffr0YceOHXTp0oUuXbqwd+/eHEyeN2SnfwH8/f2vOnZPnjyZQ4nzllKlSvHRRx+xbds2tm7dyl133cUDDzzAvn37rttex27WZLV/Qcdudm3ZsoWJEydSq1atG7bTMZw9me1f0DGcVTVq1Liqv9atW5dh2zxz/BqSKwHG/Pnzb9hm2LBhRo0aNa567pFHHjE6duzowmT5Q2b6d+XKlQZgXL58OUcy5SdRUVEGYKxevTrDNt27dzfuueeeq55r3LixMWDAAFfHy/My07/fffedERAQkHOh8pnChQsbkydPvu7XdOzeuhv1r47d7ImNjTUqVapkLF++3GjVqpXxwgsvZNhWx3DWZaV/dQxnzVtvvWXUrl070+3zyvGrkaw8bOPGjbRr1+6q5zp27MjGjRvdlCh/qlOnDsHBwbRv357169e7O06eEB0dDUBgYGCGbXT8Zl9m+hcgLi6OsmXLUrp06ZuOHIhDWloaM2fOJD4+niZNmly3jY7d7MtM/4KO3ex49tlnueeee645Nq9Hx3DWZaV/QcdwVh0+fJiSJUtSvnx5Hn/8cU6dOpVh27xy/FrdHUCyLzIykhIlSlz1XIkSJYiJiSExMREfHx83JcsfgoODmTBhAg0aNCA5OZnJkyfTunVrwsLCqFevnrvj5Vp2u50hQ4bQrFkzatasmWG7jI5f3fN2Y5nt3ypVqjBlyhRq1apFdHQ0o0aNomnTpuzbt49SpUrlYOK8Yc+ePTRp0oSkpCQKFizI/PnzqV69+nXb6tjNuqz0r47drJs5cybbt29ny5YtmWqvYzhrstq/OoazpnHjxkydOpUqVaoQERHByJEjadGiBXv37sXPz++a9nnl+FWRJZKBKlWqUKVKlfTHTZs25ejRo4wZM4Yff/zRjclyt2effZa9e/fe8Hpqyb7M9m+TJk2uGilo2rQp1apVY+LEibz77ruujpnnVKlShZ07dxIdHc2cOXN48sknWb16dYaFgGRNVvpXx27WhIeH88ILL7B8+XJNruAC2elfHcNZc/fdd6d/XqtWLRo3bkzZsmX5+eef6dOnjxuT3RoVWXlYUFAQ586du+q5c+fO4e/vr1EsF2nUqJGKhxsYPHgwv/32G2vWrLnpX+syOn6DgoJcGTFPy0r//peHhwd169blyJEjLkqXt3l6elKxYkUA6tevz5YtWxg7diwTJ068pq2O3azLSv/+l47dG9u2bRtRUVFXXWGRlpbGmjVr+PLLL0lOTsZisVy1jY7hzMtO//6XjuGsKVSoEJUrV86wv/LK8at7svKwJk2a8Oeff1713PLly294nbvcmp07dxIcHOzuGLmOYRgMHjyY+fPns2LFCsqVK3fTbXT8Zl52+ve/0tLS2LNnj47fTLLb7SQnJ1/3azp2b92N+ve/dOzeWNu2bdmzZw87d+5M/2jQoAGPP/44O3fuvG4BoGM487LTv/+lYzhr4uLiOHr0aIb9lWeOX3fPvCH/LzY21tixY4exY8cOAzA+++wzY8eOHcbJkycNwzCM4cOHGz179kxvf+zYMcPX19cYOnSosX//fmP8+PGGxWIxli5d6q63kKtltX/HjBlj/PLLL8bhw4eNPXv2GC+88IJhNpuNP/74w11vIdcaOHCgERAQYKxatcqIiIhI/0hISEhv07NnT2P48OHpj9evX29YrVZj1KhRxv79+4233nrL8PDwMPbs2eOOt5CrZad/R44cafz+++/G0aNHjW3bthmPPvqo4e3tbezbt88dbyFXGz58uLF69Wrj+PHjxu7du43hw4cbJpPJWLZsmWEYOnZvVVb7V8furfvv7Hc6hp3rZv2rYzhrXn75ZWPVqlXG8ePHjfXr1xvt2rUzihYtakRFRRmGkXePXxVZucg/U4b/9+PJJ580DMMwnnzySaNVq1bXbFOnTh3D09PTKF++vPHdd9/leO68Iqv9+/HHHxsVKlQwvL29jcDAQKN169bGihUr3BM+l7tevwJXHY+tWrVK7+t//Pzzz0blypUNT09Po0aNGsaiRYtyNngekZ3+HTJkiFGmTBnD09PTKFGihNG5c2dj+/btOR8+D3j66aeNsmXLGp6enkaxYsWMtm3bphcAhqFj91ZltX917N66/xYBOoad62b9q2M4ax555BEjODjY8PT0NEJCQoxHHnnEOHLkSPrX8+rxazIMw8i5cTMREREREZH8TfdkiYiIiIiIOJGKLBERERERESdSkSUiIiIiIuJEKrJEREREREScSEWWiIiIiIiIE6nIEhERERERcSIVWSIiIiIiIk6kIktERERERMSJVGSJiIhkQevWrRkyZMgN24SGhvL555/nSB4REcl9VGSJiMhtp3fv3phMpms+jhw54u5oIiKSD1jdHUBERMQdOnXqxHfffXfVc8WKFXNTGhERyU80kiUiIrclLy8vgoKCrvqwWCysXr2aRo0a4eXlRXBwMMOHDyc1NTXD/URFRXHffffh4+NDuXLlmD59eg6+CxERyY00kiUiIvK3M2fO0LlzZ3r37s0PP/zAgQMH6NevH97e3rz99tvX3aZ3796cPXuWlStX4uHhwfPPP09UVFTOBhcRkVxFRZaIiNyWfvvtNwoWLJj++O6776Zy5cqULl2aL7/8EpPJRNWqVTl79iyvvvoqb775Jmbz1ReAHDp0iCVLlrB582YaNmwIwLfffku1atVy9L2IiEjuoiJLRERuS23atOHrr79Of1ygQAGeffZZmjRpgslkSn++WbNmxMXFcfr0acqUKXPVPvbv34/VaqV+/frpz1WtWpVChQq5PL+IiOReKrJEROS2VKBAASpWrOjuGCIikg9p4gsREZG/VatWjY0bN2IYRvpz69evx8/Pj1KlSl3TvmrVqqSmprJt27b05w4ePMiVK1dyIq6IiORSKrJERET+NmjQIMLDw3nuuec4cOAAv/76K2+99RYvvfTSNfdjAVSpUoVOnToxYMAAwsLC2LZtG3379sXHx8cN6UVEJLdQkSUiIvK3kJAQFi9ezObNm6lduzbPPPMMffr04Y033shwm++++46SJUvSqlUrHnroIfr370/x4sVzMLWIiOQ2JuPf10SIiIiIiIjILdFIloiIiIiIiBOpyBIREREREXEiFVkiIiIiIiJOpCJLRERERETEiVRkiYiIiIiIOJGKLBERERERESdSkSUiIiIiIuJEKrJEREREREScSEWWiIiIiIiIE6nIEhERERERcSIVWSIiIiIiIk70fxRutIiMGeMbAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyError",
          "evalue": "'rmse'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1243814851.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded RMSE:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rmse\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded MAE:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaded_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mae\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'rmse'"
          ]
        }
      ],
      "source": [
        "loaded_results = run_ssl_pipeline_kfold_with_r2(\n",
        "    labeled_df=df,\n",
        "    load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z75zF89KzcXV"
      },
      "source": [
        "### Extract Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUE_fyzvzc4j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import optuna\n",
        "\n",
        "# -----------------------------\n",
        "# Utility Functions\n",
        "# -----------------------------\n",
        "def extract_rgb_hsv_features(image_path, resize_shape=(224,224)):\n",
        "    \"\"\"\n",
        "    Extract mean Red channel and mean Saturation from a lip image.\n",
        "    Returns: np.array([mean_R, mean_S])\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    img = img.resize(resize_shape)\n",
        "    img_np = np.array(img)\n",
        "\n",
        "    mean_r = img_np[:,:,0].mean()\n",
        "    img_hsv = cv2.cvtColor(img_np, cv2.COLOR_RGB2HSV)\n",
        "    mean_s = img_hsv[:,:,1].mean()\n",
        "\n",
        "    return np.array([mean_r, mean_s], dtype=np.float32)\n",
        "\n",
        "def extract_features(df, backbone=None, transform=None, device=\"cuda\", mode=\"ssl+rgb\"):\n",
        "    \"\"\"\n",
        "    Extract features based on mode:\n",
        "      - ssl: SSL embeddings only\n",
        "      - rgb: RGB/HSV only\n",
        "      - ssl+rgb: concatenate SSL embeddings and RGB/HSV\n",
        "    \"\"\"\n",
        "    X_list, y_list = [], []\n",
        "    if backbone is not None:\n",
        "        backbone.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, row in df.iterrows():\n",
        "            img_path = row['Filename']\n",
        "            feats = []\n",
        "\n",
        "            # SSL embeddings\n",
        "            if mode in [\"ssl\", \"ssl+rgb\"]:\n",
        "                img_tensor = transform(Image.open(img_path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "                ssl_feat = backbone(img_tensor).view(1, -1).cpu().numpy().flatten()\n",
        "                feats.append(ssl_feat)\n",
        "\n",
        "            # RGB/HSV features\n",
        "            if mode in [\"rgb\", \"ssl+rgb\"]:\n",
        "                rgb_hsv_feat = extract_rgb_hsv_features(img_path)\n",
        "                feats.append(rgb_hsv_feat)\n",
        "\n",
        "            X_list.append(np.concatenate(feats))\n",
        "            y_list.append(row['Hb'])\n",
        "\n",
        "    X = np.vstack(X_list)\n",
        "    y = np.array(y_list, dtype=np.float32)\n",
        "    return X, y\n",
        "\n",
        "# -----------------------------\n",
        "# Main K-Fold Regression Pipeline\n",
        "# -----------------------------\n",
        "def run_ssl_rgb_pipeline_kfold(labeled_df,\n",
        "                               unlabelled_dir=None,\n",
        "                               ssl_transform=None,\n",
        "                               val_transform=None,\n",
        "                               ssl_epochs=20,\n",
        "                               ssl_batch=8,\n",
        "                               fine_tune_backbone=True,\n",
        "                               fine_tune_epochs=10,\n",
        "                               mode=\"ssl+rgb\",\n",
        "                               optuna_trials=20,\n",
        "                               n_splits=5,\n",
        "                               random_seed=42,\n",
        "                               run_base_dir=\"models\",\n",
        "                               load_run_dir=None,\n",
        "                               device=\"cuda\"):\n",
        "\n",
        "    import os\n",
        "    os.makedirs(run_base_dir, exist_ok=True)\n",
        "    run_dir = load_run_dir if load_run_dir else os.path.join(run_base_dir, \"run_\"+str(np.random.randint(1e5)))\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    print(f\"Run directory: {run_dir}\")\n",
        "\n",
        "    # -----------------------\n",
        "    # Load or pretrain SSL backbone\n",
        "    # -----------------------\n",
        "    backbone = get_ssl_backbone(pretrained=False)\n",
        "    proj_head = ProjectionHead().to(device)\n",
        "\n",
        "    if load_run_dir and os.path.exists(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\")):\n",
        "        backbone.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"), map_location=device))\n",
        "        proj_head.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"), map_location=device))\n",
        "        backbone.to(device).eval()\n",
        "        proj_head.to(device).eval()\n",
        "        print(\"Loaded pretrained backbone and projection head from saved run.\")\n",
        "    else:\n",
        "        print(\"=== SSL Pretraining ===\")\n",
        "        if unlabelled_dir:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(unlabelled_dir, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=True)\n",
        "        else:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(labeled_df, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=False)\n",
        "\n",
        "    # -----------------------\n",
        "    # K-Fold CV\n",
        "    # -----------------------\n",
        "    from sklearn.model_selection import KFold\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    import xgboost as xgb\n",
        "    import optuna\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_seed)\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(labeled_df)):\n",
        "        print(f\"\\n--- K-Fold {fold_idx+1}/{n_splits} ---\")\n",
        "        train_df, test_df = labeled_df.iloc[train_idx], labeled_df.iloc[test_idx]\n",
        "\n",
        "        # Fine-tune backbone if required\n",
        "        if fine_tune_backbone and \"ssl\" in mode:\n",
        "            dataset = HbImageDataset(train_df, transform=ssl_transform, n_views=2)\n",
        "            loader = DataLoader(dataset, batch_size=ssl_batch, shuffle=True, num_workers=2, pin_memory=True)\n",
        "            backbone.train()\n",
        "            proj_head.train()\n",
        "            optimizer = torch.optim.Adam(list(backbone.parameters()) + list(proj_head.parameters()), lr=1e-4)\n",
        "            for epoch in range(fine_tune_epochs):\n",
        "                total_loss = 0.0\n",
        "                for views, _ in loader:\n",
        "                    v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "                    feats1, feats2 = backbone(v1).view(v1.size(0), -1), backbone(v2).view(v2.size(0), -1)\n",
        "                    z1, z2 = proj_head(feats1), proj_head(feats2)\n",
        "                    loss = nt_xent_loss(z1, z2)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "                print(f\"Fold {fold_idx+1} Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "        # -----------------------\n",
        "        # Feature extraction\n",
        "        # -----------------------\n",
        "        X_train, y_train = extract_features(train_df, backbone if \"ssl\" in mode else None,\n",
        "                                            transform=val_transform, device=device, mode=mode)\n",
        "        X_test, y_test = extract_features(test_df, backbone if \"ssl\" in mode else None,\n",
        "                                          transform=val_transform, device=device, mode=mode)\n",
        "\n",
        "        # -----------------------\n",
        "        # Optuna Hyperparameter Tuning\n",
        "        # -----------------------\n",
        "        def objective(trial):\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\",50,300),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\",2,10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\",0.01,0.3,log=True),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\",0.5,1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.5,1.0),\n",
        "                \"random_state\": random_seed,\n",
        "                \"verbosity\": 0,\n",
        "                \"n_jobs\": 1,\n",
        "                \"tree_method\": \"hist\"\n",
        "            }\n",
        "            kf_inner = KFold(n_splits=min(5, len(train_df)), shuffle=True, random_state=random_seed)\n",
        "            maes = []\n",
        "            for tr_idx, val_idx in kf_inner.split(X_train):\n",
        "                model = xgb.XGBRegressor(**params)\n",
        "                model.fit(X_train[tr_idx], y_train[tr_idx])\n",
        "                y_pred = model.predict(X_train[val_idx])\n",
        "                maes.append(mean_absolute_error(y_train[val_idx], y_pred))\n",
        "            return np.mean(maes)\n",
        "\n",
        "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=random_seed))\n",
        "        study.optimize(objective, n_trials=optuna_trials, show_progress_bar=False)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best params fold {fold_idx+1}: {best_params}\")\n",
        "\n",
        "        # -----------------------\n",
        "        # Train final model\n",
        "        # -----------------------\n",
        "        final_model = xgb.XGBRegressor(**best_params, random_state=random_seed, n_jobs=-1, tree_method=\"hist\")\n",
        "        final_model.fit(X_train, y_train)\n",
        "        y_pred = final_model.predict(X_test)\n",
        "\n",
        "        # Save XGBoost model\n",
        "        model_path = os.path.join(run_dir, f\"xgb_fold{fold_idx+1}.json\")\n",
        "        final_model.save_model(model_path)\n",
        "        print(f\"Saved XGBoost model for fold {fold_idx+1} at {model_path}\")\n",
        "\n",
        "        # -----------------------\n",
        "        # Metrics\n",
        "        # -----------------------\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        print(f\"[Fold {fold_idx+1}] RMSE: {rmse:.4f}, MAE: {mae:.4f}, R: {r2:.4f}\")\n",
        "        fold_metrics.append({\"fold\": fold_idx+1, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n",
        "\n",
        "    # -----------------------\n",
        "    # Summary metrics\n",
        "    # -----------------------\n",
        "    metrics_df = pd.DataFrame(fold_metrics)\n",
        "    metrics_df.loc[\"Mean\"] = metrics_df.mean()\n",
        "    print(\"\\n=== Fold Metrics Summary ===\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    # Save backbone + projection head\n",
        "    if \"ssl\" in mode:\n",
        "        torch.save(backbone.state_dict(), os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"))\n",
        "        torch.save(proj_head.state_dict(), os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"))\n",
        "\n",
        "    return metrics_df, run_dir\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eur5B7ZrziV7"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI2AgeF7zglz",
        "outputId": "ca25234f-439e-4d42-a394-d59b833cf209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run directory: /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\n",
            "Loaded pretrained backbone and projection head from saved run.\n",
            "\n",
            "--- K-Fold 1/5 ---\n",
            "Fold 1 Epoch 1 Loss: 1.1195\n",
            "Fold 1 Epoch 2 Loss: 1.0502\n",
            "Fold 1 Epoch 3 Loss: 1.0614\n",
            "Fold 1 Epoch 4 Loss: 1.0643\n",
            "Fold 1 Epoch 5 Loss: 0.9797\n",
            "Fold 1 Epoch 6 Loss: 1.0569\n",
            "Fold 1 Epoch 7 Loss: 0.9918\n",
            "Fold 1 Epoch 8 Loss: 1.0422\n",
            "Fold 1 Epoch 9 Loss: 0.9975\n",
            "Fold 1 Epoch 10 Loss: 0.9845\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:26:14,190] A new study created in memory with name: no-name-8c6e8c9c-db27-4b18-93b9-0b6564f3620e\n",
            "[I 2025-10-07 13:26:15,788] Trial 0 finished with value: 1.9691081762313842 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.9691081762313842.\n",
            "[I 2025-10-07 13:26:16,146] Trial 1 finished with value: 1.9143864870071412 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.9143864870071412.\n",
            "[I 2025-10-07 13:26:16,520] Trial 2 finished with value: 2.014492988586426 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 1.9143864870071412.\n",
            "[I 2025-10-07 13:26:17,117] Trial 3 finished with value: 1.786890411376953 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 1.786890411376953.\n",
            "[I 2025-10-07 13:26:18,193] Trial 4 finished with value: 1.9907170295715333 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 1.786890411376953.\n",
            "[I 2025-10-07 13:26:19,157] Trial 5 finished with value: 1.9137231349945067 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 3 with value: 1.786890411376953.\n",
            "[I 2025-10-07 13:26:20,491] Trial 6 finished with value: 1.660827922821045 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 6 with value: 1.660827922821045.\n",
            "[I 2025-10-07 13:26:22,201] Trial 7 finished with value: 1.8631847143173217 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 6 with value: 1.660827922821045.\n",
            "[I 2025-10-07 13:26:22,818] Trial 8 finished with value: 1.9336982488632202 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 6 with value: 1.660827922821045.\n",
            "[I 2025-10-07 13:26:24,044] Trial 9 finished with value: 1.764780879020691 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 6 with value: 1.660827922821045.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 1: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}\n",
            "Saved XGBoost model for fold 1 at /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740/xgb_fold1.json\n",
            "[Fold 1] RMSE: 3.6573, MAE: 2.4402, R: -0.6919\n",
            "\n",
            "--- K-Fold 2/5 ---\n",
            "Fold 2 Epoch 1 Loss: 0.7829\n",
            "Fold 2 Epoch 2 Loss: 0.7560\n",
            "Fold 2 Epoch 3 Loss: 0.7604\n",
            "Fold 2 Epoch 4 Loss: 0.7665\n",
            "Fold 2 Epoch 5 Loss: 0.7729\n",
            "Fold 2 Epoch 6 Loss: 0.7358\n",
            "Fold 2 Epoch 7 Loss: 0.7524\n",
            "Fold 2 Epoch 8 Loss: 0.7243\n",
            "Fold 2 Epoch 9 Loss: 0.7186\n",
            "Fold 2 Epoch 10 Loss: 0.7109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:26:52,163] A new study created in memory with name: no-name-61dde273-e5e0-49dd-a70a-2984597d2065\n",
            "[I 2025-10-07 13:26:53,224] Trial 0 finished with value: 2.3083893775939943 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.3083893775939943.\n",
            "[I 2025-10-07 13:26:53,568] Trial 1 finished with value: 1.705501127243042 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.705501127243042.\n",
            "[I 2025-10-07 13:26:54,062] Trial 2 finished with value: 2.3222278594970702 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 1.705501127243042.\n",
            "[I 2025-10-07 13:26:55,031] Trial 3 finished with value: 1.9386889219284058 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 1.705501127243042.\n",
            "[I 2025-10-07 13:26:56,790] Trial 4 finished with value: 1.9150012016296387 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 1.705501127243042.\n",
            "[I 2025-10-07 13:26:58,048] Trial 5 finished with value: 2.1165454387664795 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 1.705501127243042.\n",
            "[I 2025-10-07 13:26:59,355] Trial 6 finished with value: 1.5943248271942139 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 6 with value: 1.5943248271942139.\n",
            "[I 2025-10-07 13:27:01,057] Trial 7 finished with value: 1.7457662343978881 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 6 with value: 1.5943248271942139.\n",
            "[I 2025-10-07 13:27:01,586] Trial 8 finished with value: 2.074930262565613 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 6 with value: 1.5943248271942139.\n",
            "[I 2025-10-07 13:27:02,877] Trial 9 finished with value: 2.120929646492004 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 6 with value: 1.5943248271942139.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 2: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}\n",
            "Saved XGBoost model for fold 2 at /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740/xgb_fold2.json\n",
            "[Fold 2] RMSE: 3.5470, MAE: 2.1194, R: -2.0331\n",
            "\n",
            "--- K-Fold 3/5 ---\n",
            "Fold 3 Epoch 1 Loss: 0.7319\n",
            "Fold 3 Epoch 2 Loss: 0.7289\n",
            "Fold 3 Epoch 3 Loss: 0.7313\n",
            "Fold 3 Epoch 4 Loss: 0.7151\n",
            "Fold 3 Epoch 5 Loss: 0.7283\n",
            "Fold 3 Epoch 6 Loss: 0.7008\n",
            "Fold 3 Epoch 7 Loss: 0.7180\n",
            "Fold 3 Epoch 8 Loss: 0.7049\n",
            "Fold 3 Epoch 9 Loss: 0.6958\n",
            "Fold 3 Epoch 10 Loss: 0.7186\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:27:33,133] A new study created in memory with name: no-name-b89ce22e-c7c5-4448-9dd5-9ffaeedab006\n",
            "[I 2025-10-07 13:27:34,226] Trial 0 finished with value: 2.5161930561065673 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.5161930561065673.\n",
            "[I 2025-10-07 13:27:34,600] Trial 1 finished with value: 2.527419829368591 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.5161930561065673.\n",
            "[I 2025-10-07 13:27:34,999] Trial 2 finished with value: 2.2174423694610597 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.2174423694610597.\n",
            "[I 2025-10-07 13:27:35,616] Trial 3 finished with value: 2.5222216844558716 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 2.2174423694610597.\n",
            "[I 2025-10-07 13:27:37,279] Trial 4 finished with value: 2.567925524711609 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 2.2174423694610597.\n",
            "[I 2025-10-07 13:27:38,918] Trial 5 finished with value: 2.4041481018066406 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 2.2174423694610597.\n",
            "[I 2025-10-07 13:27:40,534] Trial 6 finished with value: 3.1847208023071287 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 2.2174423694610597.\n",
            "[I 2025-10-07 13:27:42,322] Trial 7 finished with value: 2.7667809009552 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 2.2174423694610597.\n",
            "[I 2025-10-07 13:27:42,891] Trial 8 finished with value: 2.7868019580841064 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 2.2174423694610597.\n",
            "[I 2025-10-07 13:27:44,146] Trial 9 finished with value: 2.4171459436416627 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 2.2174423694610597.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 3: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}\n",
            "Saved XGBoost model for fold 3 at /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740/xgb_fold3.json\n",
            "[Fold 3] RMSE: 2.7373, MAE: 1.9622, R: -0.0654\n",
            "\n",
            "--- K-Fold 4/5 ---\n",
            "Fold 4 Epoch 1 Loss: 0.7211\n",
            "Fold 4 Epoch 2 Loss: 0.7408\n",
            "Fold 4 Epoch 3 Loss: 0.7148\n",
            "Fold 4 Epoch 4 Loss: 0.7210\n",
            "Fold 4 Epoch 5 Loss: 0.7005\n",
            "Fold 4 Epoch 6 Loss: 0.7001\n",
            "Fold 4 Epoch 7 Loss: 0.6965\n",
            "Fold 4 Epoch 8 Loss: 0.7052\n",
            "Fold 4 Epoch 9 Loss: 0.7003\n",
            "Fold 4 Epoch 10 Loss: 0.7077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:28:16,475] A new study created in memory with name: no-name-c36ff56d-8d95-4e54-8071-aa50625c197f\n",
            "[I 2025-10-07 13:28:17,654] Trial 0 finished with value: 2.404097783565521 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.404097783565521.\n",
            "[I 2025-10-07 13:28:18,247] Trial 1 finished with value: 2.3308253645896913 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.3308253645896913.\n",
            "[I 2025-10-07 13:28:18,892] Trial 2 finished with value: 2.221395754814148 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.221395754814148.\n",
            "[I 2025-10-07 13:28:19,885] Trial 3 finished with value: 2.2921078205108643 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 2.221395754814148.\n",
            "[I 2025-10-07 13:28:21,434] Trial 4 finished with value: 2.4015008449554442 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 2.221395754814148.\n",
            "[I 2025-10-07 13:28:22,491] Trial 5 finished with value: 2.426528549194336 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 2.221395754814148.\n",
            "[I 2025-10-07 13:28:23,953] Trial 6 finished with value: 2.3045894861221314 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 2.221395754814148.\n",
            "[I 2025-10-07 13:28:25,726] Trial 7 finished with value: 2.308556652069092 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 2.221395754814148.\n",
            "[I 2025-10-07 13:28:26,350] Trial 8 finished with value: 2.2386242389678954 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 2.221395754814148.\n",
            "[I 2025-10-07 13:28:27,643] Trial 9 finished with value: 2.3619044065475463 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 2.221395754814148.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 4: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}\n",
            "Saved XGBoost model for fold 4 at /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740/xgb_fold4.json\n",
            "[Fold 4] RMSE: 3.4050, MAE: 3.0960, R: -0.4672\n",
            "\n",
            "--- K-Fold 5/5 ---\n",
            "Fold 5 Epoch 1 Loss: 0.6976\n",
            "Fold 5 Epoch 2 Loss: 0.6940\n",
            "Fold 5 Epoch 3 Loss: 0.7175\n",
            "Fold 5 Epoch 4 Loss: 0.7020\n",
            "Fold 5 Epoch 5 Loss: 0.7065\n",
            "Fold 5 Epoch 6 Loss: 0.7029\n",
            "Fold 5 Epoch 7 Loss: 0.7009\n",
            "Fold 5 Epoch 8 Loss: 0.6990\n",
            "Fold 5 Epoch 9 Loss: 0.7036\n",
            "Fold 5 Epoch 10 Loss: 0.7009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 13:28:54,276] A new study created in memory with name: no-name-acc9ddd0-e0d3-4524-9b7c-ea833dc7c5b9\n",
            "[I 2025-10-07 13:28:55,416] Trial 0 finished with value: 2.0219948291778564 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.0219948291778564.\n",
            "[I 2025-10-07 13:28:55,779] Trial 1 finished with value: 2.1999799489974974 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.0219948291778564.\n",
            "[I 2025-10-07 13:28:56,209] Trial 2 finished with value: 2.0173492670059203 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.0173492670059203.\n",
            "[I 2025-10-07 13:28:56,877] Trial 3 finished with value: 2.1020614862442017 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 2.0173492670059203.\n",
            "[I 2025-10-07 13:28:58,010] Trial 4 finished with value: 2.0497758388519287 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 2.0173492670059203.\n",
            "[I 2025-10-07 13:28:59,029] Trial 5 finished with value: 2.0425270915031435 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 2.0173492670059203.\n",
            "[I 2025-10-07 13:29:01,252] Trial 6 finished with value: 2.040940982103348 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 2.0173492670059203.\n",
            "[I 2025-10-07 13:29:03,797] Trial 7 finished with value: 2.084032416343689 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 2.0173492670059203.\n",
            "[I 2025-10-07 13:29:04,510] Trial 8 finished with value: 2.1448307752609255 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 2.0173492670059203.\n",
            "[I 2025-10-07 13:29:05,821] Trial 9 finished with value: 2.007338583469391 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 9 with value: 2.007338583469391.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params fold 5: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}\n",
            "Saved XGBoost model for fold 5 at /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740/xgb_fold5.json\n",
            "[Fold 5] RMSE: 3.4698, MAE: 2.0711, R: 0.2249\n",
            "\n",
            "=== Fold Metrics Summary ===\n",
            "      fold      RMSE       MAE        R2\n",
            "0      1.0  3.657303  2.440218 -0.691924\n",
            "1      2.0  3.547014  2.119395 -2.033061\n",
            "2      3.0  2.737258  1.962170 -0.065422\n",
            "3      4.0  3.405017  3.096026 -0.467201\n",
            "4      5.0  3.469792  2.071146  0.224886\n",
            "Mean   3.0  3.363277  2.337791 -0.606544\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as T\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "ssl_transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ----------------------------\n",
        "# Run the pipeline\n",
        "# ----------------------------\n",
        "# # Example 1: SSL embeddings only\n",
        "# metrics_ssl, run_dir_ssl = run_ssl_rgb_pipeline_kfold(\n",
        "#     labeled_df=df,\n",
        "#     unlabelled_dir=None,       # or path to unlabelled images if available\n",
        "#     ssl_transform=ssl_transform,\n",
        "#     val_transform=val_transform,\n",
        "#     fine_tune_backbone=True,\n",
        "#     fine_tune_epochs=5,\n",
        "#     mode=\"ssl\",\n",
        "#     load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\"\n",
        "# )\n",
        "\n",
        "# # Example 2: RGB/HSV features only\n",
        "# metrics_rgb, run_dir_rgb = run_ssl_rgb_pipeline_kfold(\n",
        "#     labeled_df=df,\n",
        "#     ssl_transform=ssl_transform,   # still required for function signature\n",
        "#     val_transform=val_transform,\n",
        "#     mode=\"rgb\",\n",
        "#     n_splits=5,\n",
        "#     optuna_trials=10,\n",
        "#     random_seed=42,\n",
        "#     run_base_dir=\"models\"\n",
        "# )\n",
        "\n",
        "# Example 3: Combined SSL + RGB/HSV\n",
        "metrics_ssl_rgb, run_dir_ssl_rgb = run_ssl_rgb_pipeline_kfold(\n",
        "    labeled_df=df,\n",
        "    ssl_transform=ssl_transform,\n",
        "    val_transform=val_transform,\n",
        "    mode=\"ssl+rgb\",\n",
        "    n_splits=5,\n",
        "    optuna_trials=10,\n",
        "    random_seed=42,\n",
        "    run_base_dir=\"models\",\n",
        "    load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrFnr4Cv4uHL"
      },
      "source": [
        "#### Extract Full Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "3mo55lFyzgjj",
        "outputId": "01692f3f-6d84-4278-a486-c4b14dcbb879"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'greycomatrix' from 'skimage.feature.texture' (/usr/local/lib/python3.12/dist-packages/skimage/feature/texture.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-398305034.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# For scikit-image >=0.20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexture\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgreycomatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreycoprops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlocal_binary_pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'greycomatrix' from 'skimage.feature.texture' (/usr/local/lib/python3.12/dist-packages/skimage/feature/texture.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import xgboost as xgb\n",
        "import optuna\n",
        "# For scikit-image >=0.20\n",
        "from skimage.feature.texture import greycomatrix, greycoprops\n",
        "from skimage.feature import local_binary_pattern\n",
        "\n",
        "from scipy.stats import skew, kurtosis\n",
        "import mediapipe as mp\n",
        "\n",
        "# ---------------------------\n",
        "# ROI detection (lips/conjunctiva)\n",
        "# ---------------------------\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "\n",
        "def detect_lips_roi(img_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(img_rgb)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        landmarks = results.multi_face_landmarks[0].landmark\n",
        "        xs = [landmarks[i].x for i in range(61, 89)]\n",
        "        ys = [landmarks[i].y for i in range(61, 89)]\n",
        "        h, w, _ = img.shape\n",
        "        x1, x2 = int(min(xs)*w), int(max(xs)*w)\n",
        "        y1, y2 = int(min(ys)*h), int(max(ys)*h)\n",
        "        roi = img[y1:y2, x1:x2]\n",
        "        return roi\n",
        "    else:\n",
        "        return img  # fallback: use whole image\n",
        "\n",
        "# ---------------------------\n",
        "# Rich feature extractor\n",
        "# ---------------------------\n",
        "def extract_rich_features(img, resize_shape=(224,224), hist_bins=16, glcm_distances=[1], glcm_angles=[0]):\n",
        "    img = cv2.resize(img, resize_shape)\n",
        "\n",
        "    # Color spaces\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    img_lab = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)\n",
        "\n",
        "    features = []\n",
        "\n",
        "    # --- Color statistics & histograms ---\n",
        "    for channel in [img_rgb, img_hsv, img_lab]:\n",
        "        for i in range(3):\n",
        "            c = channel[:,:,i].flatten()\n",
        "            features.extend([c.mean(), c.std(), skew(c), kurtosis(c), c.min(), c.max()])\n",
        "            hist = np.histogram(c, bins=hist_bins, range=(0,255))[0]\n",
        "            hist = hist / hist.sum()\n",
        "            features.extend(hist)\n",
        "\n",
        "    # --- Color ratios ---\n",
        "    R, G, B = img_rgb[:,:,0], img_rgb[:,:,1], img_rgb[:,:,2]\n",
        "    features.extend([(R.mean()+1e-5)/(G.mean()+1e-5), (R.mean()+1e-5)/(B.mean()+1e-5), (G.mean()+1e-5)/(B.mean()+1e-5)])\n",
        "    H, S, V = img_hsv[:,:,0], img_hsv[:,:,1], img_hsv[:,:,2]\n",
        "    features.extend([(H.mean()+1e-5)/(S.mean()+1e-5), (S.mean()+1e-5)/(V.mean()+1e-5)])\n",
        "\n",
        "    # --- Texture: GLCM features ---\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    gray_q = (gray/255*15).astype(np.uint8)\n",
        "    glcm = greycomatrix(gray_q, distances=glcm_distances, angles=glcm_angles, symmetric=True, normed=True)\n",
        "    for prop in ['contrast', 'correlation', 'energy', 'homogeneity']:\n",
        "        features.append(greycoprops(glcm, prop).mean())\n",
        "\n",
        "    # --- LBP histogram ---\n",
        "    lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')\n",
        "    n_bins = int(lbp.max() + 1)\n",
        "    lbp_hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins))\n",
        "    lbp_hist = lbp_hist / lbp_hist.sum()\n",
        "    features.extend(lbp_hist)\n",
        "\n",
        "    # --- Edge density ---\n",
        "    edges = cv2.Canny(gray, 100, 200)\n",
        "    features.append(edges.mean())\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "# ---------------------------\n",
        "# Wrapper to process dataframe\n",
        "# ---------------------------\n",
        "def extract_rich_features_from_df(df, img_column=\"Filename\"):\n",
        "    all_features = []\n",
        "    for path in df[img_column]:\n",
        "        roi = detect_lips_roi(path)\n",
        "        feats = extract_rich_features(roi)\n",
        "        all_features.append(feats)\n",
        "    return np.array(all_features)\n",
        "\n",
        "# ---------------------------\n",
        "# Main K-Fold SSL + XGBoost pipeline\n",
        "# ---------------------------\n",
        "def run_ssl_pipeline_kfold_with_rich_features(labeled_df,\n",
        "                                              unlabelled_dir=None,\n",
        "                                              ssl_epochs=20,\n",
        "                                              ssl_batch=8,\n",
        "                                              fine_tune_backbone=True,\n",
        "                                              fine_tune_epochs=10,\n",
        "                                              use_metadata=False,\n",
        "                                              optuna_trials=20,\n",
        "                                              run_base_dir=\"models\",\n",
        "                                              load_run_dir=None,\n",
        "                                              n_splits=5):\n",
        "    run_dir = load_run_dir if load_run_dir else make_run_dir(run_base_dir)\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    print(f\"Run directory: {run_dir}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Load or pretrain SSL backbone\n",
        "    # -------------------\n",
        "    backbone = get_ssl_backbone(pretrained=False)\n",
        "    proj_head = ProjectionHead().to(device)\n",
        "\n",
        "    if load_run_dir and os.path.exists(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\")):\n",
        "        backbone.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"), map_location=device))\n",
        "        proj_head.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"), map_location=device))\n",
        "        backbone.to(device).eval()\n",
        "        proj_head.to(device).eval()\n",
        "        print(\"Loaded pretrained backbone and projection head from saved run.\")\n",
        "    else:\n",
        "        print(\"=== SSL Pretraining ===\")\n",
        "        if unlabelled_dir:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(unlabelled_dir, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=True)\n",
        "        else:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(labeled_df, ssl_transform,\n",
        "                                                           epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                                                           run_dir=run_dir, unlabelled=False)\n",
        "\n",
        "    # -------------------\n",
        "    # K-Fold CV\n",
        "    # -------------------\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
        "    fold_metrics = []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(labeled_df)):\n",
        "        print(f\"\\n--- K-Fold {fold_idx+1}/{n_splits} ---\")\n",
        "        train_df, test_df = labeled_df.iloc[train_idx], labeled_df.iloc[test_idx]\n",
        "\n",
        "        # Fine-tune backbone if required\n",
        "        if fine_tune_backbone:\n",
        "            dataset = HbImageDataset(train_df, transform=ssl_transform, n_views=2)\n",
        "            loader = DataLoader(dataset, batch_size=ssl_batch, shuffle=True, num_workers=2, pin_memory=True)\n",
        "            backbone.train()\n",
        "            proj_head.train()\n",
        "            optimizer = torch.optim.Adam(list(backbone.parameters()) + list(proj_head.parameters()), lr=1e-4)\n",
        "            for epoch in range(fine_tune_epochs):\n",
        "                total_loss = 0.0\n",
        "                for views, _ in loader:\n",
        "                    v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "                    feats1, feats2 = backbone(v1).view(v1.size(0), -1), backbone(v2).view(v2.size(0), -1)\n",
        "                    z1, z2 = proj_head(feats1), proj_head(feats2)\n",
        "                    loss = nt_xent_loss(z1, z2)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "                print(f\"Fold {fold_idx+1} Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "        # -------------------\n",
        "        # Extract embeddings and rich features\n",
        "        # -------------------\n",
        "        X_train_ssl, y_train = extract_embeddings(train_df, backbone, val_transform)\n",
        "        X_test_ssl, y_test = extract_embeddings(test_df, backbone, val_transform)\n",
        "\n",
        "        X_train_rich = extract_rich_features_from_df(train_df)\n",
        "        X_test_rich = extract_rich_features_from_df(test_df)\n",
        "\n",
        "        X_train = np.hstack([X_train_ssl, X_train_rich])\n",
        "        X_test = np.hstack([X_test_ssl, X_test_rich])\n",
        "\n",
        "        # -------------------\n",
        "        # Optuna hyperparameter tuning\n",
        "        # -------------------\n",
        "        def objective(trial):\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\",50,300),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\",2,10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\",0.01,0.3,log=True),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\",0.5,1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.5,1.0),\n",
        "                \"random_state\": RANDOM_SEED,\n",
        "                \"verbosity\": 0,\n",
        "                \"n_jobs\": 1,\n",
        "                \"tree_method\": \"hist\"\n",
        "            }\n",
        "            kf_inner = KFold(n_splits=min(5, len(train_df)), shuffle=True, random_state=RANDOM_SEED)\n",
        "            maes = []\n",
        "            for tr_idx, val_idx in kf_inner.split(X_train):\n",
        "                model = xgb.XGBRegressor(**params)\n",
        "                model.fit(X_train[tr_idx], y_train[tr_idx])\n",
        "                y_pred = model.predict(X_train[val_idx])\n",
        "                maes.append(mean_absolute_error(y_train[val_idx], y_pred))\n",
        "            return np.mean(maes)\n",
        "\n",
        "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "        study.optimize(objective, n_trials=optuna_trials, show_progress_bar=False)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best params fold {fold_idx+1}: {best_params}\")\n",
        "\n",
        "        # Train final model\n",
        "        final_model = xgb.XGBRegressor(**best_params, random_state=RANDOM_SEED, n_jobs=-1, tree_method=\"hist\")\n",
        "        final_model.fit(X_train, y_train)\n",
        "        y_pred = final_model.predict(X_test)\n",
        "\n",
        "        # Compute metrics\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        print(f\"[Fold {fold_idx+1}] RMSE: {rmse:.4f}, MAE: {mae:.4f}, R: {r2:.4f}\")\n",
        "\n",
        "        fold_metrics.append({\"fold\": fold_idx+1, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n",
        "\n",
        "    # -------------------\n",
        "    # Summary metrics\n",
        "    # -------------------\n",
        "    metrics_df = pd.DataFrame(fold_metrics)\n",
        "    metrics_df.loc[\"Mean\"] = metrics_df.mean()\n",
        "    print(\"\\n=== Fold Metrics Summary ===\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    # Save metrics and models\n",
        "    metrics_df.to_csv(os.path.join(run_dir, \"kfold_metrics.csv\"), index=False)\n",
        "    torch.save(backbone.state_dict(), os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"))\n",
        "    torch.save(proj_head.state_dict(), os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"))\n",
        "\n",
        "    # Visualization\n",
        "    plt.figure(figsize=(10,6))\n",
        "    plt.plot(metrics_df['fold'][:-1], metrics_df['RMSE'][:-1], marker='o', label='RMSE')\n",
        "    plt.plot(metrics_df['fold'][:-1], metrics_df['MAE'][:-1], marker='s', label='MAE')\n",
        "    plt.plot(metrics_df['fold'][:-1], metrics_df['R2'][:-1], marker='^', label='R')\n",
        "    plt.axhline(metrics_df.loc[\"Mean\", 'RMSE'], color='blue', linestyle='--', alpha=0.5)\n",
        "    plt.axhline(metrics_df.loc[\"Mean\", 'MAE'], color='orange', linestyle='--', alpha=0.5)\n",
        "    plt.axhline(metrics_df.loc[\"Mean\", 'R2'], color='green', linestyle='--', alpha=0.5)\n",
        "    plt.xlabel(\"Fold\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.title(\"K-Fold Regression Metrics (RMSE, MAE, R)\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(run_dir, \"kfold_metrics_plot.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        \"backbone\": backbone,\n",
        "        \"proj_head\": proj_head,\n",
        "        \"metrics_df\": metrics_df,\n",
        "        \"run_dir\": run_dir\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arxbGvUp4xeY"
      },
      "source": [
        "#### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "vJYiEl204xP4",
        "outputId": "6c5a75fb-b611-48e1-d1b6-8eaa1a19cefe"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'run_ssl_pipeline_kfold_with_rich_features' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2242386338.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m results = run_ssl_pipeline_kfold_with_rich_features(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlabeled_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabeled_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0munlabelled_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/DSA_Comp/Lip Images\"\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m# or path to unlabeled images for SSL pretraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mssl_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mssl_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'run_ssl_pipeline_kfold_with_rich_features' is not defined"
          ]
        }
      ],
      "source": [
        "results = run_ssl_pipeline_kfold_with_rich_features(\n",
        "    labeled_df=labeled_df,\n",
        "    unlabelled_dir=\"/content/drive/MyDrive/DSA_Comp/Lip Images\",        # or path to unlabeled images for SSL pretraining\n",
        "    ssl_epochs=20,\n",
        "    ssl_batch=8,\n",
        "    fine_tune_backbone=True,\n",
        "    fine_tune_epochs=10,\n",
        "    use_metadata=False,\n",
        "    optuna_trials=20,\n",
        "    run_base_dir=\"models\",\n",
        "    n_splits=5\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7mHJh0NoE2q"
      },
      "source": [
        "### Multi Task Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4np5webvoFNw",
        "outputId": "e0c6ddd1-8220-4f05-b0e6-610fd3077f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import pillow_heif\n",
        "pillow_heif.register_heif_opener()\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration / Reproducibility\n",
        "# ---------------------------\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Utilities\n",
        "# ---------------------------\n",
        "def make_run_dir(base=\"models\", prefix=\"run\"):\n",
        "    ts = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    run_dir = os.path.join(base, f\"{prefix}_{ts}\")\n",
        "    os.makedirs(run_dir, exist_ok=True)\n",
        "    return run_dir\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset Classes\n",
        "# ---------------------------\n",
        "class HbImageDataset(Dataset):\n",
        "    \"\"\"Labeled dataset for SSL (ignores Hb target during SSL)\"\"\"\n",
        "    def __init__(self, df, transform=None, path_col=\"Filename\", target_col=\"Hb\", n_views=2):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.path_col = path_col\n",
        "        self.target_col = target_col\n",
        "        self.n_views = n_views\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[self.path_col]).convert(\"RGB\")\n",
        "        views = [self.transform(img) for _ in range(self.n_views)]\n",
        "        return torch.stack(views), torch.tensor(-1), True  # True indicates labeled\n",
        "\n",
        "class CombinedSSLImageDataset(Dataset):\n",
        "    \"\"\"Combines labeled and unlabeled datasets for SSL pretraining\"\"\"\n",
        "    def __init__(self, labeled_df=None, unlabelled_dir=None, transform=None, n_views=2):\n",
        "        self.labeled_dataset = HbImageDataset(labeled_df, transform=transform, n_views=n_views) if labeled_df is not None else None\n",
        "\n",
        "        # Unlabeled images\n",
        "        self.unlabelled_paths, self.unlabelled_labels = [], []\n",
        "        if unlabelled_dir is not None:\n",
        "            for class_idx, folder_name in enumerate(sorted(os.listdir(unlabelled_dir))):\n",
        "                folder_path = os.path.join(unlabelled_dir, folder_name)\n",
        "                if not os.path.isdir(folder_path):\n",
        "                    continue\n",
        "                for fname in os.listdir(folder_path):\n",
        "                    if fname.lower().endswith((\".jpg\",\".jpeg\",\".png\",\".heic\",\".heif\")):\n",
        "                        self.unlabelled_paths.append(os.path.join(folder_path, fname))\n",
        "                        self.unlabelled_labels.append(class_idx)\n",
        "\n",
        "        self.transform = transform\n",
        "        self.n_views = n_views\n",
        "        self.labelled_len = len(self.labeled_dataset) if self.labeled_dataset else 0\n",
        "        self.unlabelled_len = len(self.unlabelled_paths)\n",
        "        self.total_len = self.labelled_len + self.unlabelled_len\n",
        "\n",
        "        print(f\"[INFO] Total images for SSL: {self.total_len} (Labeled={self.labelled_len}, Unlabeled={self.unlabelled_len})\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx < self.labelled_len:\n",
        "            return self.labeled_dataset[idx]\n",
        "        else:\n",
        "            un_idx = idx - self.labelled_len\n",
        "            img_path = self.unlabelled_paths[un_idx]\n",
        "            label = self.unlabelled_labels[un_idx]\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            views = torch.stack([self.transform(img) for _ in range(self.n_views)])\n",
        "            return views, torch.tensor(label), False\n",
        "\n",
        "# ---------------------------\n",
        "# Transforms\n",
        "# ---------------------------\n",
        "ssl_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8,1.0)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# Backbone and Heads\n",
        "# ---------------------------\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, input_dim=512, hidden_dim=256, output_dim=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, input_dim=512, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "def get_ssl_backbone(pretrained=True):\n",
        "    resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "    return backbone.to(device)\n",
        "\n",
        "# ---------------------------\n",
        "# NT-Xent Loss\n",
        "# ---------------------------\n",
        "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
        "    z_i = nn.functional.normalize(z_i, dim=1)\n",
        "    z_j = nn.functional.normalize(z_j, dim=1)\n",
        "    batch_size = z_i.size(0)\n",
        "    reps = torch.cat([z_i, z_j], dim=0)\n",
        "    sim_matrix = torch.matmul(reps, reps.T)/temperature\n",
        "    mask = torch.eye(2*batch_size, device=z_i.device).bool()\n",
        "    sim_matrix = sim_matrix.masked_fill(mask, -1e9)\n",
        "    labels = torch.arange(batch_size, device=z_i.device)\n",
        "    labels = torch.cat([labels+batch_size, labels], dim=0)\n",
        "    return nn.CrossEntropyLoss()(sim_matrix, labels)\n",
        "\n",
        "# ---------------------------\n",
        "# SSL Pretraining Function\n",
        "# ---------------------------\n",
        "def pretrain_ssl_with_aux(labeled_df=None, unlabelled_dir=None, transform=ssl_transform,\n",
        "                          epochs=20, batch_size=8, lr=1e-3, run_dir=None):\n",
        "\n",
        "    dataset = CombinedSSLImageDataset(labeled_df, unlabelled_dir, transform=transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    backbone = get_ssl_backbone(pretrained=True)\n",
        "    proj_head = ProjectionHead().to(device)\n",
        "    class_head = ClassificationHead().to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(list(backbone.parameters()) + list(proj_head.parameters()) + list(class_head.parameters()), lr=lr)\n",
        "\n",
        "    backbone.train(); proj_head.train(); class_head.train()\n",
        "    ssl_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for views, aux_labels, is_labeled in loader:\n",
        "            v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "            feats1, feats2 = backbone(v1).view(v1.size(0),-1), backbone(v2).view(v2.size(0),-1)\n",
        "            z1, z2 = proj_head(feats1), proj_head(feats2)\n",
        "            loss_ntxent = nt_xent_loss(z1, z2)\n",
        "\n",
        "            mask_unlabeled = (~is_labeled).bool()\n",
        "            if mask_unlabeled.any():\n",
        "                feats_unlabeled = feats1[mask_unlabeled.to(device)]\n",
        "                labels_unlabeled = aux_labels[mask_unlabeled].to(device)\n",
        "                logits = class_head(feats_unlabeled)\n",
        "                loss_aux = nn.CrossEntropyLoss()(logits, labels_unlabeled)\n",
        "            else:\n",
        "                loss_aux = 0.0\n",
        "\n",
        "            loss = loss_ntxent + 0.5 * loss_aux\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss/len(loader)\n",
        "        ssl_losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - SSL Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save models & losses\n",
        "    if run_dir:\n",
        "        os.makedirs(run_dir, exist_ok=True)\n",
        "        torch.save(backbone.state_dict(), os.path.join(run_dir,\"ssl_backbone.pth\"))\n",
        "        torch.save(proj_head.state_dict(), os.path.join(run_dir,\"ssl_proj_head.pth\"))\n",
        "        torch.save(class_head.state_dict(), os.path.join(run_dir,\"ssl_class_head.pth\"))\n",
        "        pd.DataFrame({\"ssl_loss\": ssl_losses}).to_csv(os.path.join(run_dir,\"ssl_loss_history.csv\"), index=False)\n",
        "\n",
        "    return backbone, proj_head, class_head, ssl_losses\n",
        "\n",
        "# ---------------------------\n",
        "# Embedding Visualization\n",
        "# ---------------------------\n",
        "def visualize_embeddings(backbone, dataset, run_dir, n_samples=500):\n",
        "    backbone.eval()\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "    all_feats, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for views, aux_labels, is_labeled in loader:\n",
        "            v = views[:,0].to(device)\n",
        "            feats = backbone(v).view(v.size(0), -1)\n",
        "            all_feats.append(feats.cpu().numpy())\n",
        "            all_labels.append(aux_labels.numpy())\n",
        "\n",
        "    feats = np.concatenate(all_feats, axis=0)[:n_samples]\n",
        "    labels = np.concatenate(all_labels, axis=0)[:n_samples]\n",
        "\n",
        "    # PCA\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_emb = pca.fit_transform(feats)\n",
        "\n",
        "    # t-SNE\n",
        "    tsne = TSNE(n_components=2, random_state=RANDOM_SEED)\n",
        "    tsne_emb = tsne.fit_transform(feats)\n",
        "\n",
        "    # Plot function\n",
        "    def plot_emb(emb, title, fname):\n",
        "        plt.figure(figsize=(6,6))\n",
        "        sns.scatterplot(x=emb[:,0], y=emb[:,1], hue=labels, palette=\"Set1\", legend=\"full\")\n",
        "        plt.title(title)\n",
        "        plt.xlabel(\"Dim 1\"); plt.ylabel(\"Dim 2\")\n",
        "        plt.savefig(os.path.join(run_dir, fname))\n",
        "        plt.close()\n",
        "\n",
        "    plot_emb(pca_emb, \"PCA Embeddings\", \"pca_embeddings.png\")\n",
        "    plot_emb(tsne_emb, \"t-SNE Embeddings\", \"tsne_embeddings.png\")\n",
        "    print(f\"[INFO] Embedding plots saved in {run_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7o6v-IwoMqG"
      },
      "source": [
        "#### Run Multi Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wfm5CR9oMcL",
        "outputId": "3048faca-49ad-4503-fb0f-60a9a3ffd8a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Total images for SSL: 741 (Labeled=31, Unlabeled=710)\n",
            "Epoch 1/20 - SSL Loss: 1.9677\n",
            "Epoch 2/20 - SSL Loss: 1.6197\n",
            "Epoch 3/20 - SSL Loss: 1.6028\n",
            "Epoch 4/20 - SSL Loss: 1.5310\n",
            "Epoch 5/20 - SSL Loss: 1.5447\n",
            "Epoch 6/20 - SSL Loss: 1.5219\n",
            "Epoch 7/20 - SSL Loss: 1.5114\n",
            "Epoch 8/20 - SSL Loss: 1.4577\n",
            "Epoch 9/20 - SSL Loss: 1.4581\n",
            "Epoch 10/20 - SSL Loss: 1.4110\n",
            "Epoch 11/20 - SSL Loss: 1.4181\n",
            "Epoch 12/20 - SSL Loss: 1.3995\n",
            "Epoch 13/20 - SSL Loss: 1.4322\n",
            "Epoch 14/20 - SSL Loss: 1.4089\n",
            "Epoch 15/20 - SSL Loss: 1.3952\n",
            "Epoch 16/20 - SSL Loss: 1.3899\n",
            "Epoch 17/20 - SSL Loss: 1.3760\n",
            "Epoch 18/20 - SSL Loss: 1.3998\n",
            "Epoch 19/20 - SSL Loss: 1.4220\n",
            "Epoch 20/20 - SSL Loss: 1.4262\n",
            "[INFO] Total images for SSL: 741 (Labeled=31, Unlabeled=710)\n",
            "[INFO] Embedding plots saved in ssl_models/ssl_hb_2025-10-07_12-26-33\n"
          ]
        }
      ],
      "source": [
        "run_dir = make_run_dir(\"ssl_models\", \"ssl_hb\")\n",
        "backbone, proj_head, class_head, ssl_losses = pretrain_ssl_with_aux(\n",
        "    labeled_df=df,\n",
        "    unlabelled_dir=\"/content/drive/MyDrive/DSA_Comp/Lip Images\",\n",
        "    epochs=20,\n",
        "    batch_size=8,\n",
        "    run_dir=run_dir\n",
        ")\n",
        "\n",
        "# Visualize embeddings\n",
        "dataset = CombinedSSLImageDataset(labeled_df=df, unlabelled_dir=\"/content/drive/MyDrive/DSA_Comp/Lip Images\", transform=ssl_transform)\n",
        "visualize_embeddings(backbone, dataset, run_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GjjSUrwFMSB"
      },
      "source": [
        "### Semi Supervised Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tArxnadFIAp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "backbone_output_dim = 512\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Semi-Supervised Fine-Tune Head\n",
        "# -----------------------------\n",
        "class HbRegressionHead(nn.Module):\n",
        "    \"\"\"Regression head to predict Hb from backbone embeddings\"\"\"\n",
        "    def __init__(self, backbone_output_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(backbone_output_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# -----------------------------\n",
        "# Main K-Fold SSL + Semi-supervised Pipeline\n",
        "# -----------------------------\n",
        "def run_ssl_pipeline_kfold_semi(\n",
        "    labeled_df,\n",
        "    unlabelled_dir=None,\n",
        "    ssl_epochs=20,\n",
        "    ssl_batch=8,\n",
        "    fine_tune_backbone=True,\n",
        "    semi_supervised=True,\n",
        "    use_metadata=False,\n",
        "    optuna_trials=20,\n",
        "    run_base_dir=\"models\",\n",
        "    load_run_dir=None,\n",
        "    n_splits=5,\n",
        "    fine_tune_epochs=2,\n",
        "    ssl_transform=None,\n",
        "    val_transform=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    SSL + Semi-Supervised Fine-Tuning + K-Fold CV + XGBoost evaluation.\n",
        "\n",
        "    Returns backbone, regression head, RMSE, MAE, run_dir.\n",
        "    \"\"\"\n",
        "    run_dir = load_run_dir if load_run_dir else make_run_dir(run_base_dir)\n",
        "    print(f\"Run directory: {run_dir}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Step 1: SSL Pretraining or Load\n",
        "    # -------------------\n",
        "    backbone = get_ssl_backbone(pretrained=False)\n",
        "    proj_head = ProjectionHead().to(device)\n",
        "\n",
        "    if load_run_dir and os.path.exists(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\")):\n",
        "        backbone.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"), map_location=device))\n",
        "        proj_head.load_state_dict(torch.load(os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"), map_location=device))\n",
        "        backbone.to(device).eval()\n",
        "        proj_head.to(device).eval()\n",
        "        print(\"Loaded pretrained backbone and projection head from saved run.\")\n",
        "    else:\n",
        "        print(\"=== SSL Pretraining ===\")\n",
        "        if unlabelled_dir:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(\n",
        "                unlabelled_dir, ssl_transform,\n",
        "                epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                run_dir=run_dir, unlabelled=True\n",
        "            )\n",
        "        else:\n",
        "            backbone, proj_head, ssl_losses = pretrain_ssl(\n",
        "                labeled_df, ssl_transform,\n",
        "                epochs=ssl_epochs, batch_size=ssl_batch,\n",
        "                run_dir=run_dir, unlabelled=False\n",
        "            )\n",
        "\n",
        "    # -------------------\n",
        "    # K-Fold CV\n",
        "    # -------------------\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(labeled_df)):\n",
        "        print(f\"\\n--- K-Fold {fold_idx+1}/{n_splits} ---\")\n",
        "        train_df, test_df = labeled_df.iloc[train_idx], labeled_df.iloc[test_idx]\n",
        "\n",
        "        # -------------------\n",
        "        # Semi-supervised fine-tuning on train fold\n",
        "        # -------------------\n",
        "        if fine_tune_backbone:\n",
        "            dataset = HbImageDataset(train_df, transform=ssl_transform, n_views=2)\n",
        "            loader = DataLoader(dataset, batch_size=ssl_batch, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "            regression_head = HbRegressionHead(backbone_output_dim=backbone_output_dim).to(device)\n",
        "            backbone.train()\n",
        "            proj_head.train()\n",
        "            regression_head.train()\n",
        "            optimizer = torch.optim.Adam(\n",
        "                list(backbone.parameters()) + list(proj_head.parameters()) + list(regression_head.parameters()),\n",
        "                lr=1e-4\n",
        "            )\n",
        "\n",
        "            for epoch in range(fine_tune_epochs):\n",
        "                total_loss = 0.0\n",
        "                for views, hb in loader:\n",
        "                    v1, v2 = views[:,0].to(device), views[:,1].to(device)\n",
        "                    hb = hb.to(device)\n",
        "\n",
        "                    # SSL contrastive embeddings\n",
        "                    feats1, feats2 = backbone(v1), backbone(v2)\n",
        "                    z1, z2 = proj_head(feats1), proj_head(feats2)\n",
        "                    ssl_loss = nt_xent_loss(z1, z2)\n",
        "\n",
        "                    # Hb regression loss\n",
        "                    if semi_supervised:\n",
        "                        hb_pred = regression_head(feats1)\n",
        "                        hb_loss = nn.L1Loss()(hb_pred.squeeze(), hb)\n",
        "                        loss = 0.5 * ssl_loss + 1.0 * hb_loss\n",
        "                    else:\n",
        "                        loss = ssl_loss\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                print(f\"Fold {fold_idx+1} Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "        # -------------------\n",
        "        # Extract embeddings\n",
        "        # -------------------\n",
        "        X_train, y_train = extract_embeddings(train_df, backbone, val_transform)\n",
        "        X_test, y_test = extract_embeddings(test_df, backbone, val_transform)\n",
        "\n",
        "        if use_metadata:\n",
        "            X_train = combine_metadata(X_train, train_df)\n",
        "            X_test = combine_metadata(X_test, test_df)\n",
        "\n",
        "        # -------------------\n",
        "        # Optuna hyperparameter tuning\n",
        "        # -------------------\n",
        "        def objective(trial):\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\",50,300),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\",2,10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\",0.01,0.3,log=True),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\",0.5,1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\",0.5,1.0),\n",
        "                \"random_state\": RANDOM_SEED,\n",
        "                \"verbosity\": 0,\n",
        "                \"n_jobs\": 1,\n",
        "                \"tree_method\": \"hist\"\n",
        "            }\n",
        "            kf_inner = KFold(n_splits=min(5, len(train_df)), shuffle=True, random_state=RANDOM_SEED)\n",
        "            maes = []\n",
        "            for tr_idx, val_idx in kf_inner.split(X_train):\n",
        "                model = xgb.XGBRegressor(**params)\n",
        "                model.fit(X_train[tr_idx], y_train[tr_idx])\n",
        "                y_pred = model.predict(X_train[val_idx])\n",
        "                maes.append(mean_absolute_error(y_train[val_idx], y_pred))\n",
        "            return np.mean(maes)\n",
        "\n",
        "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "        study.optimize(objective, n_trials=optuna_trials, show_progress_bar=False)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best params fold {fold_idx+1}: {best_params}\")\n",
        "\n",
        "        # -------------------\n",
        "        # Train final XGBoost on train fold\n",
        "        # -------------------\n",
        "        final_model = xgb.XGBRegressor(**best_params, random_state=RANDOM_SEED, n_jobs=-1, tree_method=\"hist\")\n",
        "        final_model.fit(X_train, y_train)\n",
        "\n",
        "        # -------------------\n",
        "        # Predict on test fold\n",
        "        # -------------------\n",
        "        y_pred = final_model.predict(X_test)\n",
        "        all_preds.extend(y_pred)\n",
        "        all_targets.extend(y_test)\n",
        "\n",
        "    # -------------------\n",
        "    # Evaluation\n",
        "    # -------------------\n",
        "    all_preds, all_targets = np.array(all_preds), np.array(all_targets)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "    print(f\"\\n=== K-Fold Evaluation ===\\nRMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
        "\n",
        "    # -------------------\n",
        "    # Save artifacts\n",
        "    # -------------------\n",
        "    pd.DataFrame({\"y_true\": all_targets, \"y_pred\": all_preds}).to_csv(os.path.join(run_dir, \"kfold_predictions.csv\"), index=False)\n",
        "    pd.DataFrame({\"RMSE\":[rmse], \"MAE\":[mae]}).to_csv(os.path.join(run_dir, \"evaluation_metrics.csv\"), index=False)\n",
        "    torch.save(backbone.state_dict(), os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"))\n",
        "    torch.save(proj_head.state_dict(), os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"))\n",
        "    torch.save(regression_head.state_dict(), os.path.join(run_dir, \"regression_head_state_dict.pth\"))\n",
        "\n",
        "    return {\n",
        "        \"backbone\": backbone,\n",
        "        \"proj_head\": proj_head,\n",
        "        \"regression_head\": regression_head,\n",
        "        \"rmse\": rmse,\n",
        "        \"mae\": mae,\n",
        "        \"run_dir\": run_dir\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47cIGKixFfN9"
      },
      "source": [
        "#### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "IqWtmUG3FeKh",
        "outputId": "bdc27901-7262-4dfb-81be-216cca666117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run directory: /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\n",
            "Loaded pretrained backbone and projection head from saved run.\n",
            "\n",
            "--- K-Fold 1/5 ---\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (4096x1 and 512x256)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2330874188.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loaded_results = run_ssl_pipeline_kfold_semi(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mlabeled_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0munlabelled_dir\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/DSA_Comp/Lip Images\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mload_run_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mssl_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mssl_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3948955845.py\u001b[0m in \u001b[0;36mrun_ssl_pipeline_kfold_semi\u001b[0;34m(labeled_df, unlabelled_dir, ssl_epochs, ssl_batch, fine_tune_backbone, semi_supervised, use_metadata, optuna_trials, run_base_dir, load_run_dir, n_splits, fine_tune_epochs, ssl_transform, val_transform)\u001b[0m\n\u001b[1;32m    117\u001b[0m                     \u001b[0;31m# SSL contrastive embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0mfeats1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                     \u001b[0mssl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnt_xent_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2596625780.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_ssl_backbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4096x1 and 512x256)"
          ]
        }
      ],
      "source": [
        "loaded_results = run_ssl_pipeline_kfold_semi(\n",
        "    labeled_df=df,\n",
        "    unlabelled_dir= \"/content/drive/MyDrive/DSA_Comp/Lip Images\",\n",
        "    load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_740\",\n",
        "    ssl_transform=ssl_transform,\n",
        ")\n",
        "\n",
        "print(\"Loaded RMSE:\", loaded_results[\"rmse\"])\n",
        "print(\"Loaded MAE:\", loaded_results[\"mae\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiWW7daUKp1i"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae28QGPIKq-J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset\n",
        "# ---------------------------\n",
        "class HbImageDataset(Dataset):\n",
        "    \"\"\"Simple labeled dataset for supervised learning.\"\"\"\n",
        "    def __init__(self, df, transform=None, path_col=\"Filename\", target_col=\"Hb\"):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.path_col = path_col\n",
        "        self.target_col = target_col\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[self.path_col]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        target = torch.tensor(row[self.target_col], dtype=torch.float32)\n",
        "        return img, target\n",
        "\n",
        "# ---------------------------\n",
        "# Transforms\n",
        "# ---------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# Simple ResNet-based regressor\n",
        "# ---------------------------\n",
        "class HbRegressor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.head = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x).view(x.size(0), -1)\n",
        "        return self.head(x)\n",
        "\n",
        "# ---------------------------\n",
        "# Baseline K-Fold training\n",
        "# ---------------------------\n",
        "def run_baseline_kfold(labeled_df, n_splits=5, batch_size=8, epochs=10, lr=1e-4):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(labeled_df)):\n",
        "        print(f\"\\n--- Fold {fold_idx+1}/{n_splits} ---\")\n",
        "        train_df, val_df = labeled_df.iloc[train_idx], labeled_df.iloc[val_idx]\n",
        "\n",
        "        train_dataset = HbImageDataset(train_df, transform=train_transform)\n",
        "        val_dataset = HbImageDataset(val_df, transform=val_transform)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "        model = HbRegressor(pretrained=True).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        criterion = nn.MSELoss()  # Baseline regression loss\n",
        "\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for imgs, targets in train_loader:\n",
        "                imgs, targets = imgs.to(device), targets.to(device)\n",
        "                preds = model(imgs).squeeze(1)\n",
        "                loss = criterion(preds, targets)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item() * imgs.size(0)\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader.dataset):.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        fold_preds, fold_targets = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, targets in val_loader:\n",
        "                imgs, targets = imgs.to(device), targets.to(device)\n",
        "                preds = model(imgs).squeeze(1)\n",
        "                fold_preds.extend(preds.cpu().numpy())\n",
        "                fold_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "        all_preds.extend(fold_preds)\n",
        "        all_targets.extend(fold_targets)\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "    print(f\"\\n=== Baseline K-Fold Results ===\\nRMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
        "\n",
        "    return rmse, mae, all_preds, all_targets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJOiunYHLEHJ"
      },
      "source": [
        "### Run Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8dLOBghKq1I",
        "outputId": "409627f3-af1d-4ec1-c4ba-c9d4a678ed8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 1/5 ---\n",
            "Epoch 1/10 - Loss: 112.3794\n",
            "Epoch 2/10 - Loss: 102.3416\n",
            "Epoch 3/10 - Loss: 97.3596\n",
            "Epoch 4/10 - Loss: 91.0890\n",
            "Epoch 5/10 - Loss: 86.7641\n",
            "Epoch 6/10 - Loss: 83.0270\n",
            "Epoch 7/10 - Loss: 78.6522\n",
            "Epoch 8/10 - Loss: 73.1325\n",
            "Epoch 9/10 - Loss: 69.0567\n",
            "Epoch 10/10 - Loss: 65.7167\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Epoch 1/10 - Loss: 137.4318\n",
            "Epoch 2/10 - Loss: 127.8954\n",
            "Epoch 3/10 - Loss: 118.9164\n",
            "Epoch 4/10 - Loss: 113.7754\n",
            "Epoch 5/10 - Loss: 106.5062\n",
            "Epoch 6/10 - Loss: 102.0925\n",
            "Epoch 7/10 - Loss: 95.0964\n",
            "Epoch 8/10 - Loss: 91.7221\n",
            "Epoch 9/10 - Loss: 86.0973\n",
            "Epoch 10/10 - Loss: 80.2952\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Epoch 1/10 - Loss: 148.7835\n",
            "Epoch 2/10 - Loss: 136.5297\n",
            "Epoch 3/10 - Loss: 128.5645\n",
            "Epoch 4/10 - Loss: 121.2703\n",
            "Epoch 5/10 - Loss: 116.2511\n",
            "Epoch 6/10 - Loss: 107.3866\n",
            "Epoch 7/10 - Loss: 103.1872\n",
            "Epoch 8/10 - Loss: 96.8651\n",
            "Epoch 9/10 - Loss: 89.8014\n",
            "Epoch 10/10 - Loss: 82.3832\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Epoch 1/10 - Loss: 136.1932\n",
            "Epoch 2/10 - Loss: 126.7223\n",
            "Epoch 3/10 - Loss: 116.9806\n",
            "Epoch 4/10 - Loss: 111.6444\n",
            "Epoch 5/10 - Loss: 103.9445\n",
            "Epoch 6/10 - Loss: 99.1403\n",
            "Epoch 7/10 - Loss: 91.5969\n",
            "Epoch 8/10 - Loss: 87.3973\n",
            "Epoch 9/10 - Loss: 83.0052\n",
            "Epoch 10/10 - Loss: 76.3845\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Epoch 1/10 - Loss: 153.9948\n",
            "Epoch 2/10 - Loss: 143.3563\n",
            "Epoch 3/10 - Loss: 134.6450\n",
            "Epoch 4/10 - Loss: 129.0547\n",
            "Epoch 5/10 - Loss: 119.8074\n",
            "Epoch 6/10 - Loss: 113.2822\n",
            "Epoch 7/10 - Loss: 113.6941\n",
            "Epoch 8/10 - Loss: 99.7410\n",
            "Epoch 9/10 - Loss: 94.5834\n",
            "Epoch 10/10 - Loss: 87.9331\n",
            "\n",
            "=== Baseline K-Fold Results ===\n",
            "RMSE: 8.9568 | MAE: 8.5442\n"
          ]
        }
      ],
      "source": [
        "# Run the baseline\n",
        "rmse, mae, preds, targets = run_baseline_kfold(\n",
        "    labeled_df=df,\n",
        "    n_splits=5,       # 5-fold CV\n",
        "    batch_size=8,     # adjust if memory limited\n",
        "    epochs=10,        # you can increase to 20-30 for better results\n",
        "    lr=1e-4           # learning rate\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8XzBp5_zd3U"
      },
      "source": [
        "#### Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTHxnYMYzghS",
        "outputId": "344730dc-0b8e-4476-96bc-469253b4c8dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading models and artifacts from: /content/drive/MyDrive/DSA_Comp/ssl_models/ssl_combined\n",
            "Loaded backbone and projection head from saved run.\n",
            "=== Extracting embeddings ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:34:44,587] A new study created in memory with name: no-name-23d03952-dbc4-4f24-9fef-635c732203b5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (31, 513)\n",
            "=== Optuna Hyperparameter Tuning ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 08:34:46,912] Trial 0 finished with value: 2.52391272061046 and parameters: {'n_estimators': 218, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.52391272061046.\n",
            "[I 2025-10-07 08:34:47,559] Trial 1 finished with value: 2.276321439450648 and parameters: {'n_estimators': 120, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.276321439450648.\n",
            "[I 2025-10-07 08:34:48,187] Trial 2 finished with value: 2.6511290631156266 and parameters: {'n_estimators': 59, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 2.276321439450648.\n",
            "[I 2025-10-07 08:34:49,302] Trial 3 finished with value: 2.2532069096061695 and parameters: {'n_estimators': 132, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:34:51,369] Trial 4 finished with value: 2.293637808791032 and parameters: {'n_estimators': 325, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:34:53,276] Trial 5 finished with value: 2.2916370963911232 and parameters: {'n_estimators': 404, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:34:56,041] Trial 6 finished with value: 2.603615553898119 and parameters: {'n_estimators': 324, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:01,973] Trial 7 finished with value: 2.334209334334911 and parameters: {'n_estimators': 414, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:03,498] Trial 8 finished with value: 2.525692851983661 and parameters: {'n_estimators': 105, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:05,909] Trial 9 finished with value: 2.282957915761802 and parameters: {'n_estimators': 348, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:08,485] Trial 10 finished with value: 2.356653235997862 and parameters: {'n_estimators': 214, 'max_depth': 7, 'learning_rate': 0.030315725522749647, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8285618345363206}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:09,457] Trial 11 finished with value: 2.704861255266987 and parameters: {'n_estimators': 150, 'max_depth': 2, 'learning_rate': 0.27750763568100384, 'subsample': 0.6673975881116181, 'colsample_bytree': 0.8582590089757371}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:13,023] Trial 12 finished with value: 2.444025502912778 and parameters: {'n_estimators': 174, 'max_depth': 5, 'learning_rate': 0.11020070455466847, 'subsample': 0.8683766692535109, 'colsample_bytree': 0.8443070778541023}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:13,459] Trial 13 finished with value: 3.0116922503022736 and parameters: {'n_estimators': 59, 'max_depth': 2, 'learning_rate': 0.2939802561264774, 'subsample': 0.7005754115147793, 'colsample_bytree': 0.9469561526267276}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:16,020] Trial 14 finished with value: 2.4520886225372065 and parameters: {'n_estimators': 254, 'max_depth': 8, 'learning_rate': 0.07937357644840533, 'subsample': 0.8947385331597324, 'colsample_bytree': 0.6738167152482913}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:21,232] Trial 15 finished with value: 2.3182415391984565 and parameters: {'n_estimators': 485, 'max_depth': 5, 'learning_rate': 0.03208899494279826, 'subsample': 0.5946502169162168, 'colsample_bytree': 0.7937752016696815}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:21,927] Trial 16 finished with value: 2.3876280208478176 and parameters: {'n_estimators': 136, 'max_depth': 2, 'learning_rate': 0.18071775553231698, 'subsample': 0.7305538214659407, 'colsample_bytree': 0.9047894591765572}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:22,847] Trial 17 finished with value: 2.5036527255329544 and parameters: {'n_estimators': 103, 'max_depth': 4, 'learning_rate': 0.02068086816741835, 'subsample': 0.6098754041173404, 'colsample_bytree': 0.7774685121591056}. Best is trial 3 with value: 2.2532069096061695.\n",
            "[I 2025-10-07 08:35:25,922] Trial 18 finished with value: 2.2283985277188987 and parameters: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.04104975015072629, 'subsample': 0.7367194768156402, 'colsample_bytree': 0.6758582224857361}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:29,210] Trial 19 finished with value: 2.283581638108521 and parameters: {'n_estimators': 271, 'max_depth': 7, 'learning_rate': 0.0412636853778724, 'subsample': 0.6452700111123667, 'colsample_bytree': 0.675760829066606}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:30,973] Trial 20 finished with value: 2.318075612302189 and parameters: {'n_estimators': 194, 'max_depth': 6, 'learning_rate': 0.07942383092448925, 'subsample': 0.7233729064226975, 'colsample_bytree': 0.5023700598265446}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:32,144] Trial 21 finished with value: 2.3721939638795333 and parameters: {'n_estimators': 114, 'max_depth': 5, 'learning_rate': 0.041698819090049646, 'subsample': 0.8302996229142291, 'colsample_bytree': 0.6740456018789517}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:33,243] Trial 22 finished with value: 2.3777585512970996 and parameters: {'n_estimators': 167, 'max_depth': 3, 'learning_rate': 0.08242047873638257, 'subsample': 0.7451151345263023, 'colsample_bytree': 0.7355961029422885}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:35,195] Trial 23 finished with value: 2.376647635571607 and parameters: {'n_estimators': 234, 'max_depth': 4, 'learning_rate': 0.0188579634518934, 'subsample': 0.8989734577225666, 'colsample_bytree': 0.6391686706406166}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:36,227] Trial 24 finished with value: 2.488001797497752 and parameters: {'n_estimators': 82, 'max_depth': 5, 'learning_rate': 0.19558986295656183, 'subsample': 0.7581121313357125, 'colsample_bytree': 0.9033256512985252}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:38,615] Trial 25 finished with value: 2.3516871116796456 and parameters: {'n_estimators': 139, 'max_depth': 6, 'learning_rate': 0.12651346670467387, 'subsample': 0.8082305697104212, 'colsample_bytree': 0.7712752020045126}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:39,852] Trial 26 finished with value: 2.244186428408219 and parameters: {'n_estimators': 188, 'max_depth': 2, 'learning_rate': 0.04485881026741094, 'subsample': 0.5303773315185901, 'colsample_bytree': 0.6990542838143478}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:41,165] Trial 27 finished with value: 2.343775598767375 and parameters: {'n_estimators': 186, 'max_depth': 3, 'learning_rate': 0.04367918305672107, 'subsample': 0.5162635149541891, 'colsample_bytree': 0.7018485391730186}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:43,617] Trial 28 finished with value: 2.2854174101015925 and parameters: {'n_estimators': 250, 'max_depth': 7, 'learning_rate': 0.050919648666641185, 'subsample': 0.568398434703415, 'colsample_bytree': 0.6376087445060156}. Best is trial 18 with value: 2.2283985277188987.\n",
            "[I 2025-10-07 08:35:45,547] Trial 29 finished with value: 2.3129515124537314 and parameters: {'n_estimators': 215, 'max_depth': 9, 'learning_rate': 0.06328900231674149, 'subsample': 0.5442087573661307, 'colsample_bytree': 0.5565757572232559}. Best is trial 18 with value: 2.2283985277188987.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.04104975015072629, 'subsample': 0.7367194768156402, 'colsample_bytree': 0.6758582224857361}\n",
            "RMSE: 0.0919 | MAE: 0.0378\n",
            "Loaded RMSE: 0.09189393613908409\n",
            "Loaded MAE: 0.037828829139471054\n"
          ]
        }
      ],
      "source": [
        "loaded_results = run_ssl_supervised_pipeline(\n",
        "    labeled_df=df,\n",
        "    load_run_dir=\"/content/drive/MyDrive/DSA_Comp/ssl_models/ssl_combined\"\n",
        ")\n",
        "\n",
        "print(\"Loaded RMSE:\", loaded_results[\"rmse\"])\n",
        "print(\"Loaded MAE:\", loaded_results[\"mae\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3qwBsmrL1BM"
      },
      "source": [
        "### Baseline XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo9Ye6R7L0jz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset\n",
        "# ---------------------------\n",
        "class HbImageDataset(Dataset):\n",
        "    \"\"\"Labeled dataset for supervised learning (CNN embeddings  XGBoost).\"\"\"\n",
        "    def __init__(self, df, transform=None, path_col=\"Filename\", target_col=\"Hb\"):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.path_col = path_col\n",
        "        self.target_col = target_col\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[self.path_col]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        target = torch.tensor(row[self.target_col], dtype=torch.float32)\n",
        "        return img, target\n",
        "\n",
        "# ---------------------------\n",
        "# Transforms\n",
        "# ---------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# Pretrained CNN Backbone for Embeddings\n",
        "# ---------------------------\n",
        "def get_cnn_backbone(pretrained=True):\n",
        "    resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    backbone = nn.Sequential(*list(resnet.children())[:-1])  # Remove final FC layer\n",
        "    return backbone\n",
        "\n",
        "def extract_embeddings(df, backbone, transform, batch_size=8):\n",
        "    dataset = HbImageDataset(df, transform=transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    device = next(backbone.parameters()).device\n",
        "    backbone.eval()\n",
        "    embeddings = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, t in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            feats = backbone(imgs).view(imgs.size(0), -1)\n",
        "            embeddings.append(feats.cpu().numpy())\n",
        "            targets.extend(t.numpy())\n",
        "\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    targets = np.array(targets)\n",
        "    return embeddings, targets\n",
        "\n",
        "# ---------------------------\n",
        "# Supervised CNN  XGBoost pipeline with K-Fold\n",
        "# ---------------------------\n",
        "def run_cnn_xgb_kfold(labeled_df, n_splits=5, batch_size=8, xgb_trials=20):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    backbone = get_cnn_backbone(pretrained=True).to(device)\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(labeled_df)):\n",
        "        print(f\"\\n--- Fold {fold_idx+1}/{n_splits} ---\")\n",
        "        train_df, val_df = labeled_df.iloc[train_idx], labeled_df.iloc[val_idx]\n",
        "\n",
        "        # Extract embeddings\n",
        "        X_train, y_train = extract_embeddings(train_df, backbone, train_transform, batch_size)\n",
        "        X_val, y_val = extract_embeddings(val_df, backbone, val_transform, batch_size)\n",
        "\n",
        "        # -------------------\n",
        "        # Hyperparameter tuning via Optuna (minimize MAE)\n",
        "        # -------------------\n",
        "        import optuna\n",
        "        def objective(trial):\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "                \"verbosity\": 0,\n",
        "                \"n_jobs\": 1,\n",
        "                \"tree_method\": \"hist\",\n",
        "                \"random_state\": 42\n",
        "            }\n",
        "            # simple holdout within train for MAE\n",
        "            val_split = int(0.8*len(X_train))\n",
        "            model = xgb.XGBRegressor(**params)\n",
        "            model.fit(X_train[:val_split], y_train[:val_split])\n",
        "            preds = model.predict(X_train[val_split:])\n",
        "            return mean_absolute_error(y_train[val_split:], preds)\n",
        "\n",
        "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
        "        study.optimize(objective, n_trials=xgb_trials, show_progress_bar=False)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Fold {fold_idx+1} Best params: {best_params}\")\n",
        "\n",
        "        # -------------------\n",
        "        # Train final XGBoost model\n",
        "        # -------------------\n",
        "        model = xgb.XGBRegressor(**best_params, random_state=42, n_jobs=-1, tree_method=\"hist\")\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on validation fold\n",
        "        preds = model.predict(X_val)\n",
        "        all_preds.extend(preds)\n",
        "        all_targets.extend(y_val)\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "\n",
        "    print(f\"\\n=== CNN + XGBoost K-Fold Results ===\\nRMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"backbone\": backbone,\n",
        "        \"rmse\": rmse,\n",
        "        \"mae\": mae,\n",
        "        \"preds\": all_preds,\n",
        "        \"targets\": all_targets\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ko_-3SRL6qs"
      },
      "source": [
        "#### Run XGBoost Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TBpBNHT9L6Od",
        "outputId": "614bd5a3-4548-4b18-a7c8-09905f908b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 1/5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 10:21:01,577] A new study created in memory with name: no-name-f92afb87-0f41-4fbe-90f8-3c6cd87bcf0b\n",
            "[I 2025-10-07 10:21:01,797] Trial 0 finished with value: 2.1456875801086426 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.1456875801086426.\n",
            "[I 2025-10-07 10:21:01,872] Trial 1 finished with value: 2.193974256515503 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.1456875801086426.\n",
            "[I 2025-10-07 10:21:01,952] Trial 2 finished with value: 2.3329265117645264 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.1456875801086426.\n",
            "[I 2025-10-07 10:21:02,088] Trial 3 finished with value: 2.143420696258545 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:02,292] Trial 4 finished with value: 2.3376307487487793 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:02,481] Trial 5 finished with value: 2.192864418029785 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:02,750] Trial 6 finished with value: 2.460022211074829 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:03,112] Trial 7 finished with value: 2.565422773361206 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:03,226] Trial 8 finished with value: 2.4605040550231934 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:03,468] Trial 9 finished with value: 2.163404703140259 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:03,676] Trial 10 finished with value: 2.1709721088409424 and parameters: {'n_estimators': 141, 'max_depth': 7, 'learning_rate': 0.030315725522749647, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8285618345363206}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:03,872] Trial 11 finished with value: 2.1828622817993164 and parameters: {'n_estimators': 136, 'max_depth': 10, 'learning_rate': 0.11285320502212905, 'subsample': 0.6673975881116181, 'colsample_bytree': 0.5059728552059523}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:04,131] Trial 12 finished with value: 2.1863646507263184 and parameters: {'n_estimators': 137, 'max_depth': 7, 'learning_rate': 0.10067315756769533, 'subsample': 0.867463702429408, 'colsample_bytree': 0.6712579968688783}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:04,298] Trial 13 finished with value: 2.165759563446045 and parameters: {'n_estimators': 110, 'max_depth': 8, 'learning_rate': 0.28906809046942233, 'subsample': 0.7005754115147793, 'colsample_bytree': 0.7971362539195694}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:04,556] Trial 14 finished with value: 2.4697399139404297 and parameters: {'n_estimators': 164, 'max_depth': 5, 'learning_rate': 0.07937357644840533, 'subsample': 0.8947385331597324, 'colsample_bytree': 0.6514718587565762}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:04,899] Trial 15 finished with value: 2.2695441246032715 and parameters: {'n_estimators': 292, 'max_depth': 9, 'learning_rate': 0.03480403591628763, 'subsample': 0.5946502169162168, 'colsample_bytree': 0.5528283725944974}. Best is trial 3 with value: 2.143420696258545.\n",
            "[I 2025-10-07 10:21:04,991] Trial 16 finished with value: 2.0795085430145264 and parameters: {'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.13905024297763677, 'subsample': 0.7314110483180468, 'colsample_bytree': 0.6930907592254294}. Best is trial 16 with value: 2.0795085430145264.\n",
            "[I 2025-10-07 10:21:05,099] Trial 17 finished with value: 2.702995777130127 and parameters: {'n_estimators': 54, 'max_depth': 5, 'learning_rate': 0.043068629666279584, 'subsample': 0.731057379200564, 'colsample_bytree': 0.9035735976073841}. Best is trial 16 with value: 2.0795085430145264.\n",
            "[I 2025-10-07 10:21:05,251] Trial 18 finished with value: 2.5942654609680176 and parameters: {'n_estimators': 98, 'max_depth': 5, 'learning_rate': 0.019631797854441864, 'subsample': 0.6256091317184369, 'colsample_bytree': 0.713577756031246}. Best is trial 16 with value: 2.0795085430145264.\n",
            "[I 2025-10-07 10:21:05,362] Trial 19 finished with value: 2.262826681137085 and parameters: {'n_estimators': 70, 'max_depth': 6, 'learning_rate': 0.07354205158964185, 'subsample': 0.5054945946218856, 'colsample_bytree': 0.7654412764488479}. Best is trial 16 with value: 2.0795085430145264.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Best params: {'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.13905024297763677, 'subsample': 0.7314110483180468, 'colsample_bytree': 0.6930907592254294}\n",
            "\n",
            "--- Fold 2/5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 10:21:07,950] A new study created in memory with name: no-name-0a97d986-a507-43f0-82d5-a6826b735684\n",
            "[I 2025-10-07 10:21:08,285] Trial 0 finished with value: 3.3055756092071533 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 3.3055756092071533.\n",
            "[I 2025-10-07 10:21:08,424] Trial 1 finished with value: 3.2188477516174316 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:08,557] Trial 2 finished with value: 3.2583541870117188 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:08,746] Trial 3 finished with value: 3.295616865158081 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:09,086] Trial 4 finished with value: 3.6053466796875 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:09,425] Trial 5 finished with value: 3.46703839302063 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:09,839] Trial 6 finished with value: 4.045717239379883 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:10,367] Trial 7 finished with value: 3.529839277267456 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:10,562] Trial 8 finished with value: 3.5122227668762207 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:10,831] Trial 9 finished with value: 3.4242496490478516 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:11,049] Trial 10 finished with value: 4.547146797180176 and parameters: {'n_estimators': 139, 'max_depth': 7, 'learning_rate': 0.27047297227177763, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9168158328299749}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:11,165] Trial 11 finished with value: 3.2876598834991455 and parameters: {'n_estimators': 59, 'max_depth': 10, 'learning_rate': 0.28088381941259366, 'subsample': 0.5899164086425722, 'colsample_bytree': 0.8582590089757371}. Best is trial 1 with value: 3.2188477516174316.\n",
            "[I 2025-10-07 10:21:11,376] Trial 12 finished with value: 3.1823883056640625 and parameters: {'n_estimators': 115, 'max_depth': 8, 'learning_rate': 0.13436119963045842, 'subsample': 0.6427754537375224, 'colsample_bytree': 0.8102888477597007}. Best is trial 12 with value: 3.1823883056640625.\n",
            "[I 2025-10-07 10:21:11,648] Trial 13 finished with value: 3.8278591632843018 and parameters: {'n_estimators': 119, 'max_depth': 8, 'learning_rate': 0.11419445419933459, 'subsample': 0.8944520699149165, 'colsample_bytree': 0.8234945891839655}. Best is trial 12 with value: 3.1823883056640625.\n",
            "[I 2025-10-07 10:21:11,907] Trial 14 finished with value: 3.2040634155273438 and parameters: {'n_estimators': 162, 'max_depth': 8, 'learning_rate': 0.14118748326755173, 'subsample': 0.6570198172973656, 'colsample_bytree': 0.8138214726279749}. Best is trial 12 with value: 3.1823883056640625.\n",
            "[I 2025-10-07 10:21:12,293] Trial 15 finished with value: 3.2271199226379395 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.09894162917596783, 'subsample': 0.6496778672004454, 'colsample_bytree': 0.79007520675116}. Best is trial 12 with value: 3.1823883056640625.\n",
            "[I 2025-10-07 10:21:12,612] Trial 16 finished with value: 4.06210994720459 and parameters: {'n_estimators': 171, 'max_depth': 8, 'learning_rate': 0.03797461786789197, 'subsample': 0.5279156779925589, 'colsample_bytree': 0.9219900508555974}. Best is trial 12 with value: 3.1823883056640625.\n",
            "[I 2025-10-07 10:21:12,883] Trial 17 finished with value: 3.4319519996643066 and parameters: {'n_estimators': 165, 'max_depth': 6, 'learning_rate': 0.08661407781039654, 'subsample': 0.6031965237211039, 'colsample_bytree': 0.7718414708382442}. Best is trial 12 with value: 3.1823883056640625.\n",
            "[I 2025-10-07 10:21:13,075] Trial 18 finished with value: 3.1924145221710205 and parameters: {'n_estimators': 119, 'max_depth': 9, 'learning_rate': 0.17035356242136207, 'subsample': 0.6999903496918574, 'colsample_bytree': 0.6899958868467535}. Best is trial 12 with value: 3.1823883056640625.\n",
            "[I 2025-10-07 10:21:13,282] Trial 19 finished with value: 3.5198211669921875 and parameters: {'n_estimators': 119, 'max_depth': 9, 'learning_rate': 0.08085129291261448, 'subsample': 0.7397636973977778, 'colsample_bytree': 0.6787936308399247}. Best is trial 12 with value: 3.1823883056640625.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Best params: {'n_estimators': 115, 'max_depth': 8, 'learning_rate': 0.13436119963045842, 'subsample': 0.6427754537375224, 'colsample_bytree': 0.8102888477597007}\n",
            "\n",
            "--- Fold 3/5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 10:21:15,789] A new study created in memory with name: no-name-402bef08-0e4c-42e0-99e3-e5982a8723ab\n",
            "[I 2025-10-07 10:21:16,002] Trial 0 finished with value: 3.3055598735809326 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 3.3055598735809326.\n",
            "[I 2025-10-07 10:21:16,081] Trial 1 finished with value: 2.9866271018981934 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 2.9866271018981934.\n",
            "[I 2025-10-07 10:21:16,165] Trial 2 finished with value: 2.9204630851745605 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:16,290] Trial 3 finished with value: 3.534014940261841 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:16,500] Trial 4 finished with value: 3.351940870285034 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:16,702] Trial 5 finished with value: 3.5147621631622314 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:17,014] Trial 6 finished with value: 3.770688533782959 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:17,369] Trial 7 finished with value: 3.9095847606658936 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:17,474] Trial 8 finished with value: 3.7494583129882812 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:17,731] Trial 9 finished with value: 3.3870346546173096 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:17,922] Trial 10 finished with value: 3.3407986164093018 and parameters: {'n_estimators': 146, 'max_depth': 10, 'learning_rate': 0.27047297227177763, 'subsample': 0.5148027085118481, 'colsample_bytree': 0.8259332753890892}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:18,075] Trial 11 finished with value: 3.1267752647399902 and parameters: {'n_estimators': 59, 'max_depth': 8, 'learning_rate': 0.28088381941259366, 'subsample': 0.5899164086425722, 'colsample_bytree': 0.8692027723625105}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:18,298] Trial 12 finished with value: 3.1470963954925537 and parameters: {'n_estimators': 112, 'max_depth': 7, 'learning_rate': 0.13436119963045842, 'subsample': 0.6427754537375224, 'colsample_bytree': 0.8616315620551012}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:18,436] Trial 13 finished with value: 3.6770167350769043 and parameters: {'n_estimators': 52, 'max_depth': 6, 'learning_rate': 0.1361058138823361, 'subsample': 0.8944520699149165, 'colsample_bytree': 0.9608295945492284}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:18,661] Trial 14 finished with value: 3.117192268371582 and parameters: {'n_estimators': 131, 'max_depth': 8, 'learning_rate': 0.1737183744716791, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.7904137304680051}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:18,857] Trial 15 finished with value: 3.093536376953125 and parameters: {'n_estimators': 292, 'max_depth': 2, 'learning_rate': 0.08962717626942573, 'subsample': 0.6274745095999714, 'colsample_bytree': 0.6924366913699725}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:19,022] Trial 16 finished with value: 3.6599621772766113 and parameters: {'n_estimators': 81, 'max_depth': 9, 'learning_rate': 0.032485569301071056, 'subsample': 0.722357282012059, 'colsample_bytree': 0.9176374336571642}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:19,154] Trial 17 finished with value: 3.7589378356933594 and parameters: {'n_estimators': 165, 'max_depth': 6, 'learning_rate': 0.20960605443577704, 'subsample': 0.8791817744657436, 'colsample_bytree': 0.5017405481665318}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:19,360] Trial 18 finished with value: 3.4459564685821533 and parameters: {'n_estimators': 114, 'max_depth': 7, 'learning_rate': 0.10113546182301378, 'subsample': 0.582778140692689, 'colsample_bytree': 0.7710055997151901}. Best is trial 2 with value: 2.9204630851745605.\n",
            "[I 2025-10-07 10:21:19,480] Trial 19 finished with value: 3.3567452430725098 and parameters: {'n_estimators': 72, 'max_depth': 5, 'learning_rate': 0.08045188005091736, 'subsample': 0.6869888528707923, 'colsample_bytree': 0.673351727591049}. Best is trial 2 with value: 2.9204630851745605.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Best params: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}\n",
            "\n",
            "--- Fold 4/5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 10:21:22,446] A new study created in memory with name: no-name-072ff4e9-7042-4a36-8212-124eb1793ac0\n",
            "[I 2025-10-07 10:21:22,816] Trial 0 finished with value: 2.0801196098327637 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.0801196098327637.\n",
            "[I 2025-10-07 10:21:22,932] Trial 1 finished with value: 1.8143589496612549 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:23,063] Trial 2 finished with value: 2.0264151096343994 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:23,258] Trial 3 finished with value: 1.9682738780975342 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:23,626] Trial 4 finished with value: 2.1088919639587402 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:23,942] Trial 5 finished with value: 2.081261157989502 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:24,352] Trial 6 finished with value: 2.5584464073181152 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:24,682] Trial 7 finished with value: 2.349074125289917 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:24,795] Trial 8 finished with value: 2.5726113319396973 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:25,042] Trial 9 finished with value: 2.0863535404205322 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:25,265] Trial 10 finished with value: 2.7054412364959717 and parameters: {'n_estimators': 139, 'max_depth': 7, 'learning_rate': 0.27047297227177763, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9168158328299749}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:25,357] Trial 11 finished with value: 1.9462330341339111 and parameters: {'n_estimators': 105, 'max_depth': 2, 'learning_rate': 0.09572926329940228, 'subsample': 0.6673975881116181, 'colsample_bytree': 0.8265454206124712}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:25,485] Trial 12 finished with value: 2.189727306365967 and parameters: {'n_estimators': 120, 'max_depth': 2, 'learning_rate': 0.11038055977556087, 'subsample': 0.6398620628262128, 'colsample_bytree': 0.8443070778541023}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:25,596] Trial 13 finished with value: 2.713280200958252 and parameters: {'n_estimators': 56, 'max_depth': 6, 'learning_rate': 0.2956173918129623, 'subsample': 0.8944520699149165, 'colsample_bytree': 0.8229282203516206}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:25,714] Trial 14 finished with value: 2.140842914581299 and parameters: {'n_estimators': 161, 'max_depth': 2, 'learning_rate': 0.10763982481121913, 'subsample': 0.5431844609096352, 'colsample_bytree': 0.8138214726279749}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:26,031] Trial 15 finished with value: 2.455275535583496 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.19242000667882905, 'subsample': 0.6037931352962199, 'colsample_bytree': 0.892146309820842}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:26,188] Trial 16 finished with value: 2.0733606815338135 and parameters: {'n_estimators': 105, 'max_depth': 5, 'learning_rate': 0.03072827671186214, 'subsample': 0.7258078387998668, 'colsample_bytree': 0.7771618950778919}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:26,285] Trial 17 finished with value: 1.860629677772522 and parameters: {'n_estimators': 80, 'max_depth': 2, 'learning_rate': 0.08928569305720581, 'subsample': 0.8791817744657436, 'colsample_bytree': 0.9980084946806067}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:26,471] Trial 18 finished with value: 2.387302875518799 and parameters: {'n_estimators': 74, 'max_depth': 8, 'learning_rate': 0.17509626839738895, 'subsample': 0.8852907125720524, 'colsample_bytree': 0.9680547962552339}. Best is trial 1 with value: 1.8143589496612549.\n",
            "[I 2025-10-07 10:21:26,743] Trial 19 finished with value: 2.5083301067352295 and parameters: {'n_estimators': 131, 'max_depth': 5, 'learning_rate': 0.07980862767093125, 'subsample': 0.9019885430838464, 'colsample_bytree': 0.9196740594856193}. Best is trial 1 with value: 1.8143589496612549.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 Best params: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}\n",
            "\n",
            "--- Fold 5/5 ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 10:21:29,061] A new study created in memory with name: no-name-92a03a1f-ad4d-4064-b0e5-e1fb51bee4fb\n",
            "[I 2025-10-07 10:21:29,258] Trial 0 finished with value: 1.6283514499664307 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.6283514499664307.\n",
            "[I 2025-10-07 10:21:29,331] Trial 1 finished with value: 1.705529808998108 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.6283514499664307.\n",
            "[I 2025-10-07 10:21:29,422] Trial 2 finished with value: 1.5926430225372314 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.5926430225372314.\n",
            "[I 2025-10-07 10:21:29,554] Trial 3 finished with value: 1.649303674697876 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.5926430225372314.\n",
            "[I 2025-10-07 10:21:29,761] Trial 4 finished with value: 1.7240419387817383 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.5926430225372314.\n",
            "[I 2025-10-07 10:21:29,962] Trial 5 finished with value: 1.7263374328613281 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.5926430225372314.\n",
            "[I 2025-10-07 10:21:30,259] Trial 6 finished with value: 2.4194507598876953 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 1.5926430225372314.\n",
            "[I 2025-10-07 10:21:30,625] Trial 7 finished with value: 1.8058302402496338 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 1.5926430225372314.\n",
            "[I 2025-10-07 10:21:30,746] Trial 8 finished with value: 1.9155088663101196 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 1.5926430225372314.\n",
            "[I 2025-10-07 10:21:31,004] Trial 9 finished with value: 1.3990490436553955 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 9 with value: 1.3990490436553955.\n",
            "[I 2025-10-07 10:21:31,465] Trial 10 finished with value: 2.101717472076416 and parameters: {'n_estimators': 291, 'max_depth': 7, 'learning_rate': 0.03001215871999436, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8259332753890892}. Best is trial 9 with value: 1.3990490436553955.\n",
            "[I 2025-10-07 10:21:31,616] Trial 11 finished with value: 1.6239163875579834 and parameters: {'n_estimators': 158, 'max_depth': 10, 'learning_rate': 0.2763967942370458, 'subsample': 0.5971603596691708, 'colsample_bytree': 0.5037892523442965}. Best is trial 9 with value: 1.3990490436553955.\n",
            "[I 2025-10-07 10:21:31,811] Trial 12 finished with value: 1.524092674255371 and parameters: {'n_estimators': 120, 'max_depth': 7, 'learning_rate': 0.1068199129169437, 'subsample': 0.6461192327509931, 'colsample_bytree': 0.6362066546548343}. Best is trial 9 with value: 1.3990490436553955.\n",
            "[I 2025-10-07 10:21:32,038] Trial 13 finished with value: 1.7491779327392578 and parameters: {'n_estimators': 129, 'max_depth': 8, 'learning_rate': 0.09540529615221291, 'subsample': 0.6621695088941971, 'colsample_bytree': 0.6789777716246673}. Best is trial 9 with value: 1.3990490436553955.\n",
            "[I 2025-10-07 10:21:32,393] Trial 14 finished with value: 2.1069138050079346 and parameters: {'n_estimators': 196, 'max_depth': 5, 'learning_rate': 0.08096562309896964, 'subsample': 0.8895189437801674, 'colsample_bytree': 0.7994209827157244}. Best is trial 9 with value: 1.3990490436553955.\n",
            "[I 2025-10-07 10:21:32,713] Trial 15 finished with value: 1.756340742111206 and parameters: {'n_estimators': 239, 'max_depth': 8, 'learning_rate': 0.033439619925994035, 'subsample': 0.5970948630542097, 'colsample_bytree': 0.5747844875870822}. Best is trial 9 with value: 1.3990490436553955.\n",
            "[I 2025-10-07 10:21:32,903] Trial 16 finished with value: 1.5555686950683594 and parameters: {'n_estimators': 117, 'max_depth': 6, 'learning_rate': 0.04282743579807444, 'subsample': 0.7350530692394646, 'colsample_bytree': 0.6725403351822288}. Best is trial 9 with value: 1.3990490436553955.\n",
            "[I 2025-10-07 10:21:33,205] Trial 17 finished with value: 2.17339825630188 and parameters: {'n_estimators': 175, 'max_depth': 8, 'learning_rate': 0.02068086816741835, 'subsample': 0.5213237912287488, 'colsample_bytree': 0.9398069633632561}. Best is trial 9 with value: 1.3990490436553955.\n",
            "[I 2025-10-07 10:21:33,529] Trial 18 finished with value: 1.4591600894927979 and parameters: {'n_estimators': 290, 'max_depth': 5, 'learning_rate': 0.13098736285363616, 'subsample': 0.6471208002214496, 'colsample_bytree': 0.7649542846692565}. Best is trial 9 with value: 1.3990490436553955.\n",
            "[I 2025-10-07 10:21:33,827] Trial 19 finished with value: 2.1864185333251953 and parameters: {'n_estimators': 294, 'max_depth': 5, 'learning_rate': 0.14920643617808257, 'subsample': 0.9219016466606671, 'colsample_bytree': 0.8905193220684801}. Best is trial 9 with value: 1.3990490436553955.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 Best params: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}\n",
            "\n",
            "=== CNN + XGBoost K-Fold Results ===\n",
            "RMSE: 3.3126 | MAE: 2.3565\n"
          ]
        }
      ],
      "source": [
        "results = run_cnn_xgb_kfold(\n",
        "    labeled_df=df,      # your DataFrame\n",
        "    n_splits=5,         # 5-Fold CV\n",
        "    batch_size=8,       # batch size for embeddings extraction\n",
        "    xgb_trials=20       # number of Optuna trials to tune XGBoost per fold\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUgB3da-NDth"
      },
      "source": [
        "### Baseline XGBoost + Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5KEZGzUM7i-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import xgboost as xgb\n",
        "\n",
        "# ---------------------------\n",
        "# Dataset\n",
        "# ---------------------------\n",
        "class HbImageDataset(Dataset):\n",
        "    \"\"\"Simple labeled dataset for supervised learning (CNN embeddings  XGBoost).\"\"\"\n",
        "    def __init__(self, df, transform=None, path_col=\"Filename\", target_col=\"Hb\"):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.path_col = path_col\n",
        "        self.target_col = target_col\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[self.path_col]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        target = torch.tensor(row[self.target_col], dtype=torch.float32)\n",
        "        return img, target\n",
        "\n",
        "# ---------------------------\n",
        "# Transforms\n",
        "# ---------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# ---------------------------\n",
        "# Pretrained CNN Backbone\n",
        "# ---------------------------\n",
        "def get_cnn_backbone(pretrained=True):\n",
        "    resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "    backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "    return backbone\n",
        "\n",
        "# ---------------------------\n",
        "# Fine-tune CNN per fold\n",
        "# ---------------------------\n",
        "def fine_tune_backbone(backbone, train_df, transform, device, epochs=2, batch_size=8, lr=1e-4):\n",
        "    dataset = HbImageDataset(train_df, transform=transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    head = nn.Linear(512, 1)\n",
        "    model = nn.Sequential(backbone, nn.Flatten(), head).to(device)  # <-- Move entire model here\n",
        "\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for imgs, targets in loader:\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "            preds = model(imgs).view(-1)\n",
        "            loss = criterion(preds, targets.view(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "        print(f\"Fine-tune Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(loader.dataset):.4f}\")\n",
        "\n",
        "    # Return only fine-tuned backbone weights\n",
        "    return model[0]  # backbone is first element of Sequential\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Extract embeddings\n",
        "# ---------------------------\n",
        "def extract_embeddings(df, backbone, transform, device, batch_size=8):\n",
        "    dataset = HbImageDataset(df, transform=transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    backbone = backbone.to(device)\n",
        "    backbone.eval()\n",
        "    embeddings = []\n",
        "    targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, t in loader:\n",
        "            imgs = imgs.to(device)\n",
        "            feats = backbone(imgs).view(imgs.size(0), -1)\n",
        "            embeddings.append(feats.cpu().numpy())\n",
        "            targets.extend(t.numpy())\n",
        "\n",
        "    embeddings = np.vstack(embeddings)\n",
        "    targets = np.array(targets)\n",
        "    return embeddings, targets\n",
        "\n",
        "# ---------------------------\n",
        "# CNN  XGBoost K-Fold pipeline\n",
        "# ---------------------------\n",
        "def run_cnn_xgb_kfold_finetune(labeled_df, n_splits=5, batch_size=8, fine_tune_epochs=2, xgb_trials=20):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    all_preds, all_targets = [], []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(kf.split(labeled_df)):\n",
        "        print(f\"\\n--- Fold {fold_idx+1}/{n_splits} ---\")\n",
        "        train_df, val_df = labeled_df.iloc[train_idx], labeled_df.iloc[val_idx]\n",
        "\n",
        "        # Initialize fresh backbone per fold to avoid leakage\n",
        "        backbone = get_cnn_backbone(pretrained=True)\n",
        "\n",
        "        # Fine-tune backbone on this fold's training data only\n",
        "        backbone = fine_tune_backbone(backbone, train_df, train_transform, device, epochs=fine_tune_epochs, batch_size=batch_size)\n",
        "\n",
        "        # Extract embeddings\n",
        "        X_train, y_train = extract_embeddings(train_df, backbone, train_transform, device, batch_size)\n",
        "        X_val, y_val = extract_embeddings(val_df, backbone, val_transform, device, batch_size)\n",
        "\n",
        "        # -------------------\n",
        "        # Hyperparameter tuning via Optuna (minimize MAE)\n",
        "        # -------------------\n",
        "        import optuna\n",
        "        def objective(trial):\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "                \"verbosity\": 0,\n",
        "                \"n_jobs\": 1,\n",
        "                \"tree_method\": \"hist\",\n",
        "                \"random_state\": 42\n",
        "            }\n",
        "            val_split = int(0.8*len(X_train))\n",
        "            model = xgb.XGBRegressor(**params)\n",
        "            model.fit(X_train[:val_split], y_train[:val_split])\n",
        "            preds = model.predict(X_train[val_split:])\n",
        "            return mean_absolute_error(y_train[val_split:], preds)\n",
        "\n",
        "        study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=42))\n",
        "        study.optimize(objective, n_trials=xgb_trials, show_progress_bar=False)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Fold {fold_idx+1} Best params: {best_params}\")\n",
        "\n",
        "        # Train final XGBoost model\n",
        "        model = xgb.XGBRegressor(**best_params, random_state=42, n_jobs=-1, tree_method=\"hist\")\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict on validation fold\n",
        "        preds = model.predict(X_val)\n",
        "        all_preds.extend(preds)\n",
        "        all_targets.extend(y_val)\n",
        "\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_targets = np.array(all_targets)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "\n",
        "    print(f\"\\n=== CNN + XGBoost K-Fold with Fine-Tune Results ===\\nRMSE: {rmse:.4f} | MAE: {mae:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"rmse\": rmse,\n",
        "        \"mae\": mae,\n",
        "        \"preds\": all_preds,\n",
        "        \"targets\": all_targets\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYRHXo4MNY1X"
      },
      "source": [
        "#### Run Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcVPd232NBz-",
        "outputId": "ce17e552-26e8-48de-daa4-679228f6573e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 1/5 ---\n",
            "Fine-tune Epoch 1/2 - Loss: 140.2715\n",
            "Fine-tune Epoch 2/2 - Loss: 130.4629\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 10:31:44,201] A new study created in memory with name: no-name-cc89fe00-30fc-4200-ac9f-50cec6d3f4fd\n",
            "[I 2025-10-07 10:31:44,395] Trial 0 finished with value: 1.891453742980957 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.891453742980957.\n",
            "[I 2025-10-07 10:31:44,527] Trial 1 finished with value: 1.9258248805999756 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 1.891453742980957.\n",
            "[I 2025-10-07 10:31:44,712] Trial 2 finished with value: 1.6853649616241455 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 2 with value: 1.6853649616241455.\n",
            "[I 2025-10-07 10:31:44,935] Trial 3 finished with value: 1.7828010320663452 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 2 with value: 1.6853649616241455.\n",
            "[I 2025-10-07 10:31:45,613] Trial 4 finished with value: 1.8188594579696655 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 2 with value: 1.6853649616241455.\n",
            "[I 2025-10-07 10:31:46,351] Trial 5 finished with value: 1.7656177282333374 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 1.6853649616241455.\n",
            "[I 2025-10-07 10:31:48,005] Trial 6 finished with value: 1.7948213815689087 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 2 with value: 1.6853649616241455.\n",
            "[I 2025-10-07 10:31:49,261] Trial 7 finished with value: 1.8878543376922607 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 2 with value: 1.6853649616241455.\n",
            "[I 2025-10-07 10:31:49,479] Trial 8 finished with value: 2.628098487854004 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 2 with value: 1.6853649616241455.\n",
            "[I 2025-10-07 10:31:49,883] Trial 9 finished with value: 1.6563640832901 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 9 with value: 1.6563640832901.\n",
            "[I 2025-10-07 10:31:50,755] Trial 10 finished with value: 2.270198345184326 and parameters: {'n_estimators': 291, 'max_depth': 7, 'learning_rate': 0.03001215871999436, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8259332753890892}. Best is trial 9 with value: 1.6563640832901.\n",
            "[I 2025-10-07 10:31:51,001] Trial 11 finished with value: 2.3146016597747803 and parameters: {'n_estimators': 158, 'max_depth': 10, 'learning_rate': 0.2763967942370458, 'subsample': 0.5971603596691708, 'colsample_bytree': 0.5037892523442965}. Best is trial 9 with value: 1.6563640832901.\n",
            "[I 2025-10-07 10:31:51,287] Trial 12 finished with value: 1.740014672279358 and parameters: {'n_estimators': 120, 'max_depth': 7, 'learning_rate': 0.1068199129169437, 'subsample': 0.6461192327509931, 'colsample_bytree': 0.6362066546548343}. Best is trial 9 with value: 1.6563640832901.\n",
            "[I 2025-10-07 10:31:51,541] Trial 13 finished with value: 2.406627893447876 and parameters: {'n_estimators': 205, 'max_depth': 8, 'learning_rate': 0.10732661985640791, 'subsample': 0.5581783455057111, 'colsample_bytree': 0.5731285222762703}. Best is trial 9 with value: 1.6563640832901.\n",
            "[I 2025-10-07 10:31:51,942] Trial 14 finished with value: 1.838823676109314 and parameters: {'n_estimators': 243, 'max_depth': 5, 'learning_rate': 0.040013351116217365, 'subsample': 0.8892324397219236, 'colsample_bytree': 0.6738167152482913}. Best is trial 9 with value: 1.6563640832901.\n",
            "[I 2025-10-07 10:31:52,053] Trial 15 finished with value: 2.353611469268799 and parameters: {'n_estimators': 58, 'max_depth': 9, 'learning_rate': 0.17569711022060572, 'subsample': 0.6345203940681049, 'colsample_bytree': 0.7948407283109317}. Best is trial 9 with value: 1.6563640832901.\n",
            "[I 2025-10-07 10:31:52,299] Trial 16 finished with value: 1.8008434772491455 and parameters: {'n_estimators': 183, 'max_depth': 5, 'learning_rate': 0.07459243021342128, 'subsample': 0.7324170166197722, 'colsample_bytree': 0.5805259159512318}. Best is trial 9 with value: 1.6563640832901.\n",
            "[I 2025-10-07 10:31:52,576] Trial 17 finished with value: 1.7819515466690063 and parameters: {'n_estimators': 130, 'max_depth': 8, 'learning_rate': 0.02068086816741835, 'subsample': 0.9040330172235821, 'colsample_bytree': 0.9398069633632561}. Best is trial 9 with value: 1.6563640832901.\n",
            "[I 2025-10-07 10:31:52,835] Trial 18 finished with value: 2.1599271297454834 and parameters: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.2215320124390677, 'subsample': 0.546095434435093, 'colsample_bytree': 0.6863967764833431}. Best is trial 9 with value: 1.6563640832901.\n",
            "[I 2025-10-07 10:31:53,023] Trial 19 finished with value: 1.7359111309051514 and parameters: {'n_estimators': 172, 'max_depth': 9, 'learning_rate': 0.14920643617808257, 'subsample': 0.6787709968782899, 'colsample_bytree': 0.5518073548085333}. Best is trial 9 with value: 1.6563640832901.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Best params: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "Fine-tune Epoch 1/2 - Loss: 138.5878\n",
            "Fine-tune Epoch 2/2 - Loss: 125.6013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 10:31:59,209] A new study created in memory with name: no-name-90f0fe28-fb0d-4b8b-9ba7-4691cd1be1a3\n",
            "[I 2025-10-07 10:31:59,413] Trial 0 finished with value: 2.9473876953125 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 2.9473876953125.\n",
            "[I 2025-10-07 10:31:59,523] Trial 1 finished with value: 3.2474594116210938 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 2.9473876953125.\n",
            "[I 2025-10-07 10:31:59,655] Trial 2 finished with value: 3.045907974243164 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 2.9473876953125.\n",
            "[I 2025-10-07 10:31:59,868] Trial 3 finished with value: 2.8746025562286377 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.8746025562286377.\n",
            "[I 2025-10-07 10:32:00,237] Trial 4 finished with value: 3.0347025394439697 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 3 with value: 2.8746025562286377.\n",
            "[I 2025-10-07 10:32:00,559] Trial 5 finished with value: 2.9010910987854004 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 3 with value: 2.8746025562286377.\n",
            "[I 2025-10-07 10:32:00,980] Trial 6 finished with value: 3.413759231567383 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 3 with value: 2.8746025562286377.\n",
            "[I 2025-10-07 10:32:01,524] Trial 7 finished with value: 3.2583320140838623 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 3 with value: 2.8746025562286377.\n",
            "[I 2025-10-07 10:32:01,698] Trial 8 finished with value: 3.3277313709259033 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 3 with value: 2.8746025562286377.\n",
            "[I 2025-10-07 10:32:02,121] Trial 9 finished with value: 2.864701271057129 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 9 with value: 2.864701271057129.\n",
            "[I 2025-10-07 10:32:02,851] Trial 10 finished with value: 3.286111354827881 and parameters: {'n_estimators': 291, 'max_depth': 7, 'learning_rate': 0.03001215871999436, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8259332753890892}. Best is trial 9 with value: 2.864701271057129.\n",
            "[I 2025-10-07 10:32:03,061] Trial 11 finished with value: 2.5687427520751953 and parameters: {'n_estimators': 136, 'max_depth': 5, 'learning_rate': 0.06463513151371238, 'subsample': 0.6758580632546128, 'colsample_bytree': 0.6557853050977308}. Best is trial 11 with value: 2.5687427520751953.\n",
            "[I 2025-10-07 10:32:03,323] Trial 12 finished with value: 3.2409439086914062 and parameters: {'n_estimators': 156, 'max_depth': 6, 'learning_rate': 0.08671259841619296, 'subsample': 0.6241410648508355, 'colsample_bytree': 0.6712579968688783}. Best is trial 11 with value: 2.5687427520751953.\n",
            "[I 2025-10-07 10:32:03,486] Trial 13 finished with value: 3.352510929107666 and parameters: {'n_estimators': 129, 'max_depth': 5, 'learning_rate': 0.031149929333001484, 'subsample': 0.891594593893088, 'colsample_bytree': 0.5064621786572163}. Best is trial 11 with value: 2.5687427520751953.\n",
            "[I 2025-10-07 10:32:03,664] Trial 14 finished with value: 3.3456661701202393 and parameters: {'n_estimators': 193, 'max_depth': 8, 'learning_rate': 0.27595474380178503, 'subsample': 0.6372424866412404, 'colsample_bytree': 0.7994209827157244}. Best is trial 11 with value: 2.5687427520751953.\n",
            "[I 2025-10-07 10:32:03,966] Trial 15 finished with value: 3.098668336868286 and parameters: {'n_estimators': 239, 'max_depth': 5, 'learning_rate': 0.08962717626942573, 'subsample': 0.5392311924833902, 'colsample_bytree': 0.5778918965965928}. Best is trial 11 with value: 2.5687427520751953.\n",
            "[I 2025-10-07 10:32:04,264] Trial 16 finished with value: 3.0218772888183594 and parameters: {'n_estimators': 172, 'max_depth': 8, 'learning_rate': 0.04282743579807444, 'subsample': 0.7327934525896926, 'colsample_bytree': 0.6873291548115421}. Best is trial 11 with value: 2.5687427520751953.\n",
            "[I 2025-10-07 10:32:04,443] Trial 17 finished with value: 3.309809923171997 and parameters: {'n_estimators': 117, 'max_depth': 5, 'learning_rate': 0.019446414442102006, 'subsample': 0.6757153413310089, 'colsample_bytree': 0.7629983482021694}. Best is trial 11 with value: 2.5687427520751953.\n",
            "[I 2025-10-07 10:32:04,632] Trial 18 finished with value: 2.6086297035217285 and parameters: {'n_estimators': 220, 'max_depth': 2, 'learning_rate': 0.08624422233695168, 'subsample': 0.8949828442656664, 'colsample_bytree': 0.910236126593346}. Best is trial 11 with value: 2.5687427520751953.\n",
            "[I 2025-10-07 10:32:04,836] Trial 19 finished with value: 2.818117618560791 and parameters: {'n_estimators': 284, 'max_depth': 2, 'learning_rate': 0.09266819359521046, 'subsample': 0.9024747466287594, 'colsample_bytree': 0.9119023211761477}. Best is trial 11 with value: 2.5687427520751953.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Best params: {'n_estimators': 136, 'max_depth': 5, 'learning_rate': 0.06463513151371238, 'subsample': 0.6758580632546128, 'colsample_bytree': 0.6557853050977308}\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "Fine-tune Epoch 1/2 - Loss: 151.9076\n",
            "Fine-tune Epoch 2/2 - Loss: 139.4186\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 10:32:11,057] A new study created in memory with name: no-name-7f339f59-2fea-4410-802e-e10975e2edf2\n",
            "[I 2025-10-07 10:32:11,265] Trial 0 finished with value: 3.5185558795928955 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 3.5185558795928955.\n",
            "[I 2025-10-07 10:32:11,342] Trial 1 finished with value: 3.8417365550994873 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 0 with value: 3.5185558795928955.\n",
            "[I 2025-10-07 10:32:11,429] Trial 2 finished with value: 4.030134677886963 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 0 with value: 3.5185558795928955.\n",
            "[I 2025-10-07 10:32:11,567] Trial 3 finished with value: 3.8537261486053467 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 0 with value: 3.5185558795928955.\n",
            "[I 2025-10-07 10:32:11,782] Trial 4 finished with value: 3.450131893157959 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 4 with value: 3.450131893157959.\n",
            "[I 2025-10-07 10:32:12,003] Trial 5 finished with value: 3.4632225036621094 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 4 with value: 3.450131893157959.\n",
            "[I 2025-10-07 10:32:12,304] Trial 6 finished with value: 5.695667743682861 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 4 with value: 3.450131893157959.\n",
            "[I 2025-10-07 10:32:12,655] Trial 7 finished with value: 4.426128387451172 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 4 with value: 3.450131893157959.\n",
            "[I 2025-10-07 10:32:12,762] Trial 8 finished with value: 3.8413093090057373 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 4 with value: 3.450131893157959.\n",
            "[I 2025-10-07 10:32:13,147] Trial 9 finished with value: 3.9198882579803467 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 4 with value: 3.450131893157959.\n",
            "[I 2025-10-07 10:32:13,868] Trial 10 finished with value: 3.631657361984253 and parameters: {'n_estimators': 289, 'max_depth': 7, 'learning_rate': 0.023969116039403743, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8200442512337782}. Best is trial 4 with value: 3.450131893157959.\n",
            "[I 2025-10-07 10:32:14,226] Trial 11 finished with value: 3.119755506515503 and parameters: {'n_estimators': 178, 'max_depth': 6, 'learning_rate': 0.030241001410559513, 'subsample': 0.6673975881116181, 'colsample_bytree': 0.5031369754022077}. Best is trial 11 with value: 3.119755506515503.\n",
            "[I 2025-10-07 10:32:14,694] Trial 12 finished with value: 3.445634365081787 and parameters: {'n_estimators': 173, 'max_depth': 8, 'learning_rate': 0.026438783878681333, 'subsample': 0.6461192327509931, 'colsample_bytree': 0.7363570033289744}. Best is trial 11 with value: 3.119755506515503.\n",
            "[I 2025-10-07 10:32:15,168] Trial 13 finished with value: 3.542271375656128 and parameters: {'n_estimators': 152, 'max_depth': 8, 'learning_rate': 0.02834801773410774, 'subsample': 0.6134697240046978, 'colsample_bytree': 0.9469561526267276}. Best is trial 11 with value: 3.119755506515503.\n",
            "[I 2025-10-07 10:32:15,624] Trial 14 finished with value: 3.607940673828125 and parameters: {'n_estimators': 152, 'max_depth': 8, 'learning_rate': 0.03720181861619697, 'subsample': 0.6372424866412404, 'colsample_bytree': 0.8138214726279749}. Best is trial 11 with value: 3.119755506515503.\n",
            "[I 2025-10-07 10:32:15,882] Trial 15 finished with value: 3.371161937713623 and parameters: {'n_estimators': 125, 'max_depth': 6, 'learning_rate': 0.09894162917596783, 'subsample': 0.5392311924833902, 'colsample_bytree': 0.5014116766150295}. Best is trial 11 with value: 3.119755506515503.\n",
            "[I 2025-10-07 10:32:16,132] Trial 16 finished with value: 3.461501359939575 and parameters: {'n_estimators': 121, 'max_depth': 6, 'learning_rate': 0.08719209000999652, 'subsample': 0.534483975903986, 'colsample_bytree': 0.5242104299366955}. Best is trial 11 with value: 3.119755506515503.\n",
            "[I 2025-10-07 10:32:16,295] Trial 17 finished with value: 3.4599151611328125 and parameters: {'n_estimators': 125, 'max_depth': 5, 'learning_rate': 0.26663748121620445, 'subsample': 0.5694064187762937, 'colsample_bytree': 0.5008266493815525}. Best is trial 11 with value: 3.119755506515503.\n",
            "[I 2025-10-07 10:32:16,562] Trial 18 finished with value: 3.569344997406006 and parameters: {'n_estimators': 180, 'max_depth': 7, 'learning_rate': 0.09746587984812109, 'subsample': 0.5694345922488891, 'colsample_bytree': 0.6689964904433673}. Best is trial 11 with value: 3.119755506515503.\n",
            "[I 2025-10-07 10:32:16,806] Trial 19 finished with value: 3.4874348640441895 and parameters: {'n_estimators': 180, 'max_depth': 7, 'learning_rate': 0.018555471812323813, 'subsample': 0.7129568166205323, 'colsample_bytree': 0.5495839204752634}. Best is trial 11 with value: 3.119755506515503.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Best params: {'n_estimators': 178, 'max_depth': 6, 'learning_rate': 0.030241001410559513, 'subsample': 0.6673975881116181, 'colsample_bytree': 0.5031369754022077}\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "Fine-tune Epoch 1/2 - Loss: 151.4980\n",
            "Fine-tune Epoch 2/2 - Loss: 139.7704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 10:32:23,616] A new study created in memory with name: no-name-913174f2-460f-4f99-9905-200e418f63fa\n",
            "[I 2025-10-07 10:32:23,803] Trial 0 finished with value: 1.904070258140564 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.904070258140564.\n",
            "[I 2025-10-07 10:32:23,875] Trial 1 finished with value: 1.3831326961517334 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:23,957] Trial 2 finished with value: 2.5242691040039062 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:24,081] Trial 3 finished with value: 2.064018726348877 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:24,309] Trial 4 finished with value: 1.9591331481933594 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:24,506] Trial 5 finished with value: 2.0337231159210205 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:24,792] Trial 6 finished with value: 2.1765081882476807 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:25,124] Trial 7 finished with value: 1.9194743633270264 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:25,250] Trial 8 finished with value: 2.212099313735962 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:25,504] Trial 9 finished with value: 1.9622970819473267 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:25,703] Trial 10 finished with value: 2.430375337600708 and parameters: {'n_estimators': 139, 'max_depth': 7, 'learning_rate': 0.27047297227177763, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9168158328299749}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:25,942] Trial 11 finished with value: 1.9798059463500977 and parameters: {'n_estimators': 138, 'max_depth': 10, 'learning_rate': 0.1286674737306269, 'subsample': 0.8730087382509263, 'colsample_bytree': 0.8416909650214576}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:26,194] Trial 12 finished with value: 1.9621989727020264 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.13116982721498638, 'subsample': 0.8837864985996776, 'colsample_bytree': 0.8576737848754852}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:26,446] Trial 13 finished with value: 2.249861240386963 and parameters: {'n_estimators': 110, 'max_depth': 6, 'learning_rate': 0.29838444194171054, 'subsample': 0.6412396842185336, 'colsample_bytree': 0.8229282203516206}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:26,915] Trial 14 finished with value: 1.936029076576233 and parameters: {'n_estimators': 172, 'max_depth': 8, 'learning_rate': 0.1062272193666865, 'subsample': 0.7917316769460038, 'colsample_bytree': 0.7698100534013989}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:27,134] Trial 15 finished with value: 1.7336490154266357 and parameters: {'n_estimators': 292, 'max_depth': 2, 'learning_rate': 0.08962717626942573, 'subsample': 0.9167270621070531, 'colsample_bytree': 0.5015731892181307}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:27,581] Trial 16 finished with value: 1.8896598815917969 and parameters: {'n_estimators': 300, 'max_depth': 2, 'learning_rate': 0.029075317300470218, 'subsample': 0.9312256601213913, 'colsample_bytree': 0.9219900508555974}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:27,755] Trial 17 finished with value: 1.8107478618621826 and parameters: {'n_estimators': 300, 'max_depth': 2, 'learning_rate': 0.20960605443577704, 'subsample': 0.9061843195542296, 'colsample_bytree': 0.5015635329613551}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:28,172] Trial 18 finished with value: 1.8237873315811157 and parameters: {'n_estimators': 246, 'max_depth': 5, 'learning_rate': 0.09031256134732037, 'subsample': 0.9952449389638871, 'colsample_bytree': 0.6868199329682921}. Best is trial 1 with value: 1.3831326961517334.\n",
            "[I 2025-10-07 10:32:28,408] Trial 19 finished with value: 1.6495403051376343 and parameters: {'n_estimators': 172, 'max_depth': 2, 'learning_rate': 0.08005408496844885, 'subsample': 0.8383849220407379, 'colsample_bytree': 0.781815367238253}. Best is trial 1 with value: 1.3831326961517334.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 4 Best params: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "Fine-tune Epoch 1/2 - Loss: 151.4926\n",
            "Fine-tune Epoch 2/2 - Loss: 140.9805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-07 10:32:34,684] A new study created in memory with name: no-name-0e03ea4b-7fcf-4599-a95e-f8cba67dfbdd\n",
            "[I 2025-10-07 10:32:34,903] Trial 0 finished with value: 1.3128811120986938 and parameters: {'n_estimators': 144, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 1.3128811120986938.\n",
            "[I 2025-10-07 10:32:34,980] Trial 1 finished with value: 1.1019995212554932 and parameters: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:35,069] Trial 2 finished with value: 2.492915391921997 and parameters: {'n_estimators': 55, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:35,196] Trial 3 finished with value: 1.3389015197753906 and parameters: {'n_estimators': 96, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:35,409] Trial 4 finished with value: 1.2706369161605835 and parameters: {'n_estimators': 203, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:35,607] Trial 5 finished with value: 1.1092689037322998 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:35,920] Trial 6 finished with value: 1.6224095821380615 and parameters: {'n_estimators': 202, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:36,283] Trial 7 finished with value: 1.442456841468811 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:36,411] Trial 8 finished with value: 1.7830305099487305 and parameters: {'n_estimators': 80, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:36,669] Trial 9 finished with value: 1.1524124145507812 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:36,917] Trial 10 finished with value: 1.4718589782714844 and parameters: {'n_estimators': 139, 'max_depth': 7, 'learning_rate': 0.27047297227177763, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.9168158328299749}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:37,119] Trial 11 finished with value: 1.3453369140625 and parameters: {'n_estimators': 277, 'max_depth': 2, 'learning_rate': 0.09425671587813331, 'subsample': 0.8739084974067326, 'colsample_bytree': 0.8582590089757371}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:37,354] Trial 12 finished with value: 1.3146928548812866 and parameters: {'n_estimators': 298, 'max_depth': 2, 'learning_rate': 0.030716454994033953, 'subsample': 0.8806736357247846, 'colsample_bytree': 0.8102888477597007}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:37,481] Trial 13 finished with value: 2.3623621463775635 and parameters: {'n_estimators': 145, 'max_depth': 6, 'learning_rate': 0.29397500854068426, 'subsample': 0.6376629900391765, 'colsample_bytree': 0.5032840393800132}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:37,978] Trial 14 finished with value: 1.281672477722168 and parameters: {'n_estimators': 239, 'max_depth': 8, 'learning_rate': 0.0383477048262245, 'subsample': 0.7837958068143924, 'colsample_bytree': 0.8138214726279749}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:38,306] Trial 15 finished with value: 1.2622044086456299 and parameters: {'n_estimators': 169, 'max_depth': 5, 'learning_rate': 0.09894162917596783, 'subsample': 0.9140258921869188, 'colsample_bytree': 0.892146309820842}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:38,399] Trial 16 finished with value: 1.7134411334991455 and parameters: {'n_estimators': 108, 'max_depth': 2, 'learning_rate': 0.17889396860003975, 'subsample': 0.8317032143059913, 'colsample_bytree': 0.7771618950778919}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:38,461] Trial 17 finished with value: 1.6967334747314453 and parameters: {'n_estimators': 59, 'max_depth': 3, 'learning_rate': 0.02068086816741835, 'subsample': 0.7261669761185063, 'colsample_bytree': 0.5015624535055074}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:38,744] Trial 18 finished with value: 1.2481756210327148 and parameters: {'n_estimators': 179, 'max_depth': 5, 'learning_rate': 0.04473317901006215, 'subsample': 0.6654554892776469, 'colsample_bytree': 0.6868199329682921}. Best is trial 1 with value: 1.1019995212554932.\n",
            "[I 2025-10-07 10:32:38,965] Trial 19 finished with value: 1.5555212497711182 and parameters: {'n_estimators': 248, 'max_depth': 2, 'learning_rate': 0.074942638251208, 'subsample': 0.56391216214132, 'colsample_bytree': 0.9398207228701253}. Best is trial 1 with value: 1.1019995212554932.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 5 Best params: {'n_estimators': 89, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}\n",
            "\n",
            "=== CNN + XGBoost K-Fold with Fine-Tune Results ===\n",
            "RMSE: 2.7820 | MAE: 1.9527\n"
          ]
        }
      ],
      "source": [
        "results = run_cnn_xgb_kfold_finetune(\n",
        "    labeled_df=df,      # your DataFrame\n",
        "    n_splits=5,         # 5-Fold CV\n",
        "    batch_size=8,       # batch size for embeddings extraction\n",
        "    xgb_trials=20       # number of Optuna trials to tune XGBoost per fold\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64v7xgiMN7TV"
      },
      "source": [
        "#### Results: MAE 1.713 - 1.913"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVOI8hJiPQr4"
      },
      "source": [
        "### Baseline Partial Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RPl_ZAGPThZ"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# cnn_xgb_finetune_pipeline_fixed.py\n",
        "# ----------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 1 Dataset class\n",
        "# ============================================================\n",
        "class ImageRegressionDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[\"Filename\"]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = torch.tensor(row[\"Hb\"], dtype=torch.float32)\n",
        "        return img, label\n",
        "\n",
        "# ============================================================\n",
        "# 2 Partial Fine-Tuning of ResNet18 Backbone\n",
        "# ============================================================\n",
        "def fine_tune_backbone_partial(backbone, train_df, transform, device, epochs=3, batch_size=8, lr=1e-4):\n",
        "    print(f\"[INFO] Starting partial fine-tuning on {len(train_df)} labeled samples...\")\n",
        "\n",
        "    # Freeze low-level layers (conv1 to layer3)\n",
        "    for name, param in backbone.named_parameters():\n",
        "        if not name.startswith(\"layer4\") and not name.startswith(\"fc\"):\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Replace final classification head with regression head\n",
        "    in_features = backbone.fc.in_features\n",
        "    backbone.fc = nn.Linear(in_features, 1)  # regression head\n",
        "\n",
        "    dataset = ImageRegressionDataset(train_df, transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, backbone.parameters()), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    backbone.to(device)\n",
        "    backbone.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(loader, desc=f\"[Fine-Tune Epoch {epoch+1}/{epochs}]\")\n",
        "        for imgs, targets in pbar:\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = backbone(imgs).view(-1)\n",
        "            loss = criterion(preds, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            pbar.set_postfix({\"MSE\": loss.item()})\n",
        "        avg_loss = total_loss / len(dataset)\n",
        "        print(f\"[INFO] Fine-tune Epoch {epoch+1} Avg MSE: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"[INFO] Partial fine-tuning complete.\")\n",
        "    return backbone\n",
        "\n",
        "# ============================================================\n",
        "# 3 Embedding Extraction\n",
        "# ============================================================\n",
        "def extract_embeddings(backbone, df, transform, device, batch_size=8):\n",
        "    backbone.eval()\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    # Remove final FC layer for feature extraction\n",
        "    feature_extractor = nn.Sequential(*list(backbone.children())[:-1]).to(device)\n",
        "\n",
        "    loader = DataLoader(ImageRegressionDataset(df, transform), batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in tqdm(loader, desc=\"Extracting embeddings\"):\n",
        "            imgs = imgs.to(device)\n",
        "            feats = feature_extractor(imgs)  # shape: [B, C, 1, 1]\n",
        "            pooled = feats.view(feats.size(0), -1)  # flatten to [B, C]\n",
        "            embeddings.append(pooled.cpu().numpy())\n",
        "            labels.append(targets.numpy())\n",
        "\n",
        "    embeddings = np.concatenate(embeddings)\n",
        "    labels = np.concatenate(labels)\n",
        "    return embeddings, labels\n",
        "\n",
        "# ============================================================\n",
        "# 4 XGBoost Regression with Optuna Optimization\n",
        "# ============================================================\n",
        "def tune_xgb_with_optuna(X_train, y_train, X_val, y_val, n_trials=20):\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"objective\": \"reg:squarederror\",\n",
        "            \"eval_metric\": \"mae\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"device\": \"cuda\",\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        }\n",
        "\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "        preds = model.predict(X_val)\n",
        "        return mean_absolute_error(y_val, preds)\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "    print(\"[Optuna] Best trial:\", study.best_trial.params)\n",
        "    return study.best_trial.params\n",
        "\n",
        "# ============================================================\n",
        "# 5 Full CNN + XGBoost K-Fold Training Pipeline\n",
        "# ============================================================\n",
        "def run_cnn_xgb_kfold_finetune(labeled_df, n_splits=5, batch_size=8, xgb_trials=20):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    metrics = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(labeled_df), 1):\n",
        "        print(f\"\\n--- Fold {fold}/{n_splits} ---\")\n",
        "        train_df, val_df = labeled_df.iloc[train_idx], labeled_df.iloc[val_idx]\n",
        "\n",
        "        print(f\"[INFO] Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
        "\n",
        "        # Load pretrained CNN\n",
        "        backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        backbone = fine_tune_backbone_partial(backbone, train_df, transform, device, epochs=3, batch_size=batch_size)\n",
        "\n",
        "        # Extract embeddings\n",
        "        X_train, y_train = extract_embeddings(backbone, train_df, transform, device, batch_size)\n",
        "        X_val, y_val = extract_embeddings(backbone, val_df, transform, device, batch_size)\n",
        "\n",
        "        # Tune and train XGBoost\n",
        "        best_params = tune_xgb_with_optuna(X_train, y_train, X_val, y_val, n_trials=xgb_trials)\n",
        "        xgb_model = xgb.XGBRegressor(**best_params)\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        preds = xgb_model.predict(X_val)\n",
        "        mae = mean_absolute_error(y_val, preds)\n",
        "        mse = mean_squared_error(y_val, preds)\n",
        "        r2 = r2_score(y_val, preds)\n",
        "\n",
        "        print(f\"[Fold {fold}] MAE: {mae:.4f}, MSE: {mse:.4f}, R: {r2:.4f}\")\n",
        "        metrics.append({\"fold\": fold, \"MAE\": mae, \"MSE\": mse, \"R2\": r2})\n",
        "\n",
        "    results_df = pd.DataFrame(metrics)\n",
        "    print(\"\\n===== Final Cross-Validation Results =====\")\n",
        "    print(results_df)\n",
        "    print(\"\\nMean MAE:\", results_df[\"MAE\"].mean())\n",
        "    print(\"Mean R:\", results_df[\"R2\"].mean())\n",
        "    return results_df\n",
        "\n",
        "# ============================================================\n",
        "#  Example Usage\n",
        "# ============================================================\n",
        "# df = pd.DataFrame({\"Filename\": [...], \"Hb\": [...]})\n",
        "# results = run_cnn_xgb_kfold_finetune(df, n_splits=5, batch_size=8, xgb_trials=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnxoaaAMPVfw"
      },
      "source": [
        "#### Run Partial Finetune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2CzARXUPYBd",
        "outputId": "536408a1-96b9-4640-e837-32f264bfdfca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 1/5 ---\n",
            "[INFO] Training samples: 24, Validation samples: 7\n",
            "[INFO] Starting partial fine-tuning on 24 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/3]: 100%|| 3/3 [00:02<00:00,  1.09it/s, MSE=102]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 118.8106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/3]: 100%|| 3/3 [00:01<00:00,  1.54it/s, MSE=86.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 109.0581\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/3]: 100%|| 3/3 [00:01<00:00,  1.79it/s, MSE=143]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 102.8792\n",
            "[INFO] Partial fine-tuning complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 3/3 [00:01<00:00,  1.82it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.60it/s]\n",
            "[I 2025-10-07 10:45:17,355] A new study created in memory with name: no-name-ded93f1d-69ed-41e6-90a6-ec22b02ef8e4\n",
            "[I 2025-10-07 10:45:18,040] Trial 0 finished with value: 2.5282208919525146 and parameters: {'learning_rate': 0.10628642072138945, 'max_depth': 10, 'n_estimators': 503, 'subsample': 0.7806445548156578, 'colsample_bytree': 0.9543908779493089}. Best is trial 0 with value: 2.5282208919525146.\n",
            "[I 2025-10-07 10:45:18,193] Trial 1 finished with value: 2.4054229259490967 and parameters: {'learning_rate': 0.010225114487514026, 'max_depth': 3, 'n_estimators': 121, 'subsample': 0.5261623186073643, 'colsample_bytree': 0.9971644973548036}. Best is trial 1 with value: 2.4054229259490967.\n",
            "[I 2025-10-07 10:45:19,286] Trial 2 finished with value: 2.2773120403289795 and parameters: {'learning_rate': 0.0013584716385151097, 'max_depth': 4, 'n_estimators': 892, 'subsample': 0.5076558530139025, 'colsample_bytree': 0.8528321651152193}. Best is trial 2 with value: 2.2773120403289795.\n",
            "[I 2025-10-07 10:45:20,219] Trial 3 finished with value: 1.8707269430160522 and parameters: {'learning_rate': 0.019631969229529356, 'max_depth': 3, 'n_estimators': 909, 'subsample': 0.7936856367061684, 'colsample_bytree': 0.5286283298512617}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:20,922] Trial 4 finished with value: 2.258540391921997 and parameters: {'learning_rate': 0.005811141085391297, 'max_depth': 7, 'n_estimators': 468, 'subsample': 0.7668801641994543, 'colsample_bytree': 0.9368541033123082}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:21,620] Trial 5 finished with value: 2.1041321754455566 and parameters: {'learning_rate': 0.0997563930639947, 'max_depth': 8, 'n_estimators': 664, 'subsample': 0.6954995524592757, 'colsample_bytree': 0.9258655849247446}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:21,891] Trial 6 finished with value: 2.7370455265045166 and parameters: {'learning_rate': 0.08253839804453163, 'max_depth': 4, 'n_estimators': 221, 'subsample': 0.9732628092079518, 'colsample_bytree': 0.7501627939443356}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:23,383] Trial 7 finished with value: 2.313971996307373 and parameters: {'learning_rate': 0.0021015991686822474, 'max_depth': 9, 'n_estimators': 937, 'subsample': 0.6690577953294222, 'colsample_bytree': 0.7947616452574672}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:24,515] Trial 8 finished with value: 2.3642067909240723 and parameters: {'learning_rate': 0.035479219768530745, 'max_depth': 8, 'n_estimators': 715, 'subsample': 0.5418896225187795, 'colsample_bytree': 0.5802773730041065}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:25,530] Trial 9 finished with value: 2.4240894317626953 and parameters: {'learning_rate': 0.14555253671387633, 'max_depth': 5, 'n_estimators': 908, 'subsample': 0.6131970468545651, 'colsample_bytree': 0.9329678716718435}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:26,487] Trial 10 finished with value: 2.243769884109497 and parameters: {'learning_rate': 0.025299415596715738, 'max_depth': 6, 'n_estimators': 709, 'subsample': 0.9123604439822267, 'colsample_bytree': 0.5090240140217656}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:27,067] Trial 11 finished with value: 2.2168095111846924 and parameters: {'learning_rate': 0.29457278416789456, 'max_depth': 7, 'n_estimators': 691, 'subsample': 0.7002130987650081, 'colsample_bytree': 0.6393287761364771}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:28,019] Trial 12 finished with value: 2.1925084590911865 and parameters: {'learning_rate': 0.03739940871854439, 'max_depth': 8, 'n_estimators': 789, 'subsample': 0.841628686111422, 'colsample_bytree': 0.6327392565569124}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:28,590] Trial 13 finished with value: 2.283494710922241 and parameters: {'learning_rate': 0.0096993989117544, 'max_depth': 6, 'n_estimators': 375, 'subsample': 0.8278378335136068, 'colsample_bytree': 0.8249486354339065}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:29,474] Trial 14 finished with value: 2.1164638996124268 and parameters: {'learning_rate': 0.05298845280110786, 'max_depth': 3, 'n_estimators': 998, 'subsample': 0.6903082632531702, 'colsample_bytree': 0.7024031361623027}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:30,252] Trial 15 finished with value: 2.1769216060638428 and parameters: {'learning_rate': 0.004291656800865331, 'max_depth': 10, 'n_estimators': 586, 'subsample': 0.6046491258107276, 'colsample_bytree': 0.5020663854059608}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:31,599] Trial 16 finished with value: 2.1550955772399902 and parameters: {'learning_rate': 0.017897837367660218, 'max_depth': 8, 'n_estimators': 827, 'subsample': 0.8629302474166116, 'colsample_bytree': 0.8940619875393645}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:32,119] Trial 17 finished with value: 2.435399293899536 and parameters: {'learning_rate': 0.21287571505055394, 'max_depth': 5, 'n_estimators': 597, 'subsample': 0.7424209574684548, 'colsample_bytree': 0.7066272190209938}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:32,610] Trial 18 finished with value: 1.962990641593933 and parameters: {'learning_rate': 0.07193464257150572, 'max_depth': 5, 'n_estimators': 392, 'subsample': 0.6239799538497814, 'colsample_bytree': 0.5785796697847347}. Best is trial 3 with value: 1.8707269430160522.\n",
            "[I 2025-10-07 10:45:33,057] Trial 19 finished with value: 2.021080732345581 and parameters: {'learning_rate': 0.06220280953392381, 'max_depth': 4, 'n_estimators': 360, 'subsample': 0.6118709178369031, 'colsample_bytree': 0.5926041282985829}. Best is trial 3 with value: 1.8707269430160522.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.019631969229529356, 'max_depth': 3, 'n_estimators': 909, 'subsample': 0.7936856367061684, 'colsample_bytree': 0.5286283298512617}\n",
            "[Fold 1] MAE: 2.3651, MSE: 10.5899, R: -0.3395\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n",
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/3]: 100%|| 4/4 [00:01<00:00,  2.04it/s, MSE=296]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 154.7171\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/3]: 100%|| 4/4 [00:02<00:00,  1.53it/s, MSE=137]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 144.1015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/3]: 100%|| 4/4 [00:01<00:00,  2.12it/s, MSE=103]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 133.2663\n",
            "[INFO] Partial fine-tuning complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:01<00:00,  2.46it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.51it/s]\n",
            "[I 2025-10-07 10:45:42,951] A new study created in memory with name: no-name-b689eb41-8662-407a-a879-44b9e6c91942\n",
            "[I 2025-10-07 10:45:43,919] Trial 0 finished with value: 1.9157780408859253 and parameters: {'learning_rate': 0.015658911031296643, 'max_depth': 3, 'n_estimators': 702, 'subsample': 0.7781136767555128, 'colsample_bytree': 0.9690346274684678}. Best is trial 0 with value: 1.9157780408859253.\n",
            "[I 2025-10-07 10:45:44,232] Trial 1 finished with value: 1.3736966848373413 and parameters: {'learning_rate': 0.16262534672766799, 'max_depth': 5, 'n_estimators': 287, 'subsample': 0.7172648473093197, 'colsample_bytree': 0.7837943766299444}. Best is trial 1 with value: 1.3736966848373413.\n",
            "[I 2025-10-07 10:45:44,391] Trial 2 finished with value: 2.0241477489471436 and parameters: {'learning_rate': 0.20584421051952387, 'max_depth': 3, 'n_estimators': 142, 'subsample': 0.6813393404894234, 'colsample_bytree': 0.7075077821469482}. Best is trial 1 with value: 1.3736966848373413.\n",
            "[I 2025-10-07 10:45:45,470] Trial 3 finished with value: 1.2701053619384766 and parameters: {'learning_rate': 0.0560451795288585, 'max_depth': 8, 'n_estimators': 989, 'subsample': 0.783861515662799, 'colsample_bytree': 0.7561676542644256}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:46,463] Trial 4 finished with value: 1.3448377847671509 and parameters: {'learning_rate': 0.037315629102786414, 'max_depth': 9, 'n_estimators': 835, 'subsample': 0.6965735524524603, 'colsample_bytree': 0.5748515342744664}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:47,229] Trial 5 finished with value: 1.9785138368606567 and parameters: {'learning_rate': 0.002795492272786554, 'max_depth': 8, 'n_estimators': 566, 'subsample': 0.6980402767391447, 'colsample_bytree': 0.6439120894312191}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:47,490] Trial 6 finished with value: 2.2102367877960205 and parameters: {'learning_rate': 0.005834901753499528, 'max_depth': 8, 'n_estimators': 168, 'subsample': 0.9414144486662791, 'colsample_bytree': 0.6802991321737004}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:48,303] Trial 7 finished with value: 2.135507583618164 and parameters: {'learning_rate': 0.0025119373530214624, 'max_depth': 9, 'n_estimators': 576, 'subsample': 0.8483968358559246, 'colsample_bytree': 0.6212972089219075}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:48,927] Trial 8 finished with value: 2.4902079105377197 and parameters: {'learning_rate': 0.28411484777867263, 'max_depth': 5, 'n_estimators': 737, 'subsample': 0.6143362248562312, 'colsample_bytree': 0.8597402212486196}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:50,099] Trial 9 finished with value: 2.05810809135437 and parameters: {'learning_rate': 0.0012813415111029868, 'max_depth': 6, 'n_estimators': 760, 'subsample': 0.6802789078502961, 'colsample_bytree': 0.6456455246466428}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:51,381] Trial 10 finished with value: 1.6880664825439453 and parameters: {'learning_rate': 0.053990719503064624, 'max_depth': 10, 'n_estimators': 953, 'subsample': 0.5186658377498055, 'colsample_bytree': 0.5204757107154124}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:52,681] Trial 11 finished with value: 2.138077735900879 and parameters: {'learning_rate': 0.04234169146673249, 'max_depth': 8, 'n_estimators': 999, 'subsample': 0.8216783726756086, 'colsample_bytree': 0.5015287726200914}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:53,623] Trial 12 finished with value: 2.2205934524536133 and parameters: {'learning_rate': 0.047034226773044356, 'max_depth': 10, 'n_estimators': 871, 'subsample': 0.9204349554706758, 'colsample_bytree': 0.8048001157454991}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:54,769] Trial 13 finished with value: 1.4059325456619263 and parameters: {'learning_rate': 0.01725205139289359, 'max_depth': 7, 'n_estimators': 827, 'subsample': 0.594143301286663, 'colsample_bytree': 0.5775139264557434}. Best is trial 3 with value: 1.2701053619384766.\n",
            "[I 2025-10-07 10:45:55,254] Trial 14 finished with value: 0.8632829785346985 and parameters: {'learning_rate': 0.07722841085364743, 'max_depth': 9, 'n_estimators': 370, 'subsample': 0.8720960843276309, 'colsample_bytree': 0.8711482794117209}. Best is trial 14 with value: 0.8632829785346985.\n",
            "[I 2025-10-07 10:45:55,703] Trial 15 finished with value: 1.262708306312561 and parameters: {'learning_rate': 0.11212913009850549, 'max_depth': 7, 'n_estimators': 407, 'subsample': 0.8721716171833627, 'colsample_bytree': 0.9024781521228065}. Best is trial 14 with value: 0.8632829785346985.\n",
            "[I 2025-10-07 10:45:56,111] Trial 16 finished with value: 1.1519465446472168 and parameters: {'learning_rate': 0.1171287690281346, 'max_depth': 6, 'n_estimators': 372, 'subsample': 0.8840810446543826, 'colsample_bytree': 0.9288725096388802}. Best is trial 14 with value: 0.8632829785346985.\n",
            "[I 2025-10-07 10:45:56,525] Trial 17 finished with value: 2.565234899520874 and parameters: {'learning_rate': 0.09479681993652264, 'max_depth': 5, 'n_estimators': 395, 'subsample': 0.9809481531551922, 'colsample_bytree': 0.9963405498546103}. Best is trial 14 with value: 0.8632829785346985.\n",
            "[I 2025-10-07 10:45:57,165] Trial 18 finished with value: 1.5749400854110718 and parameters: {'learning_rate': 0.009334323284829199, 'max_depth': 6, 'n_estimators': 405, 'subsample': 0.9113581524223988, 'colsample_bytree': 0.9226752755093662}. Best is trial 14 with value: 0.8632829785346985.\n",
            "[I 2025-10-07 10:45:57,497] Trial 19 finished with value: 1.6345826387405396 and parameters: {'learning_rate': 0.09988000372670866, 'max_depth': 6, 'n_estimators': 260, 'subsample': 0.9976037448036845, 'colsample_bytree': 0.8525184831762523}. Best is trial 14 with value: 0.8632829785346985.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.07722841085364743, 'max_depth': 9, 'n_estimators': 370, 'subsample': 0.8720960843276309, 'colsample_bytree': 0.8711482794117209}\n",
            "[Fold 2] MAE: 2.5455, MSE: 15.3192, R: -2.6931\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n",
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/3]: 100%|| 4/4 [00:01<00:00,  2.31it/s, MSE=160]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 162.6356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/3]: 100%|| 4/4 [00:01<00:00,  2.32it/s, MSE=70.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 150.8600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/3]: 100%|| 4/4 [00:02<00:00,  1.60it/s, MSE=306]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 142.4914\n",
            "[INFO] Partial fine-tuning complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:02<00:00,  1.51it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.96it/s]\n",
            "[I 2025-10-07 10:46:07,515] A new study created in memory with name: no-name-dea8c8b5-4984-4a8f-85e8-537b59fc97e3\n",
            "[I 2025-10-07 10:46:08,174] Trial 0 finished with value: 1.4189726114273071 and parameters: {'learning_rate': 0.001482342015661435, 'max_depth': 3, 'n_estimators': 442, 'subsample': 0.9981719490404464, 'colsample_bytree': 0.946741006875577}. Best is trial 0 with value: 1.4189726114273071.\n",
            "[I 2025-10-07 10:46:08,740] Trial 1 finished with value: 1.365996241569519 and parameters: {'learning_rate': 0.0024016517213080856, 'max_depth': 7, 'n_estimators': 412, 'subsample': 0.7593530029753062, 'colsample_bytree': 0.84368296555529}. Best is trial 1 with value: 1.365996241569519.\n",
            "[I 2025-10-07 10:46:09,361] Trial 2 finished with value: 1.4943219423294067 and parameters: {'learning_rate': 0.1816818537245002, 'max_depth': 8, 'n_estimators': 700, 'subsample': 0.7002167187422006, 'colsample_bytree': 0.5275480992467898}. Best is trial 1 with value: 1.365996241569519.\n",
            "[I 2025-10-07 10:46:10,299] Trial 3 finished with value: 0.8333776593208313 and parameters: {'learning_rate': 0.007872194950445825, 'max_depth': 10, 'n_estimators': 616, 'subsample': 0.8097197548288693, 'colsample_bytree': 0.9120931479508743}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:11,445] Trial 4 finished with value: 0.8392794728279114 and parameters: {'learning_rate': 0.012252252205147755, 'max_depth': 9, 'n_estimators': 772, 'subsample': 0.9024745250683309, 'colsample_bytree': 0.511900530330168}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:11,857] Trial 5 finished with value: 0.8603687286376953 and parameters: {'learning_rate': 0.04897098940860448, 'max_depth': 4, 'n_estimators': 333, 'subsample': 0.9209852798662829, 'colsample_bytree': 0.7386605844170104}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:13,229] Trial 6 finished with value: 1.3758481740951538 and parameters: {'learning_rate': 0.018364091313211664, 'max_depth': 10, 'n_estimators': 968, 'subsample': 0.6044426508492784, 'colsample_bytree': 0.6671014227092165}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:13,452] Trial 7 finished with value: 1.4853585958480835 and parameters: {'learning_rate': 0.12472355596824815, 'max_depth': 10, 'n_estimators': 139, 'subsample': 0.6789721593628475, 'colsample_bytree': 0.9964064735271291}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:13,943] Trial 8 finished with value: 0.8879728317260742 and parameters: {'learning_rate': 0.053457339711655213, 'max_depth': 5, 'n_estimators': 361, 'subsample': 0.7598334322374087, 'colsample_bytree': 0.741959642646016}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:14,204] Trial 9 finished with value: 1.2762889862060547 and parameters: {'learning_rate': 0.007149838248799321, 'max_depth': 10, 'n_estimators': 184, 'subsample': 0.7575282012053363, 'colsample_bytree': 0.7490769112501159}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:15,111] Trial 10 finished with value: 1.3348151445388794 and parameters: {'learning_rate': 0.005548318335238435, 'max_depth': 6, 'n_estimators': 668, 'subsample': 0.5145569425051675, 'colsample_bytree': 0.878047563693744}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:16,389] Trial 11 finished with value: 0.9057087898254395 and parameters: {'learning_rate': 0.010437222443639628, 'max_depth': 8, 'n_estimators': 856, 'subsample': 0.8719199051826779, 'colsample_bytree': 0.5002046198798418}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:17,485] Trial 12 finished with value: 0.9240031242370605 and parameters: {'learning_rate': 0.021832996605217456, 'max_depth': 9, 'n_estimators': 641, 'subsample': 0.8679345850013171, 'colsample_bytree': 0.6281065764346946}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:18,920] Trial 13 finished with value: 0.9301993250846863 and parameters: {'learning_rate': 0.0034787863933948104, 'max_depth': 8, 'n_estimators': 816, 'subsample': 0.8513975169575249, 'colsample_bytree': 0.8433055962721399}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:19,839] Trial 14 finished with value: 0.8542938232421875 and parameters: {'learning_rate': 0.037876178950074085, 'max_depth': 9, 'n_estimators': 562, 'subsample': 0.9809590627784177, 'colsample_bytree': 0.620012506952439}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:21,205] Trial 15 finished with value: 0.8944218754768372 and parameters: {'learning_rate': 0.01090580586033566, 'max_depth': 9, 'n_estimators': 814, 'subsample': 0.8212923322042576, 'colsample_bytree': 0.8993960127470302}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:21,934] Trial 16 finished with value: 1.4571067094802856 and parameters: {'learning_rate': 0.0010699221574658245, 'max_depth': 7, 'n_estimators': 555, 'subsample': 0.9305877599671656, 'colsample_bytree': 0.5666212893994276}. Best is trial 3 with value: 0.8333776593208313.\n",
            "[I 2025-10-07 10:46:23,405] Trial 17 finished with value: 0.7796874046325684 and parameters: {'learning_rate': 0.004249999871114709, 'max_depth': 9, 'n_estimators': 997, 'subsample': 0.821044185306938, 'colsample_bytree': 0.798904314681874}. Best is trial 17 with value: 0.7796874046325684.\n",
            "[I 2025-10-07 10:46:24,851] Trial 18 finished with value: 0.9706194996833801 and parameters: {'learning_rate': 0.0035399613545574493, 'max_depth': 6, 'n_estimators': 982, 'subsample': 0.7918778917406631, 'colsample_bytree': 0.8428836569865867}. Best is trial 17 with value: 0.7796874046325684.\n",
            "[I 2025-10-07 10:46:25,192] Trial 19 finished with value: 1.1583434343338013 and parameters: {'learning_rate': 0.005456169520714403, 'max_depth': 10, 'n_estimators': 247, 'subsample': 0.6849862867939259, 'colsample_bytree': 0.7943616964005928}. Best is trial 17 with value: 0.7796874046325684.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.004249999871114709, 'max_depth': 9, 'n_estimators': 997, 'subsample': 0.821044185306938, 'colsample_bytree': 0.798904314681874}\n",
            "[Fold 3] MAE: 0.9262, MSE: 2.2136, R: 0.6852\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n",
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/3]: 100%|| 4/4 [00:03<00:00,  1.12it/s, MSE=118]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 129.4066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/3]: 100%|| 4/4 [00:02<00:00,  1.40it/s, MSE=104]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 116.5616\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/3]: 100%|| 4/4 [00:02<00:00,  1.68it/s, MSE=90.2]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 110.2613\n",
            "[INFO] Partial fine-tuning complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:01<00:00,  2.06it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  2.87it/s]\n",
            "[I 2025-10-07 10:46:40,036] A new study created in memory with name: no-name-ec0ecadc-f486-42c0-b435-a0995d32ae4a\n",
            "[I 2025-10-07 10:46:41,496] Trial 0 finished with value: 1.8457821607589722 and parameters: {'learning_rate': 0.01875415249219911, 'max_depth': 5, 'n_estimators': 977, 'subsample': 0.929479081892802, 'colsample_bytree': 0.9693223318895019}. Best is trial 0 with value: 1.8457821607589722.\n",
            "[I 2025-10-07 10:46:42,595] Trial 1 finished with value: 1.5634962320327759 and parameters: {'learning_rate': 0.004314083120910284, 'max_depth': 7, 'n_estimators': 781, 'subsample': 0.7321631534224585, 'colsample_bytree': 0.8287292383272415}. Best is trial 1 with value: 1.5634962320327759.\n",
            "[I 2025-10-07 10:46:43,304] Trial 2 finished with value: 1.6653695106506348 and parameters: {'learning_rate': 0.008868012780031593, 'max_depth': 5, 'n_estimators': 510, 'subsample': 0.5606600392780365, 'colsample_bytree': 0.8724555912050029}. Best is trial 1 with value: 1.5634962320327759.\n",
            "[I 2025-10-07 10:46:43,951] Trial 3 finished with value: 1.8821204900741577 and parameters: {'learning_rate': 0.002258164344525801, 'max_depth': 7, 'n_estimators': 466, 'subsample': 0.8097599327902254, 'colsample_bytree': 0.7593055655901806}. Best is trial 1 with value: 1.5634962320327759.\n",
            "[I 2025-10-07 10:46:44,151] Trial 4 finished with value: 2.1438090801239014 and parameters: {'learning_rate': 0.1244627247071444, 'max_depth': 4, 'n_estimators': 163, 'subsample': 0.8639032591884801, 'colsample_bytree': 0.6276177728458943}. Best is trial 1 with value: 1.5634962320327759.\n",
            "[I 2025-10-07 10:46:44,545] Trial 5 finished with value: 1.6610536575317383 and parameters: {'learning_rate': 0.0030414256786980495, 'max_depth': 10, 'n_estimators': 286, 'subsample': 0.9955085139915383, 'colsample_bytree': 0.6507714716279931}. Best is trial 1 with value: 1.5634962320327759.\n",
            "[I 2025-10-07 10:46:45,058] Trial 6 finished with value: 1.9422234296798706 and parameters: {'learning_rate': 0.24734301722331906, 'max_depth': 8, 'n_estimators': 430, 'subsample': 0.7716932873913752, 'colsample_bytree': 0.9314768272408499}. Best is trial 1 with value: 1.5634962320327759.\n",
            "[I 2025-10-07 10:46:46,043] Trial 7 finished with value: 1.3924583196640015 and parameters: {'learning_rate': 0.06942220609420352, 'max_depth': 7, 'n_estimators': 759, 'subsample': 0.7586656301726735, 'colsample_bytree': 0.8533259765703324}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:46,796] Trial 8 finished with value: 2.3135581016540527 and parameters: {'learning_rate': 0.12867089664945094, 'max_depth': 8, 'n_estimators': 641, 'subsample': 0.7965685890791748, 'colsample_bytree': 0.7589526090388141}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:48,150] Trial 9 finished with value: 1.8418760299682617 and parameters: {'learning_rate': 0.02500123920230416, 'max_depth': 10, 'n_estimators': 844, 'subsample': 0.7248172841741561, 'colsample_bytree': 0.5107649932910474}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:48,872] Trial 10 finished with value: 1.5328410863876343 and parameters: {'learning_rate': 0.045228792429463135, 'max_depth': 3, 'n_estimators': 701, 'subsample': 0.6114210716744639, 'colsample_bytree': 0.8503731178914897}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:49,636] Trial 11 finished with value: 1.7871594429016113 and parameters: {'learning_rate': 0.048529917273424594, 'max_depth': 3, 'n_estimators': 739, 'subsample': 0.5821585531694973, 'colsample_bytree': 0.8673293101846562}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:50,480] Trial 12 finished with value: 1.8659027814865112 and parameters: {'learning_rate': 0.04736463649120824, 'max_depth': 6, 'n_estimators': 667, 'subsample': 0.6536289064436821, 'colsample_bytree': 0.997485862907779}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:51,459] Trial 13 finished with value: 1.742414116859436 and parameters: {'learning_rate': 0.0010185589267768986, 'max_depth': 3, 'n_estimators': 916, 'subsample': 0.6593549881122355, 'colsample_bytree': 0.8159211759731327}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:52,224] Trial 14 finished with value: 1.7953672409057617 and parameters: {'learning_rate': 0.053145879144908564, 'max_depth': 8, 'n_estimators': 608, 'subsample': 0.5067124865286275, 'colsample_bytree': 0.6819519499045993}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:53,053] Trial 15 finished with value: 1.7825013399124146 and parameters: {'learning_rate': 0.09319098415970341, 'max_depth': 6, 'n_estimators': 856, 'subsample': 0.6476361351306424, 'colsample_bytree': 0.9059332827901705}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:54,066] Trial 16 finished with value: 1.5440791845321655 and parameters: {'learning_rate': 0.01048167493207772, 'max_depth': 5, 'n_estimators': 710, 'subsample': 0.6957756028871045, 'colsample_bytree': 0.8008473144641821}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:54,577] Trial 17 finished with value: 2.242182970046997 and parameters: {'learning_rate': 0.29140301492527837, 'max_depth': 9, 'n_estimators': 594, 'subsample': 0.8507746726847278, 'colsample_bytree': 0.7047601062086171}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:55,014] Trial 18 finished with value: 1.7706187963485718 and parameters: {'learning_rate': 0.022571663114168944, 'max_depth': 4, 'n_estimators': 367, 'subsample': 0.5817177166128448, 'colsample_bytree': 0.5702493680498261}. Best is trial 7 with value: 1.3924583196640015.\n",
            "[I 2025-10-07 10:46:55,822] Trial 19 finished with value: 1.8348437547683716 and parameters: {'learning_rate': 0.07520938558421918, 'max_depth': 4, 'n_estimators': 791, 'subsample': 0.5233920856357391, 'colsample_bytree': 0.9380567426558586}. Best is trial 7 with value: 1.3924583196640015.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.06942220609420352, 'max_depth': 7, 'n_estimators': 759, 'subsample': 0.7586656301726735, 'colsample_bytree': 0.8533259765703324}\n",
            "[Fold 4] MAE: 1.3803, MSE: 3.5034, R: 0.5567\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n",
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/3]: 100%|| 4/4 [00:01<00:00,  2.22it/s, MSE=156]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 164.9930\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/3]: 100%|| 4/4 [00:02<00:00,  1.57it/s, MSE=147]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 152.2047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/3]: 100%|| 4/4 [00:01<00:00,  2.37it/s, MSE=144]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 143.2243\n",
            "[INFO] Partial fine-tuning complete.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:01<00:00,  2.56it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.17it/s]\n",
            "[I 2025-10-07 10:47:05,436] A new study created in memory with name: no-name-905932fe-bdda-491b-b812-40d0f34f6c06\n",
            "[I 2025-10-07 10:47:06,446] Trial 0 finished with value: 3.220717191696167 and parameters: {'learning_rate': 0.08098644119159887, 'max_depth': 5, 'n_estimators': 819, 'subsample': 0.5095696910714566, 'colsample_bytree': 0.7187989940017574}. Best is trial 0 with value: 3.220717191696167.\n",
            "[I 2025-10-07 10:47:06,763] Trial 1 finished with value: 3.056748628616333 and parameters: {'learning_rate': 0.25720590750014644, 'max_depth': 4, 'n_estimators': 311, 'subsample': 0.6264743415772844, 'colsample_bytree': 0.7364074628463547}. Best is trial 1 with value: 3.056748628616333.\n",
            "[I 2025-10-07 10:47:07,380] Trial 2 finished with value: 3.3562557697296143 and parameters: {'learning_rate': 0.0011666727239423025, 'max_depth': 7, 'n_estimators': 453, 'subsample': 0.5941985738301898, 'colsample_bytree': 0.5448887434108394}. Best is trial 1 with value: 3.056748628616333.\n",
            "[I 2025-10-07 10:47:07,813] Trial 3 finished with value: 3.1249208450317383 and parameters: {'learning_rate': 0.0031808348463644304, 'max_depth': 5, 'n_estimators': 336, 'subsample': 0.6627222913309445, 'colsample_bytree': 0.6048427131348135}. Best is trial 1 with value: 3.056748628616333.\n",
            "[I 2025-10-07 10:47:08,819] Trial 4 finished with value: 2.9240119457244873 and parameters: {'learning_rate': 0.006000247236258381, 'max_depth': 4, 'n_estimators': 819, 'subsample': 0.6567800200694737, 'colsample_bytree': 0.7059785440434362}. Best is trial 4 with value: 2.9240119457244873.\n",
            "[I 2025-10-07 10:47:09,531] Trial 5 finished with value: 2.688701629638672 and parameters: {'learning_rate': 0.0044057082143656445, 'max_depth': 3, 'n_estimators': 676, 'subsample': 0.8324473749649779, 'colsample_bytree': 0.5526852835664535}. Best is trial 5 with value: 2.688701629638672.\n",
            "[I 2025-10-07 10:47:10,134] Trial 6 finished with value: 2.693486213684082 and parameters: {'learning_rate': 0.002546800043391873, 'max_depth': 10, 'n_estimators': 395, 'subsample': 0.8947820171516252, 'colsample_bytree': 0.7676441796939821}. Best is trial 5 with value: 2.688701629638672.\n",
            "[I 2025-10-07 10:47:10,744] Trial 7 finished with value: 2.2811832427978516 and parameters: {'learning_rate': 0.009833198738342173, 'max_depth': 7, 'n_estimators': 434, 'subsample': 0.9011334440278174, 'colsample_bytree': 0.6069609355061361}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:11,312] Trial 8 finished with value: 2.9664318561553955 and parameters: {'learning_rate': 0.023984318597029652, 'max_depth': 3, 'n_estimators': 528, 'subsample': 0.7243148768805003, 'colsample_bytree': 0.5901444340495066}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:12,276] Trial 9 finished with value: 2.3491785526275635 and parameters: {'learning_rate': 0.07002143195832926, 'max_depth': 7, 'n_estimators': 891, 'subsample': 0.8912284282664398, 'colsample_bytree': 0.7750976649318662}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:12,575] Trial 10 finished with value: 2.6483802795410156 and parameters: {'learning_rate': 0.014210975741636706, 'max_depth': 9, 'n_estimators': 142, 'subsample': 0.9930989291710414, 'colsample_bytree': 0.9396921671993812}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:13,876] Trial 11 finished with value: 2.826571464538574 and parameters: {'learning_rate': 0.04371795134903203, 'max_depth': 7, 'n_estimators': 980, 'subsample': 0.8763239671861158, 'colsample_bytree': 0.8529635225188444}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:14,644] Trial 12 finished with value: 2.770425796508789 and parameters: {'learning_rate': 0.134057828178449, 'max_depth': 8, 'n_estimators': 649, 'subsample': 0.9997603783616055, 'colsample_bytree': 0.8348566146828695}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:16,193] Trial 13 finished with value: 2.6075761318206787 and parameters: {'learning_rate': 0.013637168948170431, 'max_depth': 6, 'n_estimators': 981, 'subsample': 0.7991699360531307, 'colsample_bytree': 0.6251722485816595}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:16,450] Trial 14 finished with value: 2.426798105239868 and parameters: {'learning_rate': 0.03682910751251713, 'max_depth': 8, 'n_estimators': 155, 'subsample': 0.9276820606641543, 'colsample_bytree': 0.6615595455848552}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:17,148] Trial 15 finished with value: 2.8879177570343018 and parameters: {'learning_rate': 0.07373897325805717, 'max_depth': 6, 'n_estimators': 663, 'subsample': 0.7573647273247912, 'colsample_bytree': 0.804916920098796}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:18,563] Trial 16 finished with value: 2.470998764038086 and parameters: {'learning_rate': 0.0086207528746168, 'max_depth': 8, 'n_estimators': 818, 'subsample': 0.9249835315354731, 'colsample_bytree': 0.9226117950665783}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:18,818] Trial 17 finished with value: 3.029115676879883 and parameters: {'learning_rate': 0.24661590713782341, 'max_depth': 10, 'n_estimators': 259, 'subsample': 0.8248994153476839, 'colsample_bytree': 0.5058583094515117}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:19,680] Trial 18 finished with value: 2.41359281539917 and parameters: {'learning_rate': 0.027940526676426988, 'max_depth': 7, 'n_estimators': 573, 'subsample': 0.9434586240847047, 'colsample_bytree': 0.6726251241269872}. Best is trial 7 with value: 2.2811832427978516.\n",
            "[I 2025-10-07 10:47:20,276] Trial 19 finished with value: 2.862316370010376 and parameters: {'learning_rate': 0.06227003842731445, 'max_depth': 9, 'n_estimators': 517, 'subsample': 0.8578855337663824, 'colsample_bytree': 0.8841719738349674}. Best is trial 7 with value: 2.2811832427978516.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.009833198738342173, 'max_depth': 7, 'n_estimators': 434, 'subsample': 0.9011334440278174, 'colsample_bytree': 0.6069609355061361}\n",
            "[Fold 5] MAE: 2.2891, MSE: 12.1819, R: 0.2157\n",
            "\n",
            "===== Final Cross-Validation Results =====\n",
            "   fold       MAE        MSE        R2\n",
            "0     1  2.365095  10.589876 -0.339522\n",
            "1     2  2.545492  15.319171 -2.693097\n",
            "2     3  0.926240   2.213629  0.685229\n",
            "3     4  1.380315   3.503357  0.556662\n",
            "4     5  2.289130  12.181928  0.215714\n",
            "\n",
            "Mean MAE: 1.9012542843818665\n",
            "Mean R: -0.31500294208526614\n"
          ]
        }
      ],
      "source": [
        "results = run_cnn_xgb_kfold_finetune(df, n_splits=5, batch_size=8, xgb_trials=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay-BMIZrVwil"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# cnn_xgb_finetune_pipeline_fixed.py\n",
        "# ----------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 1 Dataset class\n",
        "# ============================================================\n",
        "class ImageRegressionDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[\"Filename\"]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = torch.tensor(row[\"Hb\"], dtype=torch.float32)\n",
        "        return img, label\n",
        "\n",
        "# ============================================================\n",
        "# 2 Partial Fine-Tuning of ResNet18 Backbone\n",
        "# ============================================================\n",
        "def fine_tune_backbone_partial(backbone, train_df, transform, device, epochs=3, batch_size=8, lr=1e-4):\n",
        "    print(f\"[INFO] Starting partial fine-tuning on {len(train_df)} labeled samples...\")\n",
        "\n",
        "    # Freeze low-level layers (conv1 to layer3)\n",
        "    for name, param in backbone.named_parameters():\n",
        "        if not name.startswith(\"layer4\") and not name.startswith(\"fc\"):\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Replace final classification head with regression head\n",
        "    in_features = backbone.fc.in_features\n",
        "    backbone.fc = nn.Linear(in_features, 1)  # regression head\n",
        "\n",
        "    dataset = ImageRegressionDataset(train_df, transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, backbone.parameters()), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    backbone.to(device)\n",
        "    backbone.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(loader, desc=f\"[Fine-Tune Epoch {epoch+1}/{epochs}]\")\n",
        "        for imgs, targets in pbar:\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = backbone(imgs).view(-1)\n",
        "            loss = criterion(preds, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            pbar.set_postfix({\"MSE\": loss.item()})\n",
        "        avg_loss = total_loss / len(dataset)\n",
        "        print(f\"[INFO] Fine-tune Epoch {epoch+1} Avg MSE: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"[INFO] Partial fine-tuning complete.\")\n",
        "    return backbone\n",
        "\n",
        "# ============================================================\n",
        "# 3 Embedding Extraction\n",
        "# ============================================================\n",
        "def extract_embeddings(backbone, df, transform, device, batch_size=8):\n",
        "    backbone.eval()\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    # Remove final FC layer for feature extraction\n",
        "    feature_extractor = nn.Sequential(*list(backbone.children())[:-1]).to(device)\n",
        "\n",
        "    loader = DataLoader(ImageRegressionDataset(df, transform), batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in tqdm(loader, desc=\"Extracting embeddings\"):\n",
        "            imgs = imgs.to(device)\n",
        "            feats = feature_extractor(imgs)  # shape: [B, C, 1, 1]\n",
        "            pooled = feats.view(feats.size(0), -1)  # flatten to [B, C]\n",
        "            embeddings.append(pooled.cpu().numpy())\n",
        "            labels.append(targets.numpy())\n",
        "\n",
        "    embeddings = np.concatenate(embeddings)\n",
        "    labels = np.concatenate(labels)\n",
        "    return embeddings, labels\n",
        "\n",
        "# ============================================================\n",
        "# 4 XGBoost Regression with Optuna Optimization\n",
        "# ============================================================\n",
        "def tune_xgb_with_optuna(X_train, y_train, X_val, y_val, n_trials=20):\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"objective\": \"reg:squarederror\",\n",
        "            \"eval_metric\": \"mae\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"device\": \"cuda\",\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        }\n",
        "\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "        preds = model.predict(X_val)\n",
        "        return mean_absolute_error(y_val, preds)\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "    print(\"[Optuna] Best trial:\", study.best_trial.params)\n",
        "    return study.best_trial.params\n",
        "\n",
        "# ============================================================\n",
        "# 5 Full CNN + XGBoost K-Fold Training Pipeline\n",
        "# ============================================================\n",
        "def run_cnn_xgb_kfold_finetune(labeled_df, n_splits=5, batch_size=8, xgb_trials=20):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    metrics = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(labeled_df), 1):\n",
        "        print(f\"\\n--- Fold {fold}/{n_splits} ---\")\n",
        "        train_df, val_df = labeled_df.iloc[train_idx], labeled_df.iloc[val_idx]\n",
        "\n",
        "        print(f\"[INFO] Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
        "\n",
        "        # Load pretrained CNN\n",
        "        backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "        backbone = fine_tune_backbone_partial(backbone, train_df, transform, device, epochs=3, batch_size=batch_size)\n",
        "\n",
        "        # Extract embeddings\n",
        "        X_train, y_train = extract_embeddings(backbone, train_df, transform, device, batch_size)\n",
        "        X_val, y_val = extract_embeddings(backbone, val_df, transform, device, batch_size)\n",
        "\n",
        "        # Tune and train XGBoost\n",
        "        best_params = tune_xgb_with_optuna(X_train, y_train, X_val, y_val, n_trials=xgb_trials)\n",
        "        xgb_model = xgb.XGBRegressor(**best_params)\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        preds = xgb_model.predict(X_val)\n",
        "        mae = mean_absolute_error(y_val, preds)\n",
        "        mse = mean_squared_error(y_val, preds)\n",
        "        r2 = r2_score(y_val, preds)\n",
        "\n",
        "        print(f\"[Fold {fold}] MAE: {mae:.4f}, MSE: {mse:.4f}, R: {r2:.4f}\")\n",
        "        metrics.append({\"fold\": fold, \"MAE\": mae, \"MSE\": mse, \"R2\": r2})\n",
        "\n",
        "    results_df = pd.DataFrame(metrics)\n",
        "    print(\"\\n===== Final Cross-Validation Results =====\")\n",
        "    print(results_df)\n",
        "    print(\"\\nMean MAE:\", results_df[\"MAE\"].mean())\n",
        "    print(\"Mean R:\", results_df[\"R2\"].mean())\n",
        "    return results_df\n",
        "\n",
        "# ============================================================\n",
        "#  Example Usage\n",
        "# ============================================================\n",
        "# df = pd.DataFrame({\"Filename\": [...], \"Hb\": [...]})\n",
        "# results = run_cnn_xgb_kfold_finetune(df, n_splits=5, batch_size=8, xgb_trials=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCZHDDLKWqa_"
      },
      "source": [
        "#### Run Resnet 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7ba41850fa8d4100828e5f655899321d",
            "79da2476cb934d329ec0650f4146dca3",
            "23830de5000342f2af13414addefa744",
            "75b3ddf6450e403bb1dbb8d6b5d8a417",
            "92ef309f4e16493893c5bfa19eaf4019",
            "f477111d3f0a48558e1bcc42674e6d94",
            "dbe03b9b96b24f1b979af43d2beb1601",
            "390db69d337644bd9a984d4b7e94ac1e",
            "156ef0eda8504397aa5495e5dcd2ba43",
            "ce78233a233b47bdb25e9d43c38784ed",
            "22e06509b67c4c098760a4a3dd0d4c09"
          ]
        },
        "id": "LHPsee3AWrCX",
        "outputId": "5c1ba132-55dd-4adc-b68b-938a6daa4600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 1/5 ---\n",
            "[INFO] Training samples: 24, Validation samples: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ba41850fa8d4100828e5f655899321d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Starting partial fine-tuning on 24 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/25]: 100%|| 3/3 [00:01<00:00,  1.65it/s, MSE=142]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 129.4603\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/25]: 100%|| 3/3 [00:02<00:00,  1.15it/s, MSE=133]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 128.7813\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/25]: 100%|| 3/3 [00:02<00:00,  1.21it/s, MSE=158]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 127.7159\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 4/25]: 100%|| 3/3 [00:01<00:00,  1.71it/s, MSE=119]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 4 Avg MSE: 127.3334\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 5/25]: 100%|| 3/3 [00:01<00:00,  1.72it/s, MSE=117]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 5 Avg MSE: 126.4998\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 6/25]: 100%|| 3/3 [00:01<00:00,  1.70it/s, MSE=127]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 6 Avg MSE: 125.8486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 7/25]: 100%|| 3/3 [00:01<00:00,  1.76it/s, MSE=107]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 7 Avg MSE: 125.1377\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 8/25]: 100%|| 3/3 [00:01<00:00,  1.74it/s, MSE=130]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 8 Avg MSE: 124.5194\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 9/25]: 100%|| 3/3 [00:02<00:00,  1.31it/s, MSE=126]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 9 Avg MSE: 123.5079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 10/25]: 100%|| 3/3 [00:02<00:00,  1.05it/s, MSE=144]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 10 Avg MSE: 122.9080\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 11/25]: 100%|| 3/3 [00:01<00:00,  1.55it/s, MSE=102]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 11 Avg MSE: 122.0862\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 12/25]: 100%|| 3/3 [00:01<00:00,  1.64it/s, MSE=132]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 12 Avg MSE: 121.5300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 13/25]: 100%|| 3/3 [00:01<00:00,  1.63it/s, MSE=133]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 13 Avg MSE: 120.3557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 14/25]: 100%|| 3/3 [00:01<00:00,  1.72it/s, MSE=105]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 14 Avg MSE: 120.2222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 15/25]: 100%|| 3/3 [00:01<00:00,  1.70it/s, MSE=111]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 15 Avg MSE: 119.1572\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 16/25]: 100%|| 3/3 [00:02<00:00,  1.31it/s, MSE=126]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 16 Avg MSE: 118.7814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 17/25]: 100%|| 3/3 [00:02<00:00,  1.09it/s, MSE=115]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 17 Avg MSE: 118.2162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 18/25]: 100%|| 3/3 [00:01<00:00,  1.69it/s, MSE=124]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 18 Avg MSE: 117.0482\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 19/25]: 100%|| 3/3 [00:01<00:00,  1.63it/s, MSE=134]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 19 Avg MSE: 116.4044\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 20/25]: 100%|| 3/3 [00:01<00:00,  1.65it/s, MSE=106]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 20 Avg MSE: 116.6245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 21/25]: 100%|| 3/3 [00:01<00:00,  1.67it/s, MSE=109]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 21 Avg MSE: 115.5312\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 22/25]: 100%|| 3/3 [00:01<00:00,  1.69it/s, MSE=104]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 22 Avg MSE: 113.8825\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 23/25]: 100%|| 3/3 [00:02<00:00,  1.38it/s, MSE=102]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 23 Avg MSE: 113.7362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 24/25]: 100%|| 3/3 [00:02<00:00,  1.04it/s, MSE=109]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 24 Avg MSE: 112.5439\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 25/25]: 100%|| 3/3 [00:01<00:00,  1.64it/s, MSE=133]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 25 Avg MSE: 111.8908\n",
            "[INFO] Partial fine-tuning complete.\n",
            "[INFO] Saved fine-tuned model to saved_models/resnet50_fold1.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 3/3 [00:01<00:00,  1.62it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.48it/s]\n",
            "[I 2025-10-07 11:30:53,932] A new study created in memory with name: no-name-ba248724-fcd7-4d15-bfb1-bc36b77bb5ee\n",
            "[I 2025-10-07 11:30:55,813] Trial 0 finished with value: 2.591339588165283 and parameters: {'learning_rate': 0.023545715686934598, 'max_depth': 10, 'n_estimators': 693, 'subsample': 0.9774636909746819, 'colsample_bytree': 0.9737207102446345}. Best is trial 0 with value: 2.591339588165283.\n",
            "[I 2025-10-07 11:30:57,035] Trial 1 finished with value: 2.732860565185547 and parameters: {'learning_rate': 0.02201342078903638, 'max_depth': 10, 'n_estimators': 448, 'subsample': 0.9004961234975744, 'colsample_bytree': 0.8513019424248427}. Best is trial 0 with value: 2.591339588165283.\n",
            "[I 2025-10-07 11:30:57,469] Trial 2 finished with value: 2.3939573764801025 and parameters: {'learning_rate': 0.029730565060848504, 'max_depth': 9, 'n_estimators': 192, 'subsample': 0.8419012835837372, 'colsample_bytree': 0.538871292098458}. Best is trial 2 with value: 2.3939573764801025.\n",
            "[I 2025-10-07 11:30:58,731] Trial 3 finished with value: 2.8423879146575928 and parameters: {'learning_rate': 0.001061130653039258, 'max_depth': 5, 'n_estimators': 632, 'subsample': 0.7193712158381503, 'colsample_bytree': 0.9520105732665602}. Best is trial 2 with value: 2.3939573764801025.\n",
            "[I 2025-10-07 11:30:59,877] Trial 4 finished with value: 2.8850626945495605 and parameters: {'learning_rate': 0.0027555231332328195, 'max_depth': 10, 'n_estimators': 544, 'subsample': 0.7069051618478623, 'colsample_bytree': 0.8699401359197232}. Best is trial 2 with value: 2.3939573764801025.\n",
            "[I 2025-10-07 11:31:01,668] Trial 5 finished with value: 2.822582483291626 and parameters: {'learning_rate': 0.0013159908651394162, 'max_depth': 6, 'n_estimators': 858, 'subsample': 0.5406995201623553, 'colsample_bytree': 0.9070469589306771}. Best is trial 2 with value: 2.3939573764801025.\n",
            "[I 2025-10-07 11:31:02,963] Trial 6 finished with value: 2.810009479522705 and parameters: {'learning_rate': 0.004034882670878108, 'max_depth': 10, 'n_estimators': 556, 'subsample': 0.6948100410084067, 'colsample_bytree': 0.6783920979181617}. Best is trial 2 with value: 2.3939573764801025.\n",
            "[I 2025-10-07 11:31:04,367] Trial 7 finished with value: 2.7090251445770264 and parameters: {'learning_rate': 0.001290766230154712, 'max_depth': 5, 'n_estimators': 862, 'subsample': 0.5905274333741997, 'colsample_bytree': 0.5382329047039899}. Best is trial 2 with value: 2.3939573764801025.\n",
            "[I 2025-10-07 11:31:05,467] Trial 8 finished with value: 2.6887128353118896 and parameters: {'learning_rate': 0.0430722119313883, 'max_depth': 5, 'n_estimators': 649, 'subsample': 0.7849106027279431, 'colsample_bytree': 0.7109511409837805}. Best is trial 2 with value: 2.3939573764801025.\n",
            "[I 2025-10-07 11:31:05,807] Trial 9 finished with value: 2.872183084487915 and parameters: {'learning_rate': 0.0029020838488692515, 'max_depth': 6, 'n_estimators': 179, 'subsample': 0.661713161941575, 'colsample_bytree': 0.9783370411681149}. Best is trial 2 with value: 2.3939573764801025.\n",
            "[I 2025-10-07 11:31:06,036] Trial 10 finished with value: 2.473731756210327 and parameters: {'learning_rate': 0.24106436670175457, 'max_depth': 8, 'n_estimators': 137, 'subsample': 0.8554301462542405, 'colsample_bytree': 0.5018719313216341}. Best is trial 2 with value: 2.3939573764801025.\n",
            "[I 2025-10-07 11:31:06,213] Trial 11 finished with value: 2.574040651321411 and parameters: {'learning_rate': 0.29436042523163247, 'max_depth': 8, 'n_estimators': 102, 'subsample': 0.8399682671094761, 'colsample_bytree': 0.525677300739433}. Best is trial 2 with value: 2.3939573764801025.\n",
            "[I 2025-10-07 11:31:06,613] Trial 12 finished with value: 1.9810038805007935 and parameters: {'learning_rate': 0.1792489359282435, 'max_depth': 8, 'n_estimators': 294, 'subsample': 0.8796794498382053, 'colsample_bytree': 0.6056931156705646}. Best is trial 12 with value: 1.9810038805007935.\n",
            "[I 2025-10-07 11:31:07,085] Trial 13 finished with value: 2.163661241531372 and parameters: {'learning_rate': 0.09884068012250266, 'max_depth': 8, 'n_estimators': 331, 'subsample': 0.9969699020469747, 'colsample_bytree': 0.6040997625206573}. Best is trial 12 with value: 1.9810038805007935.\n",
            "[I 2025-10-07 11:31:07,586] Trial 14 finished with value: 1.532310128211975 and parameters: {'learning_rate': 0.09078566577605175, 'max_depth': 8, 'n_estimators': 351, 'subsample': 0.9994742754864497, 'colsample_bytree': 0.633116324223718}. Best is trial 14 with value: 1.532310128211975.\n",
            "[I 2025-10-07 11:31:08,139] Trial 15 finished with value: 1.9123775959014893 and parameters: {'learning_rate': 0.10224027760583963, 'max_depth': 7, 'n_estimators': 375, 'subsample': 0.9415463793308244, 'colsample_bytree': 0.6359473066604197}. Best is trial 14 with value: 1.532310128211975.\n",
            "[I 2025-10-07 11:31:08,608] Trial 16 finished with value: 2.833421468734741 and parameters: {'learning_rate': 0.07563631025662283, 'max_depth': 3, 'n_estimators': 369, 'subsample': 0.9377499203427924, 'colsample_bytree': 0.7743366200945293}. Best is trial 14 with value: 1.532310128211975.\n",
            "[I 2025-10-07 11:31:09,569] Trial 17 finished with value: 2.007366895675659 and parameters: {'learning_rate': 0.009352500665858946, 'max_depth': 7, 'n_estimators': 437, 'subsample': 0.9397851122953974, 'colsample_bytree': 0.6236287789807597}. Best is trial 14 with value: 1.532310128211975.\n",
            "[I 2025-10-07 11:31:10,154] Trial 18 finished with value: 2.8489699363708496 and parameters: {'learning_rate': 0.08168513796569842, 'max_depth': 7, 'n_estimators': 260, 'subsample': 0.7655642496704314, 'colsample_bytree': 0.7762446236861238}. Best is trial 14 with value: 1.532310128211975.\n",
            "[I 2025-10-07 11:31:10,803] Trial 19 finished with value: 1.9392006397247314 and parameters: {'learning_rate': 0.012323401560959865, 'max_depth': 3, 'n_estimators': 438, 'subsample': 0.9392748931691474, 'colsample_bytree': 0.6680985815543967}. Best is trial 14 with value: 1.532310128211975.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.09078566577605175, 'max_depth': 8, 'n_estimators': 351, 'subsample': 0.9994742754864497, 'colsample_bytree': 0.633116324223718}\n",
            "[Fold 1 | Model: resnet50] MAE: 2.7078, MSE: 11.5725, R: -0.4638\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n",
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/25]: 100%|| 4/4 [00:02<00:00,  1.47it/s, MSE=112]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 152.2493\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/25]: 100%|| 4/4 [00:02<00:00,  1.71it/s, MSE=138]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 151.4536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/25]: 100%|| 4/4 [00:01<00:00,  2.09it/s, MSE=110]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 150.2723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 4/25]: 100%|| 4/4 [00:01<00:00,  2.29it/s, MSE=110]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 4 Avg MSE: 149.1583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 5/25]: 100%|| 4/4 [00:01<00:00,  2.25it/s, MSE=127]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 5 Avg MSE: 147.8352\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 6/25]: 100%|| 4/4 [00:01<00:00,  2.27it/s, MSE=137]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 6 Avg MSE: 147.3941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 7/25]: 100%|| 4/4 [00:01<00:00,  2.30it/s, MSE=14.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 7 Avg MSE: 145.9208\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 8/25]: 100%|| 4/4 [00:02<00:00,  1.56it/s, MSE=106]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 8 Avg MSE: 144.7046\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 9/25]: 100%|| 4/4 [00:02<00:00,  1.61it/s, MSE=132]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 9 Avg MSE: 143.7267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 10/25]: 100%|| 4/4 [00:01<00:00,  2.33it/s, MSE=283]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 10 Avg MSE: 143.0756\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 11/25]: 100%|| 4/4 [00:01<00:00,  2.33it/s, MSE=120]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 11 Avg MSE: 141.7724\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 12/25]: 100%|| 4/4 [00:01<00:00,  2.28it/s, MSE=119]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 12 Avg MSE: 140.6394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 13/25]: 100%|| 4/4 [00:01<00:00,  2.38it/s, MSE=101]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 13 Avg MSE: 140.1532\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 14/25]: 100%|| 4/4 [00:01<00:00,  2.30it/s, MSE=49.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 14 Avg MSE: 138.6619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 15/25]: 100%|| 4/4 [00:02<00:00,  1.80it/s, MSE=101]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 15 Avg MSE: 138.2973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 16/25]: 100%|| 4/4 [00:02<00:00,  1.43it/s, MSE=125]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 16 Avg MSE: 136.8976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 17/25]: 100%|| 4/4 [00:01<00:00,  2.27it/s, MSE=97.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 17 Avg MSE: 134.6764\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 18/25]: 100%|| 4/4 [00:01<00:00,  2.22it/s, MSE=125]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 18 Avg MSE: 134.6853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 19/25]: 100%|| 4/4 [00:01<00:00,  2.28it/s, MSE=95.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 19 Avg MSE: 134.7872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 20/25]: 100%|| 4/4 [00:01<00:00,  2.33it/s, MSE=121]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 20 Avg MSE: 133.1675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 21/25]: 100%|| 4/4 [00:01<00:00,  2.29it/s, MSE=96.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 21 Avg MSE: 131.8146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 22/25]: 100%|| 4/4 [00:02<00:00,  1.85it/s, MSE=269]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 22 Avg MSE: 131.4289\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 23/25]: 100%|| 4/4 [00:02<00:00,  1.45it/s, MSE=257]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 23 Avg MSE: 130.1714\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 24/25]: 100%|| 4/4 [00:01<00:00,  2.33it/s, MSE=96.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 24 Avg MSE: 129.2335\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 25/25]: 100%|| 4/4 [00:01<00:00,  2.30it/s, MSE=114]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 25 Avg MSE: 128.3980\n",
            "[INFO] Partial fine-tuning complete.\n",
            "[INFO] Saved fine-tuned model to saved_models/resnet50_fold2.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:01<00:00,  2.29it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.50it/s]\n",
            "[I 2025-10-07 11:32:05,144] A new study created in memory with name: no-name-26da0fa6-10df-4059-93b8-726b665f0609\n",
            "[I 2025-10-07 11:32:06,533] Trial 0 finished with value: 1.745378851890564 and parameters: {'learning_rate': 0.006847711458764985, 'max_depth': 8, 'n_estimators': 492, 'subsample': 0.8000480330181036, 'colsample_bytree': 0.9776552012583959}. Best is trial 0 with value: 1.745378851890564.\n",
            "[I 2025-10-07 11:32:07,725] Trial 1 finished with value: 1.7926640510559082 and parameters: {'learning_rate': 0.018973984063499667, 'max_depth': 10, 'n_estimators': 525, 'subsample': 0.5385942414941265, 'colsample_bytree': 0.97389233826799}. Best is trial 0 with value: 1.745378851890564.\n",
            "[I 2025-10-07 11:32:09,707] Trial 2 finished with value: 1.5742541551589966 and parameters: {'learning_rate': 0.03262443706075405, 'max_depth': 8, 'n_estimators': 999, 'subsample': 0.7190874054318949, 'colsample_bytree': 0.8613707388759572}. Best is trial 2 with value: 1.5742541551589966.\n",
            "[I 2025-10-07 11:32:10,869] Trial 3 finished with value: 1.7327321767807007 and parameters: {'learning_rate': 0.038208239729296524, 'max_depth': 6, 'n_estimators': 510, 'subsample': 0.5101862783730899, 'colsample_bytree': 0.7449478889636789}. Best is trial 2 with value: 1.5742541551589966.\n",
            "[I 2025-10-07 11:32:12,575] Trial 4 finished with value: 2.09737229347229 and parameters: {'learning_rate': 0.0014595305575653735, 'max_depth': 10, 'n_estimators': 756, 'subsample': 0.5369344790763018, 'colsample_bytree': 0.8139456160093614}. Best is trial 2 with value: 1.5742541551589966.\n",
            "[I 2025-10-07 11:32:14,346] Trial 5 finished with value: 1.8603429794311523 and parameters: {'learning_rate': 0.002895400366637907, 'max_depth': 7, 'n_estimators': 714, 'subsample': 0.8384347702801693, 'colsample_bytree': 0.8645911353842279}. Best is trial 2 with value: 1.5742541551589966.\n",
            "[I 2025-10-07 11:32:14,757] Trial 6 finished with value: 1.8467273712158203 and parameters: {'learning_rate': 0.2857111130804372, 'max_depth': 4, 'n_estimators': 375, 'subsample': 0.7666188665432193, 'colsample_bytree': 0.8515278177780747}. Best is trial 2 with value: 1.5742541551589966.\n",
            "[I 2025-10-07 11:32:15,593] Trial 7 finished with value: 1.9394069910049438 and parameters: {'learning_rate': 0.17873443987494553, 'max_depth': 3, 'n_estimators': 895, 'subsample': 0.6383507103862676, 'colsample_bytree': 0.6101232367168026}. Best is trial 2 with value: 1.5742541551589966.\n",
            "[I 2025-10-07 11:32:16,115] Trial 8 finished with value: 1.4860615730285645 and parameters: {'learning_rate': 0.12720031581000485, 'max_depth': 6, 'n_estimators': 289, 'subsample': 0.5393833476708725, 'colsample_bytree': 0.665425881318441}. Best is trial 8 with value: 1.4860615730285645.\n",
            "[I 2025-10-07 11:32:16,622] Trial 9 finished with value: 1.7830082178115845 and parameters: {'learning_rate': 0.015710201468138053, 'max_depth': 3, 'n_estimators': 340, 'subsample': 0.7624422186176345, 'colsample_bytree': 0.815659626384786}. Best is trial 8 with value: 1.4860615730285645.\n",
            "[I 2025-10-07 11:32:16,871] Trial 10 finished with value: 1.9849447011947632 and parameters: {'learning_rate': 0.09506531565419375, 'max_depth': 5, 'n_estimators': 118, 'subsample': 0.9785910389256502, 'colsample_bytree': 0.5679364464724878}. Best is trial 8 with value: 1.4860615730285645.\n",
            "[I 2025-10-07 11:32:18,210] Trial 11 finished with value: 2.166574478149414 and parameters: {'learning_rate': 0.06327053488845329, 'max_depth': 8, 'n_estimators': 983, 'subsample': 0.7072441731300126, 'colsample_bytree': 0.6674359347588373}. Best is trial 8 with value: 1.4860615730285645.\n",
            "[I 2025-10-07 11:32:18,601] Trial 12 finished with value: 1.9389296770095825 and parameters: {'learning_rate': 0.041485080505511515, 'max_depth': 8, 'n_estimators': 180, 'subsample': 0.628689384547206, 'colsample_bytree': 0.6836845438239574}. Best is trial 8 with value: 1.4860615730285645.\n",
            "[I 2025-10-07 11:32:19,506] Trial 13 finished with value: 2.2730538845062256 and parameters: {'learning_rate': 0.0957730196510803, 'max_depth': 6, 'n_estimators': 705, 'subsample': 0.906773694703829, 'colsample_bytree': 0.7405061539226648}. Best is trial 8 with value: 1.4860615730285645.\n",
            "[I 2025-10-07 11:32:20,073] Trial 14 finished with value: 1.6667742729187012 and parameters: {'learning_rate': 0.017884186168418706, 'max_depth': 7, 'n_estimators': 280, 'subsample': 0.651561236843869, 'colsample_bytree': 0.5082287966380968}. Best is trial 8 with value: 1.4860615730285645.\n",
            "[I 2025-10-07 11:32:21,479] Trial 15 finished with value: 1.3431153297424316 and parameters: {'learning_rate': 0.008516820716727967, 'max_depth': 9, 'n_estimators': 628, 'subsample': 0.5968764703880347, 'colsample_bytree': 0.9030293926940496}. Best is trial 15 with value: 1.3431153297424316.\n",
            "[I 2025-10-07 11:32:22,950] Trial 16 finished with value: 1.7473853826522827 and parameters: {'learning_rate': 0.0053383940927606705, 'max_depth': 9, 'n_estimators': 634, 'subsample': 0.5872899457891724, 'colsample_bytree': 0.9231498292824742}. Best is trial 15 with value: 1.3431153297424316.\n",
            "[I 2025-10-07 11:32:23,948] Trial 17 finished with value: 1.8875900506973267 and parameters: {'learning_rate': 0.006977559025284696, 'max_depth': 5, 'n_estimators': 447, 'subsample': 0.5829073840529349, 'colsample_bytree': 0.6771797848822296}. Best is trial 15 with value: 1.3431153297424316.\n",
            "[I 2025-10-07 11:32:25,268] Trial 18 finished with value: 2.1034834384918213 and parameters: {'learning_rate': 0.0010833998187091873, 'max_depth': 9, 'n_estimators': 616, 'subsample': 0.5823924082924221, 'colsample_bytree': 0.6159510163000892}. Best is trial 15 with value: 1.3431153297424316.\n",
            "[I 2025-10-07 11:32:25,914] Trial 19 finished with value: 1.5178219079971313 and parameters: {'learning_rate': 0.0115520076411294, 'max_depth': 5, 'n_estimators': 259, 'subsample': 0.6895470666180644, 'colsample_bytree': 0.9185092713696954}. Best is trial 15 with value: 1.3431153297424316.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.008516820716727967, 'max_depth': 9, 'n_estimators': 628, 'subsample': 0.5968764703880347, 'colsample_bytree': 0.9030293926940496}\n",
            "[Fold 2 | Model: resnet50] MAE: 2.0842, MSE: 7.1822, R: -0.7315\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n",
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/25]: 100%|| 4/4 [00:01<00:00,  2.13it/s, MSE=114]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 145.6545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/25]: 100%|| 4/4 [00:01<00:00,  2.13it/s, MSE=142]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 144.4617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/25]: 100%|| 4/4 [00:02<00:00,  1.34it/s, MSE=141]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 143.6072\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 4/25]: 100%|| 4/4 [00:02<00:00,  1.70it/s, MSE=140]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 4 Avg MSE: 142.3879\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 5/25]: 100%|| 4/4 [00:01<00:00,  2.13it/s, MSE=248]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 5 Avg MSE: 141.2575\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 6/25]: 100%|| 4/4 [00:01<00:00,  2.15it/s, MSE=246]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 6 Avg MSE: 140.3074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 7/25]: 100%|| 4/4 [00:01<00:00,  2.19it/s, MSE=108]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 7 Avg MSE: 139.3036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 8/25]: 100%|| 4/4 [00:01<00:00,  2.14it/s, MSE=107]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 8 Avg MSE: 138.3965\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 9/25]: 100%|| 4/4 [00:02<00:00,  1.78it/s, MSE=134]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 9 Avg MSE: 137.1914\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 10/25]: 100%|| 4/4 [00:02<00:00,  1.34it/s, MSE=42.2]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 10 Avg MSE: 136.5644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 11/25]: 100%|| 4/4 [00:01<00:00,  2.12it/s, MSE=105]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 11 Avg MSE: 135.8864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 12/25]: 100%|| 4/4 [00:01<00:00,  2.14it/s, MSE=280]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 12 Avg MSE: 134.5666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 13/25]: 100%|| 4/4 [00:01<00:00,  2.11it/s, MSE=12.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 13 Avg MSE: 133.8696\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 14/25]: 100%|| 4/4 [00:01<00:00,  2.16it/s, MSE=279]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 14 Avg MSE: 132.6595\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 15/25]: 100%|| 4/4 [00:01<00:00,  2.16it/s, MSE=125]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 15 Avg MSE: 131.9413\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 16/25]: 100%|| 4/4 [00:02<00:00,  1.53it/s, MSE=12.2]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 16 Avg MSE: 130.9099\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 17/25]: 100%|| 4/4 [00:02<00:00,  1.52it/s, MSE=271]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 17 Avg MSE: 129.2843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 18/25]: 100%|| 4/4 [00:01<00:00,  2.14it/s, MSE=97.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 18 Avg MSE: 128.9479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 19/25]: 100%|| 4/4 [00:01<00:00,  2.20it/s, MSE=273]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 19 Avg MSE: 127.6952\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 20/25]: 100%|| 4/4 [00:01<00:00,  2.21it/s, MSE=123]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 20 Avg MSE: 127.3519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 21/25]: 100%|| 4/4 [00:01<00:00,  2.14it/s, MSE=95.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 21 Avg MSE: 126.7321\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 22/25]: 100%|| 4/4 [00:01<00:00,  2.18it/s, MSE=46.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 22 Avg MSE: 124.6280\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 23/25]: 100%|| 4/4 [00:02<00:00,  1.36it/s, MSE=96.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 23 Avg MSE: 123.9430\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 24/25]: 100%|| 4/4 [00:02<00:00,  1.67it/s, MSE=121]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 24 Avg MSE: 123.4200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 25/25]: 100%|| 4/4 [00:01<00:00,  2.20it/s, MSE=220]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 25 Avg MSE: 122.6746\n",
            "[INFO] Partial fine-tuning complete.\n",
            "[INFO] Saved fine-tuned model to saved_models/resnet50_fold3.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:02<00:00,  1.84it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.75it/s]\n",
            "[I 2025-10-07 11:33:27,516] A new study created in memory with name: no-name-ddff18c1-e34c-43f0-9dba-2505aa4c6ab1\n",
            "[I 2025-10-07 11:33:27,995] Trial 0 finished with value: 1.769784927368164 and parameters: {'learning_rate': 0.1408812434367209, 'max_depth': 6, 'n_estimators': 171, 'subsample': 0.857835112207183, 'colsample_bytree': 0.7475553848125814}. Best is trial 0 with value: 1.769784927368164.\n",
            "[I 2025-10-07 11:33:28,858] Trial 1 finished with value: 1.8850960731506348 and parameters: {'learning_rate': 0.0010290059296897624, 'max_depth': 8, 'n_estimators': 503, 'subsample': 0.5318409053393016, 'colsample_bytree': 0.826394804595616}. Best is trial 0 with value: 1.769784927368164.\n",
            "[I 2025-10-07 11:33:29,924] Trial 2 finished with value: 1.9451647996902466 and parameters: {'learning_rate': 0.0015566915870451447, 'max_depth': 4, 'n_estimators': 563, 'subsample': 0.9710290591491757, 'colsample_bytree': 0.6753912769017685}. Best is trial 0 with value: 1.769784927368164.\n",
            "[I 2025-10-07 11:33:31,230] Trial 3 finished with value: 1.904058814048767 and parameters: {'learning_rate': 0.06719688714827793, 'max_depth': 8, 'n_estimators': 912, 'subsample': 0.7253816517078804, 'colsample_bytree': 0.8038067921700869}. Best is trial 0 with value: 1.769784927368164.\n",
            "[I 2025-10-07 11:33:31,549] Trial 4 finished with value: 1.7046147584915161 and parameters: {'learning_rate': 0.010938069326693212, 'max_depth': 3, 'n_estimators': 210, 'subsample': 0.8633749634427201, 'colsample_bytree': 0.7683527252403979}. Best is trial 4 with value: 1.7046147584915161.\n",
            "[I 2025-10-07 11:33:32,660] Trial 5 finished with value: 1.824275016784668 and parameters: {'learning_rate': 0.04761235630753653, 'max_depth': 10, 'n_estimators': 466, 'subsample': 0.6174046863640135, 'colsample_bytree': 0.8796327967903534}. Best is trial 4 with value: 1.7046147584915161.\n",
            "[I 2025-10-07 11:33:33,172] Trial 6 finished with value: 1.9819273948669434 and parameters: {'learning_rate': 0.0025858741867965007, 'max_depth': 6, 'n_estimators': 248, 'subsample': 0.5732928255977028, 'colsample_bytree': 0.6420545556314741}. Best is trial 4 with value: 1.7046147584915161.\n",
            "[I 2025-10-07 11:33:35,404] Trial 7 finished with value: 1.985113263130188 and parameters: {'learning_rate': 0.0023712229788705676, 'max_depth': 9, 'n_estimators': 990, 'subsample': 0.6392312559616324, 'colsample_bytree': 0.7951832863676097}. Best is trial 4 with value: 1.7046147584915161.\n",
            "[I 2025-10-07 11:33:37,086] Trial 8 finished with value: 1.8223623037338257 and parameters: {'learning_rate': 0.0014839597648339815, 'max_depth': 8, 'n_estimators': 866, 'subsample': 0.978068666662518, 'colsample_bytree': 0.5179850981626865}. Best is trial 4 with value: 1.7046147584915161.\n",
            "[I 2025-10-07 11:33:38,422] Trial 9 finished with value: 1.8497236967086792 and parameters: {'learning_rate': 0.02357631359787551, 'max_depth': 3, 'n_estimators': 839, 'subsample': 0.5502819574302902, 'colsample_bytree': 0.9893685806245034}. Best is trial 4 with value: 1.7046147584915161.\n",
            "[I 2025-10-07 11:33:39,101] Trial 10 finished with value: 1.91561758518219 and parameters: {'learning_rate': 0.008986478791118331, 'max_depth': 4, 'n_estimators': 326, 'subsample': 0.8442239537757192, 'colsample_bytree': 0.941441006784048}. Best is trial 4 with value: 1.7046147584915161.\n",
            "[I 2025-10-07 11:33:39,300] Trial 11 finished with value: 1.7880406379699707 and parameters: {'learning_rate': 0.28294065037841604, 'max_depth': 6, 'n_estimators': 102, 'subsample': 0.8545339688132478, 'colsample_bytree': 0.6856623252509086}. Best is trial 4 with value: 1.7046147584915161.\n",
            "[I 2025-10-07 11:33:39,535] Trial 12 finished with value: 1.8308669328689575 and parameters: {'learning_rate': 0.008207408495426234, 'max_depth': 5, 'n_estimators': 106, 'subsample': 0.8625272848990692, 'colsample_bytree': 0.7247175079490074}. Best is trial 4 with value: 1.7046147584915161.\n",
            "[I 2025-10-07 11:33:39,840] Trial 13 finished with value: 2.23325514793396 and parameters: {'learning_rate': 0.21916641416838428, 'max_depth': 3, 'n_estimators': 282, 'subsample': 0.7725162948001577, 'colsample_bytree': 0.5441914735754763}. Best is trial 4 with value: 1.7046147584915161.\n",
            "[I 2025-10-07 11:33:40,615] Trial 14 finished with value: 1.563092827796936 and parameters: {'learning_rate': 0.09062022525704612, 'max_depth': 5, 'n_estimators': 622, 'subsample': 0.8941152972229804, 'colsample_bytree': 0.6059986817135103}. Best is trial 14 with value: 1.563092827796936.\n",
            "[I 2025-10-07 11:33:41,835] Trial 15 finished with value: 1.578455924987793 and parameters: {'learning_rate': 0.014884401392056767, 'max_depth': 4, 'n_estimators': 668, 'subsample': 0.9271241125560359, 'colsample_bytree': 0.6222617540681539}. Best is trial 14 with value: 1.563092827796936.\n",
            "[I 2025-10-07 11:33:42,965] Trial 16 finished with value: 1.7553256750106812 and parameters: {'learning_rate': 0.03200558319862204, 'max_depth': 5, 'n_estimators': 691, 'subsample': 0.9322192953368245, 'colsample_bytree': 0.6004122969565306}. Best is trial 14 with value: 1.563092827796936.\n",
            "[I 2025-10-07 11:33:43,815] Trial 17 finished with value: 1.955057144165039 and parameters: {'learning_rate': 0.08569784513722496, 'max_depth': 5, 'n_estimators': 723, 'subsample': 0.9247403196913708, 'colsample_bytree': 0.5931477900638944}. Best is trial 14 with value: 1.563092827796936.\n",
            "[I 2025-10-07 11:33:44,906] Trial 18 finished with value: 1.8782695531845093 and parameters: {'learning_rate': 0.005779140160533414, 'max_depth': 4, 'n_estimators': 637, 'subsample': 0.7710100796468407, 'colsample_bytree': 0.5895299762570916}. Best is trial 14 with value: 1.563092827796936.\n",
            "[I 2025-10-07 11:33:45,900] Trial 19 finished with value: 1.7569465637207031 and parameters: {'learning_rate': 0.014642101022831477, 'max_depth': 7, 'n_estimators': 406, 'subsample': 0.7161068542318668, 'colsample_bytree': 0.6371003083365329}. Best is trial 14 with value: 1.563092827796936.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.09062022525704612, 'max_depth': 5, 'n_estimators': 622, 'subsample': 0.8941152972229804, 'colsample_bytree': 0.6059986817135103}\n",
            "[Fold 3 | Model: resnet50] MAE: 1.3307, MSE: 4.7515, R: 0.3244\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n",
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/25]: 100%|| 4/4 [00:02<00:00,  1.90it/s, MSE=116]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 145.7534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/25]: 100%|| 4/4 [00:02<00:00,  2.00it/s, MSE=114]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 144.9649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/25]: 100%|| 4/4 [00:02<00:00,  1.96it/s, MSE=114]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 143.7485\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 4/25]: 100%|| 4/4 [00:02<00:00,  1.87it/s, MSE=113]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 4 Avg MSE: 143.0216\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 5/25]: 100%|| 4/4 [00:03<00:00,  1.07it/s, MSE=112]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 5 Avg MSE: 141.8617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 6/25]: 100%|| 4/4 [00:03<00:00,  1.25it/s, MSE=75.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 6 Avg MSE: 140.6320\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 7/25]: 100%|| 4/4 [00:01<00:00,  2.04it/s, MSE=292]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 7 Avg MSE: 139.6734\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 8/25]: 100%|| 4/4 [00:02<00:00,  1.98it/s, MSE=291]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 8 Avg MSE: 138.9948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 9/25]: 100%|| 4/4 [00:01<00:00,  2.01it/s, MSE=43.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 9 Avg MSE: 137.7851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 10/25]: 100%|| 4/4 [00:01<00:00,  2.02it/s, MSE=44]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 10 Avg MSE: 136.6670\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 11/25]: 100%|| 4/4 [00:02<00:00,  1.85it/s, MSE=106]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 11 Avg MSE: 136.0041\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 12/25]: 100%|| 4/4 [00:03<00:00,  1.27it/s, MSE=41.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 12 Avg MSE: 134.8642\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 13/25]: 100%|| 4/4 [00:02<00:00,  1.92it/s, MSE=105]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 13 Avg MSE: 133.8064\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 14/25]: 100%|| 4/4 [00:02<00:00,  1.86it/s, MSE=172]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 14 Avg MSE: 132.7265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 15/25]: 100%|| 4/4 [00:01<00:00,  2.01it/s, MSE=103]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 15 Avg MSE: 131.8445\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 16/25]: 100%|| 4/4 [00:01<00:00,  2.03it/s, MSE=69.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 16 Avg MSE: 130.8102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 17/25]: 100%|| 4/4 [00:02<00:00,  1.94it/s, MSE=103]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 17 Avg MSE: 130.1026\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 18/25]: 100%|| 4/4 [00:03<00:00,  1.23it/s, MSE=124]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 18 Avg MSE: 129.0848\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 19/25]: 100%|| 4/4 [00:02<00:00,  1.73it/s, MSE=100]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 19 Avg MSE: 128.2815\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 20/25]: 100%|| 4/4 [00:01<00:00,  2.02it/s, MSE=115]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 20 Avg MSE: 127.6609\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 21/25]: 100%|| 4/4 [00:01<00:00,  2.01it/s, MSE=10.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 21 Avg MSE: 125.8658\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 22/25]: 100%|| 4/4 [00:01<00:00,  2.00it/s, MSE=99.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 22 Avg MSE: 125.7234\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 23/25]: 100%|| 4/4 [00:02<00:00,  1.96it/s, MSE=63.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 23 Avg MSE: 123.9079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 24/25]: 100%|| 4/4 [00:03<00:00,  1.32it/s, MSE=111]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 24 Avg MSE: 123.7306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 25/25]: 100%|| 4/4 [00:02<00:00,  1.58it/s, MSE=117]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 25 Avg MSE: 122.9762\n",
            "[INFO] Partial fine-tuning complete.\n",
            "[INFO] Saved fine-tuned model to saved_models/resnet50_fold4.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:02<00:00,  1.68it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  2.26it/s]\n",
            "[I 2025-10-07 11:34:50,025] A new study created in memory with name: no-name-da774a1a-6839-4838-8fd6-ee175cdead85\n",
            "[I 2025-10-07 11:34:51,710] Trial 0 finished with value: 2.6910908222198486 and parameters: {'learning_rate': 0.024440560828165946, 'max_depth': 5, 'n_estimators': 747, 'subsample': 0.8590708675240623, 'colsample_bytree': 0.8137621145281999}. Best is trial 0 with value: 2.6910908222198486.\n",
            "[I 2025-10-07 11:34:52,467] Trial 1 finished with value: 3.3393356800079346 and parameters: {'learning_rate': 0.12127504398092667, 'max_depth': 3, 'n_estimators': 765, 'subsample': 0.9148511291557254, 'colsample_bytree': 0.8353011707083404}. Best is trial 0 with value: 2.6910908222198486.\n",
            "[I 2025-10-07 11:34:53,856] Trial 2 finished with value: 2.8034565448760986 and parameters: {'learning_rate': 0.041585059411482383, 'max_depth': 6, 'n_estimators': 900, 'subsample': 0.8924795022701371, 'colsample_bytree': 0.8117679009146994}. Best is trial 0 with value: 2.6910908222198486.\n",
            "[I 2025-10-07 11:34:54,499] Trial 3 finished with value: 2.4921715259552 and parameters: {'learning_rate': 0.18963962614000382, 'max_depth': 10, 'n_estimators': 548, 'subsample': 0.856569976613111, 'colsample_bytree': 0.7438084763514816}. Best is trial 3 with value: 2.4921715259552.\n",
            "[I 2025-10-07 11:34:55,803] Trial 4 finished with value: 2.4837327003479004 and parameters: {'learning_rate': 0.021430634304511115, 'max_depth': 5, 'n_estimators': 577, 'subsample': 0.8743999199773398, 'colsample_bytree': 0.7965612817970014}. Best is trial 4 with value: 2.4837327003479004.\n",
            "[I 2025-10-07 11:34:56,517] Trial 5 finished with value: 2.791548490524292 and parameters: {'learning_rate': 0.09769210152175503, 'max_depth': 7, 'n_estimators': 366, 'subsample': 0.8075409878728729, 'colsample_bytree': 0.7745364997338227}. Best is trial 4 with value: 2.4837327003479004.\n",
            "[I 2025-10-07 11:34:57,717] Trial 6 finished with value: 2.254526376724243 and parameters: {'learning_rate': 0.13908376876931777, 'max_depth': 7, 'n_estimators': 884, 'subsample': 0.7513477014295129, 'colsample_bytree': 0.6489233610516738}. Best is trial 6 with value: 2.254526376724243.\n",
            "[I 2025-10-07 11:34:59,197] Trial 7 finished with value: 2.0466411113739014 and parameters: {'learning_rate': 0.0010281625294590343, 'max_depth': 5, 'n_estimators': 798, 'subsample': 0.5695797165477064, 'colsample_bytree': 0.5201571600793895}. Best is trial 7 with value: 2.0466411113739014.\n",
            "[I 2025-10-07 11:34:59,455] Trial 8 finished with value: 1.822644829750061 and parameters: {'learning_rate': 0.010888660242347532, 'max_depth': 4, 'n_estimators': 136, 'subsample': 0.5313146850819168, 'colsample_bytree': 0.9467926941587284}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:00,176] Trial 9 finished with value: 2.800337553024292 and parameters: {'learning_rate': 0.10104954479068085, 'max_depth': 4, 'n_estimators': 702, 'subsample': 0.8359439896285481, 'colsample_bytree': 0.539447965736312}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:00,398] Trial 10 finished with value: 1.9783233404159546 and parameters: {'learning_rate': 0.004539650771780057, 'max_depth': 9, 'n_estimators': 108, 'subsample': 0.5096366866975792, 'colsample_bytree': 0.9957647017620054}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:00,621] Trial 11 finished with value: 1.9829984903335571 and parameters: {'learning_rate': 0.004529258633001258, 'max_depth': 9, 'n_estimators': 109, 'subsample': 0.5005706340872997, 'colsample_bytree': 0.99692024812586}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:00,853] Trial 12 finished with value: 2.022965669631958 and parameters: {'learning_rate': 0.005615681186694002, 'max_depth': 8, 'n_estimators': 101, 'subsample': 0.6340275251979675, 'colsample_bytree': 0.9960488633075028}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:01,437] Trial 13 finished with value: 2.0941317081451416 and parameters: {'learning_rate': 0.00596958969204522, 'max_depth': 10, 'n_estimators': 281, 'subsample': 0.6434318012531254, 'colsample_bytree': 0.9016197312557119}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:01,883] Trial 14 finished with value: 2.0554935932159424 and parameters: {'learning_rate': 0.002092181125519972, 'max_depth': 3, 'n_estimators': 278, 'subsample': 0.5055381608996773, 'colsample_bytree': 0.911606109551949}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:02,427] Trial 15 finished with value: 2.736623764038086 and parameters: {'learning_rate': 0.009311740055452093, 'max_depth': 8, 'n_estimators': 208, 'subsample': 0.9912417958461079, 'colsample_bytree': 0.9209977741839209}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:03,255] Trial 16 finished with value: 2.067613363265991 and parameters: {'learning_rate': 0.012174182309019538, 'max_depth': 9, 'n_estimators': 416, 'subsample': 0.6338336364420978, 'colsample_bytree': 0.7000444314280518}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:03,660] Trial 17 finished with value: 1.9272290468215942 and parameters: {'learning_rate': 0.002772239690474904, 'max_depth': 6, 'n_estimators': 202, 'subsample': 0.5741435873622186, 'colsample_bytree': 0.94627933781895}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:04,491] Trial 18 finished with value: 2.042396068572998 and parameters: {'learning_rate': 0.002059806575877801, 'max_depth': 4, 'n_estimators': 456, 'subsample': 0.695214961570877, 'colsample_bytree': 0.8631812752062727}. Best is trial 8 with value: 1.822644829750061.\n",
            "[I 2025-10-07 11:35:04,940] Trial 19 finished with value: 1.9103584289550781 and parameters: {'learning_rate': 0.002397673783545202, 'max_depth': 6, 'n_estimators': 228, 'subsample': 0.5597564734993996, 'colsample_bytree': 0.942222735129857}. Best is trial 8 with value: 1.822644829750061.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.010888660242347532, 'max_depth': 4, 'n_estimators': 136, 'subsample': 0.5313146850819168, 'colsample_bytree': 0.9467926941587284}\n",
            "[Fold 4 | Model: resnet50] MAE: 2.1078, MSE: 7.8304, R: 0.0091\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n",
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/25]: 100%|| 4/4 [00:01<00:00,  2.44it/s, MSE=49]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 152.0789\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/25]: 100%|| 4/4 [00:01<00:00,  2.16it/s, MSE=113]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 150.8209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/25]: 100%|| 4/4 [00:02<00:00,  1.51it/s, MSE=141]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 150.0072\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 4/25]: 100%|| 4/4 [00:01<00:00,  2.22it/s, MSE=183]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 4 Avg MSE: 149.0843\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 5/25]: 100%|| 4/4 [00:01<00:00,  2.35it/s, MSE=137]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 5 Avg MSE: 147.9632\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 6/25]: 100%|| 4/4 [00:01<00:00,  2.36it/s, MSE=110]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 6 Avg MSE: 147.1152\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 7/25]: 100%|| 4/4 [00:01<00:00,  2.38it/s, MSE=109]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 7 Avg MSE: 146.0937\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 8/25]: 100%|| 4/4 [00:01<00:00,  2.31it/s, MSE=108]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 8 Avg MSE: 144.8625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 9/25]: 100%|| 4/4 [00:01<00:00,  2.46it/s, MSE=106]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 9 Avg MSE: 143.7800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 10/25]: 100%|| 4/4 [00:02<00:00,  1.64it/s, MSE=107]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 10 Avg MSE: 142.8439\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 11/25]: 100%|| 4/4 [00:02<00:00,  1.67it/s, MSE=42.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 11 Avg MSE: 141.8308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 12/25]: 100%|| 4/4 [00:01<00:00,  2.45it/s, MSE=131]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 12 Avg MSE: 140.9527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 13/25]: 100%|| 4/4 [00:01<00:00,  2.40it/s, MSE=282]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 13 Avg MSE: 140.0153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 14/25]: 100%|| 4/4 [00:01<00:00,  2.49it/s, MSE=102]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 14 Avg MSE: 138.8719\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 15/25]: 100%|| 4/4 [00:01<00:00,  2.40it/s, MSE=118]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 15 Avg MSE: 137.7853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 16/25]: 100%|| 4/4 [00:01<00:00,  2.40it/s, MSE=167]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 16 Avg MSE: 136.9168\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 17/25]: 100%|| 4/4 [00:01<00:00,  2.30it/s, MSE=101]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 17 Avg MSE: 135.6434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 18/25]: 100%|| 4/4 [00:02<00:00,  1.49it/s, MSE=115]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 18 Avg MSE: 134.6027\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 19/25]: 100%|| 4/4 [00:01<00:00,  2.09it/s, MSE=114]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 19 Avg MSE: 134.3758\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 20/25]: 100%|| 4/4 [00:01<00:00,  2.47it/s, MSE=66]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 20 Avg MSE: 133.9016\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 21/25]: 100%|| 4/4 [00:01<00:00,  2.45it/s, MSE=162]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 21 Avg MSE: 131.7944\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 22/25]: 100%|| 4/4 [00:01<00:00,  2.44it/s, MSE=95]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 22 Avg MSE: 131.1345\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 23/25]: 100%|| 4/4 [00:01<00:00,  2.32it/s, MSE=110]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 23 Avg MSE: 130.5603\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 24/25]: 100%|| 4/4 [00:01<00:00,  2.40it/s, MSE=63.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 24 Avg MSE: 129.4284\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 25/25]: 100%|| 4/4 [00:02<00:00,  1.56it/s, MSE=118]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 25 Avg MSE: 128.0171\n",
            "[INFO] Partial fine-tuning complete.\n",
            "[INFO] Saved fine-tuned model to saved_models/resnet50_fold5.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:02<00:00,  1.71it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.21it/s]\n",
            "[I 2025-10-07 11:35:56,425] A new study created in memory with name: no-name-7c093a21-db9d-4e35-9405-0c11e8a0bdce\n",
            "[I 2025-10-07 11:35:57,310] Trial 0 finished with value: 4.117879867553711 and parameters: {'learning_rate': 0.0030243151012140576, 'max_depth': 3, 'n_estimators': 510, 'subsample': 0.8255938879672129, 'colsample_bytree': 0.9134576913287256}. Best is trial 0 with value: 4.117879867553711.\n",
            "[I 2025-10-07 11:35:58,141] Trial 1 finished with value: 3.7299187183380127 and parameters: {'learning_rate': 0.00123880295606621, 'max_depth': 5, 'n_estimators': 398, 'subsample': 0.6867424518808924, 'colsample_bytree': 0.8185695449639436}. Best is trial 1 with value: 3.7299187183380127.\n",
            "[I 2025-10-07 11:35:59,511] Trial 2 finished with value: 4.141952991485596 and parameters: {'learning_rate': 0.015305649447025135, 'max_depth': 8, 'n_estimators': 686, 'subsample': 0.6117024723020394, 'colsample_bytree': 0.6225110198797046}. Best is trial 1 with value: 3.7299187183380127.\n",
            "[I 2025-10-07 11:36:00,528] Trial 3 finished with value: 4.4560980796813965 and parameters: {'learning_rate': 0.0014375129276978603, 'max_depth': 3, 'n_estimators': 766, 'subsample': 0.9500027669465387, 'colsample_bytree': 0.6116817846365133}. Best is trial 1 with value: 3.7299187183380127.\n",
            "[I 2025-10-07 11:36:02,446] Trial 4 finished with value: 3.818483352661133 and parameters: {'learning_rate': 0.032419022538849915, 'max_depth': 7, 'n_estimators': 977, 'subsample': 0.5720315612777414, 'colsample_bytree': 0.9582472449359123}. Best is trial 1 with value: 3.7299187183380127.\n",
            "[I 2025-10-07 11:36:02,889] Trial 5 finished with value: 4.927729606628418 and parameters: {'learning_rate': 0.11427248949278014, 'max_depth': 4, 'n_estimators': 303, 'subsample': 0.8827946040645644, 'colsample_bytree': 0.9741054789477922}. Best is trial 1 with value: 3.7299187183380127.\n",
            "[I 2025-10-07 11:36:03,537] Trial 6 finished with value: 5.314987659454346 and parameters: {'learning_rate': 0.11897436657345713, 'max_depth': 3, 'n_estimators': 609, 'subsample': 0.9786824331625363, 'colsample_bytree': 0.9816949450199467}. Best is trial 1 with value: 3.7299187183380127.\n",
            "[I 2025-10-07 11:36:04,519] Trial 7 finished with value: 3.8742523193359375 and parameters: {'learning_rate': 0.003606650080480225, 'max_depth': 9, 'n_estimators': 451, 'subsample': 0.7870420423335691, 'colsample_bytree': 0.6694983943221902}. Best is trial 1 with value: 3.7299187183380127.\n",
            "[I 2025-10-07 11:36:04,882] Trial 8 finished with value: 3.6633918285369873 and parameters: {'learning_rate': 0.004180911523900892, 'max_depth': 9, 'n_estimators': 179, 'subsample': 0.57676599800687, 'colsample_bytree': 0.9021561749434746}. Best is trial 8 with value: 3.6633918285369873.\n",
            "[I 2025-10-07 11:36:05,249] Trial 9 finished with value: 4.137620449066162 and parameters: {'learning_rate': 0.0668367336885812, 'max_depth': 7, 'n_estimators': 133, 'subsample': 0.577813146118273, 'colsample_bytree': 0.9059462994622299}. Best is trial 8 with value: 3.6633918285369873.\n",
            "[I 2025-10-07 11:36:05,585] Trial 10 finished with value: 3.765172243118286 and parameters: {'learning_rate': 0.010433036013343147, 'max_depth': 10, 'n_estimators': 138, 'subsample': 0.5123115328723744, 'colsample_bytree': 0.7815160556238488}. Best is trial 8 with value: 3.6633918285369873.\n",
            "[I 2025-10-07 11:36:06,356] Trial 11 finished with value: 3.6394054889678955 and parameters: {'learning_rate': 0.0011323616754110684, 'max_depth': 5, 'n_estimators': 310, 'subsample': 0.7010391076379998, 'colsample_bytree': 0.7994782947615785}. Best is trial 11 with value: 3.6394054889678955.\n",
            "[I 2025-10-07 11:36:07,027] Trial 12 finished with value: 3.9750301837921143 and parameters: {'learning_rate': 0.0053677729719351875, 'max_depth': 5, 'n_estimators': 280, 'subsample': 0.7032022340651556, 'colsample_bytree': 0.8266631061569059}. Best is trial 11 with value: 3.6394054889678955.\n",
            "[I 2025-10-07 11:36:07,665] Trial 13 finished with value: 3.6164801120758057 and parameters: {'learning_rate': 0.0010673510253361189, 'max_depth': 6, 'n_estimators': 257, 'subsample': 0.661497637430134, 'colsample_bytree': 0.7258597319692279}. Best is trial 13 with value: 3.6164801120758057.\n",
            "[I 2025-10-07 11:36:08,315] Trial 14 finished with value: 3.592925786972046 and parameters: {'learning_rate': 0.001003241945525446, 'max_depth': 6, 'n_estimators': 299, 'subsample': 0.6947448033104751, 'colsample_bytree': 0.5264612127261321}. Best is trial 14 with value: 3.592925786972046.\n",
            "[I 2025-10-07 11:36:09,017] Trial 15 finished with value: 3.7440593242645264 and parameters: {'learning_rate': 0.0022204915055381545, 'max_depth': 6, 'n_estimators': 379, 'subsample': 0.6581819488063371, 'colsample_bytree': 0.5408117269425811}. Best is trial 14 with value: 3.592925786972046.\n",
            "[I 2025-10-07 11:36:09,465] Trial 16 finished with value: 3.9210097789764404 and parameters: {'learning_rate': 0.009023647603647858, 'max_depth': 6, 'n_estimators': 225, 'subsample': 0.7623793117337957, 'colsample_bytree': 0.507697002537769}. Best is trial 14 with value: 3.592925786972046.\n",
            "[I 2025-10-07 11:36:10,065] Trial 17 finished with value: 3.7816274166107178 and parameters: {'learning_rate': 0.2746854959061411, 'max_depth': 8, 'n_estimators': 534, 'subsample': 0.6398859481398899, 'colsample_bytree': 0.7010000764260643}. Best is trial 14 with value: 3.592925786972046.\n",
            "[I 2025-10-07 11:36:10,298] Trial 18 finished with value: 3.6861677169799805 and parameters: {'learning_rate': 0.0021216679184563234, 'max_depth': 6, 'n_estimators': 105, 'subsample': 0.81774734107677, 'colsample_bytree': 0.5858924835002478}. Best is trial 14 with value: 3.592925786972046.\n",
            "[I 2025-10-07 11:36:10,971] Trial 19 finished with value: 3.943922996520996 and parameters: {'learning_rate': 0.02971516065947955, 'max_depth': 4, 'n_estimators': 380, 'subsample': 0.5013493197107837, 'colsample_bytree': 0.7196479584049861}. Best is trial 14 with value: 3.592925786972046.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.001003241945525446, 'max_depth': 6, 'n_estimators': 299, 'subsample': 0.6947448033104751, 'colsample_bytree': 0.5264612127261321}\n",
            "[Fold 5 | Model: resnet50] MAE: 3.6527, MSE: 20.0974, R: -0.2939\n",
            "\n",
            "===== Final Cross-Validation Results =====\n",
            "   fold     Model       MAE        MSE        R2\n",
            "0     1  resnet50  2.707836  11.572522 -0.463818\n",
            "1     2  resnet50  2.084199   7.182236 -0.731471\n",
            "2     3  resnet50  1.330685   4.751451  0.324358\n",
            "3     4  resnet50  2.107787   7.830439  0.009084\n",
            "4     5  resnet50  3.652715  20.097351 -0.293890\n",
            "\n",
            "Mean MAE: 2.3766442060470583\n",
            "Mean R: -0.23114733695983886\n"
          ]
        }
      ],
      "source": [
        "results = run_cnn_xgb_kfold_finetune(df, n_splits=5, batch_size=8, xgb_trials=20, fine_tune_epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzdt6yt2X4YW"
      },
      "source": [
        "### Baseline Xception Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOTnqZqAX40L"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# cnn_xgb_finetune_pipeline_timm.py\n",
        "# ----------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "\n",
        "# ============================================================\n",
        "# 1 Dataset class\n",
        "# ============================================================\n",
        "class ImageRegressionDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[\"Filename\"]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = torch.tensor(row[\"Hb\"], dtype=torch.float32)\n",
        "        return img, label\n",
        "\n",
        "# ============================================================\n",
        "# 2 Partial Fine-Tuning\n",
        "# ============================================================\n",
        "def fine_tune_backbone_partial(backbone, train_df, transform, device,\n",
        "                               epochs=10, batch_size=8, lr=1e-4):\n",
        "    print(f\"[INFO] Starting partial fine-tuning on {len(train_df)} labeled samples...\")\n",
        "\n",
        "    # Freeze all layers except the last stage\n",
        "    for name, param in backbone.named_parameters():\n",
        "        param.requires_grad = False\n",
        "    # Replace final classification head with regression head\n",
        "    if hasattr(backbone, \"fc\"):\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Linear(in_features, 1)\n",
        "    elif hasattr(backbone, \"classifier\"):  # timm models often use 'classifier'\n",
        "        in_features = backbone.classifier.in_features\n",
        "        backbone.classifier = nn.Linear(in_features, 1)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown backbone classifier head.\")\n",
        "\n",
        "    dataset = ImageRegressionDataset(train_df, transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, backbone.parameters()), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    backbone.to(device)\n",
        "    backbone.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(loader, desc=f\"[Fine-Tune Epoch {epoch+1}/{epochs}]\")\n",
        "        for imgs, targets in pbar:\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = backbone(imgs).view(-1)\n",
        "            loss = criterion(preds, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            pbar.set_postfix({\"MSE\": loss.item()})\n",
        "        avg_loss = total_loss / len(dataset)\n",
        "        print(f\"[INFO] Fine-tune Epoch {epoch+1} Avg MSE: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"[INFO] Partial fine-tuning complete.\")\n",
        "    return backbone\n",
        "\n",
        "# ============================================================\n",
        "# 3 Embedding Extraction\n",
        "# ============================================================\n",
        "def extract_embeddings(backbone, df, transform, device, batch_size=8):\n",
        "    backbone.eval()\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    # Remove final FC layer for features\n",
        "    feature_extractor = nn.Sequential(*list(backbone.children())[:-1]).to(device)\n",
        "\n",
        "    loader = DataLoader(ImageRegressionDataset(df, transform), batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in tqdm(loader, desc=\"Extracting embeddings\"):\n",
        "            imgs = imgs.to(device)\n",
        "            feats = feature_extractor(imgs)\n",
        "            pooled = feats.view(feats.size(0), -1)\n",
        "            embeddings.append(pooled.cpu().numpy())\n",
        "            labels.append(targets.numpy())\n",
        "\n",
        "    embeddings = np.concatenate(embeddings)\n",
        "    labels = np.concatenate(labels)\n",
        "    return embeddings, labels\n",
        "\n",
        "# ============================================================\n",
        "# 4 XGBoost + Optuna\n",
        "# ============================================================\n",
        "def tune_xgb_with_optuna(X_train, y_train, X_val, y_val, n_trials=20):\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"objective\": \"reg:squarederror\",\n",
        "            \"eval_metric\": \"mae\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"device\": \"cuda\",\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        }\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "        preds = model.predict(X_val)\n",
        "        return mean_absolute_error(y_val, preds)\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "    print(\"[Optuna] Best trial:\", study.best_trial.params)\n",
        "    return study.best_trial.params\n",
        "\n",
        "# ============================================================\n",
        "# 5 Full CNN + XGBoost K-Fold Training Pipeline\n",
        "# ============================================================\n",
        "def run_cnn_xgb_kfold_finetune(labeled_df, backbone_name=\"resnet50\",\n",
        "                               train_transform=None, val_transform=None,\n",
        "                               n_splits=5, batch_size=8, fine_tune_epochs=10, xgb_trials=20,\n",
        "                               save_dir=\"saved_models\"):\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Default transforms\n",
        "    if train_transform is None:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((299, 299)),  # Xception requires 299x299\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                 [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    if val_transform is None:\n",
        "        val_transform = transforms.Compose([\n",
        "            transforms.Resize((299, 299)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                 [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    metrics = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(labeled_df), 1):\n",
        "        print(f\"\\n--- Fold {fold}/{n_splits} ---\")\n",
        "        train_df, val_df = labeled_df.iloc[train_idx], labeled_df.iloc[val_idx]\n",
        "        print(f\"[INFO] Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
        "\n",
        "        # Load pretrained backbone from timm\n",
        "        backbone = timm.create_model(backbone_name, pretrained=True)\n",
        "        backbone = fine_tune_backbone_partial(backbone, train_df, train_transform, device,\n",
        "                                              epochs=fine_tune_epochs, batch_size=batch_size)\n",
        "\n",
        "        # Save fine-tuned CNN\n",
        "        model_path = os.path.join(save_dir, f\"{backbone_name}_fold{fold}.pth\")\n",
        "        torch.save(backbone.state_dict(), model_path)\n",
        "        print(f\"[INFO] Saved fine-tuned model to {model_path}\")\n",
        "\n",
        "        # Extract embeddings\n",
        "        X_train, y_train = extract_embeddings(backbone, train_df, train_transform, device, batch_size)\n",
        "        X_val, y_val = extract_embeddings(backbone, val_df, val_transform, device, batch_size)\n",
        "\n",
        "        # Tune and train XGBoost\n",
        "        best_params = tune_xgb_with_optuna(X_train, y_train, X_val, y_val, n_trials=xgb_trials)\n",
        "        xgb_model = xgb.XGBRegressor(**best_params)\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        preds = xgb_model.predict(X_val)\n",
        "        mae = mean_absolute_error(y_val, preds)\n",
        "        mse = mean_squared_error(y_val, preds)\n",
        "        r2 = r2_score(y_val, preds)\n",
        "\n",
        "        print(f\"[Fold {fold} | Model: {backbone_name}] MAE: {mae:.4f}, MSE: {mse:.4f}, R: {r2:.4f}\")\n",
        "        metrics.append({\"fold\": fold, \"Model\": backbone_name, \"MAE\": mae, \"MSE\": mse, \"R2\": r2})\n",
        "\n",
        "    results_df = pd.DataFrame(metrics)\n",
        "    print(\"\\n===== Final Cross-Validation Results =====\")\n",
        "    print(results_df)\n",
        "    print(\"\\nMean MAE:\", results_df[\"MAE\"].mean())\n",
        "    print(\"Mean R:\", results_df[\"R2\"].mean())\n",
        "    return results_df\n",
        "\n",
        "# ============================================================\n",
        "#  Example Usage\n",
        "# ============================================================\n",
        "# df = pd.DataFrame({\"Filename\": [...], \"Hb\": [...]})\n",
        "# results = run_cnn_xgb_kfold_finetune(df, backbone_name=\"xception\", n_splits=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuzWFvnaYC-Y"
      },
      "source": [
        "#### Run Xception Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_cZdUrSX65m",
        "outputId": "abef4c99-eadc-4eb3-c51f-a268aa8e0bf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 1/5 ---\n",
            "[INFO] Training samples: 24, Validation samples: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
            "  model = create_fn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Starting partial fine-tuning on 24 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/25]: 100%|| 3/3 [00:02<00:00,  1.15it/s, MSE=106]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 129.3709\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/25]: 100%|| 3/3 [00:01<00:00,  1.69it/s, MSE=103]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 127.1344\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/25]: 100%|| 3/3 [00:01<00:00,  1.72it/s, MSE=128]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 124.7919\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 4/25]: 100%|| 3/3 [00:01<00:00,  1.71it/s, MSE=159]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 4 Avg MSE: 122.6717\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 5/25]: 100%|| 3/3 [00:02<00:00,  1.19it/s, MSE=128]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 5 Avg MSE: 120.4003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 6/25]: 100%|| 3/3 [00:02<00:00,  1.14it/s, MSE=76.2]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 6 Avg MSE: 118.4971\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 7/25]: 100%|| 3/3 [00:01<00:00,  1.68it/s, MSE=115]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 7 Avg MSE: 116.5324\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 8/25]: 100%|| 3/3 [00:01<00:00,  1.73it/s, MSE=103]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 8 Avg MSE: 113.8931\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 9/25]: 100%|| 3/3 [00:01<00:00,  1.72it/s, MSE=112]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 9 Avg MSE: 112.6821\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 10/25]: 100%|| 3/3 [00:01<00:00,  1.71it/s, MSE=120]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 10 Avg MSE: 110.3127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 11/25]: 100%|| 3/3 [00:01<00:00,  1.68it/s, MSE=121]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 11 Avg MSE: 108.2825\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 12/25]: 100%|| 3/3 [00:02<00:00,  1.26it/s, MSE=109]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 12 Avg MSE: 106.5545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 13/25]: 100%|| 3/3 [00:02<00:00,  1.14it/s, MSE=111]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 13 Avg MSE: 105.0159\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 14/25]: 100%|| 3/3 [00:01<00:00,  1.65it/s, MSE=128]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 14 Avg MSE: 102.6842\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 15/25]: 100%|| 3/3 [00:01<00:00,  1.72it/s, MSE=84.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 15 Avg MSE: 100.9702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 16/25]: 100%|| 3/3 [00:01<00:00,  1.67it/s, MSE=107]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 16 Avg MSE: 98.8031\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 17/25]: 100%|| 3/3 [00:01<00:00,  1.69it/s, MSE=88.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 17 Avg MSE: 97.4014\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 18/25]: 100%|| 3/3 [00:01<00:00,  1.73it/s, MSE=74.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 18 Avg MSE: 95.4275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 19/25]: 100%|| 3/3 [00:02<00:00,  1.24it/s, MSE=83.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 19 Avg MSE: 93.5705\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 20/25]: 100%|| 3/3 [00:02<00:00,  1.14it/s, MSE=73.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 20 Avg MSE: 92.2379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 21/25]: 100%|| 3/3 [00:01<00:00,  1.66it/s, MSE=59.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 21 Avg MSE: 90.3462\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 22/25]: 100%|| 3/3 [00:01<00:00,  1.74it/s, MSE=92.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 22 Avg MSE: 89.8045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 23/25]: 100%|| 3/3 [00:01<00:00,  1.68it/s, MSE=118]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 23 Avg MSE: 87.1032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 24/25]: 100%|| 3/3 [00:01<00:00,  1.72it/s, MSE=111]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 24 Avg MSE: 84.5417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 25/25]: 100%|| 3/3 [00:01<00:00,  1.72it/s, MSE=68.8]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 25 Avg MSE: 83.2143\n",
            "[INFO] Partial fine-tuning complete.\n",
            "[INFO] Saved fine-tuned model to saved_models/xception_fold1.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 3/3 [00:02<00:00,  1.30it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.00it/s]\n",
            "[I 2025-10-07 11:22:29,138] A new study created in memory with name: no-name-31e89255-90cc-44c6-afae-6abf71c80e3f\n",
            "[I 2025-10-07 11:22:30,119] Trial 0 finished with value: 1.9423929452896118 and parameters: {'learning_rate': 0.26585569921746516, 'max_depth': 5, 'n_estimators': 586, 'subsample': 0.5727301694610369, 'colsample_bytree': 0.6601851815106143}. Best is trial 0 with value: 1.9423929452896118.\n",
            "[I 2025-10-07 11:22:31,349] Trial 1 finished with value: 2.01274037361145 and parameters: {'learning_rate': 0.008754592929394912, 'max_depth': 4, 'n_estimators': 621, 'subsample': 0.5759102968028385, 'colsample_bytree': 0.958668745352345}. Best is trial 0 with value: 1.9423929452896118.\n",
            "[I 2025-10-07 11:22:32,157] Trial 2 finished with value: 2.4566471576690674 and parameters: {'learning_rate': 0.0846666425442602, 'max_depth': 5, 'n_estimators': 623, 'subsample': 0.9027536309094647, 'colsample_bytree': 0.755905430993711}. Best is trial 0 with value: 1.9423929452896118.\n",
            "[I 2025-10-07 11:22:33,934] Trial 3 finished with value: 2.031798839569092 and parameters: {'learning_rate': 0.0033374637122919938, 'max_depth': 5, 'n_estimators': 758, 'subsample': 0.8489378975030066, 'colsample_bytree': 0.9848121440134618}. Best is trial 0 with value: 1.9423929452896118.\n",
            "[I 2025-10-07 11:22:34,150] Trial 4 finished with value: 1.5783594846725464 and parameters: {'learning_rate': 0.2847596393377112, 'max_depth': 6, 'n_estimators': 118, 'subsample': 0.8106096262471523, 'colsample_bytree': 0.8541615181079478}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:34,789] Trial 5 finished with value: 2.746187925338745 and parameters: {'learning_rate': 0.27056622771514843, 'max_depth': 9, 'n_estimators': 576, 'subsample': 0.8532049894506298, 'colsample_bytree': 0.9072573235298809}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:36,225] Trial 6 finished with value: 2.494922161102295 and parameters: {'learning_rate': 0.002687638484386604, 'max_depth': 7, 'n_estimators': 755, 'subsample': 0.7424196595083132, 'colsample_bytree': 0.5239094976445957}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:37,602] Trial 7 finished with value: 2.257190465927124 and parameters: {'learning_rate': 0.004394729070828311, 'max_depth': 3, 'n_estimators': 874, 'subsample': 0.653469893483982, 'colsample_bytree': 0.9514413091368981}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:38,599] Trial 8 finished with value: 2.1371724605560303 and parameters: {'learning_rate': 0.044161673761513205, 'max_depth': 10, 'n_estimators': 508, 'subsample': 0.7087083328053421, 'colsample_bytree': 0.5576799793742504}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:39,094] Trial 9 finished with value: 2.2894580364227295 and parameters: {'learning_rate': 0.11496829885538769, 'max_depth': 8, 'n_estimators': 296, 'subsample': 0.7924963547860167, 'colsample_bytree': 0.5613714124487494}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:39,390] Trial 10 finished with value: 2.439330577850342 and parameters: {'learning_rate': 0.030562989497262154, 'max_depth': 7, 'n_estimators': 109, 'subsample': 0.9986789361205022, 'colsample_bytree': 0.8287295637569758}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:39,828] Trial 11 finished with value: 2.3924458026885986 and parameters: {'learning_rate': 0.2951705313606542, 'max_depth': 6, 'n_estimators': 337, 'subsample': 0.5251077021343452, 'colsample_bytree': 0.661185771005073}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:40,510] Trial 12 finished with value: 2.018685817718506 and parameters: {'learning_rate': 0.12031389227266322, 'max_depth': 5, 'n_estimators': 395, 'subsample': 0.6569202936963148, 'colsample_bytree': 0.6781209730078389}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:40,781] Trial 13 finished with value: 2.7845263481140137 and parameters: {'learning_rate': 0.0010796941769681712, 'max_depth': 3, 'n_estimators': 145, 'subsample': 0.6064475119102568, 'colsample_bytree': 0.8283643368148673}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:43,020] Trial 14 finished with value: 2.122777223587036 and parameters: {'learning_rate': 0.019699572546239174, 'max_depth': 6, 'n_estimators': 986, 'subsample': 0.5176981933115119, 'colsample_bytree': 0.6667824095693631}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:43,613] Trial 15 finished with value: 2.867814302444458 and parameters: {'learning_rate': 0.15751208024136149, 'max_depth': 4, 'n_estimators': 466, 'subsample': 0.7920295577136828, 'colsample_bytree': 0.7629549198744611}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:44,284] Trial 16 finished with value: 2.3405251502990723 and parameters: {'learning_rate': 0.05656681643160743, 'max_depth': 8, 'n_estimators': 235, 'subsample': 0.9753321513336961, 'colsample_bytree': 0.8441753383890546}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:45,099] Trial 17 finished with value: 1.8322542905807495 and parameters: {'learning_rate': 0.17505985874421084, 'max_depth': 6, 'n_estimators': 751, 'subsample': 0.6880193637276711, 'colsample_bytree': 0.6026870379685242}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:46,209] Trial 18 finished with value: 1.7085607051849365 and parameters: {'learning_rate': 0.06366453618843489, 'max_depth': 8, 'n_estimators': 762, 'subsample': 0.6869098708405743, 'colsample_bytree': 0.6084797713774185}. Best is trial 4 with value: 1.5783594846725464.\n",
            "[I 2025-10-07 11:22:48,636] Trial 19 finished with value: 2.8399951457977295 and parameters: {'learning_rate': 0.014864592413117949, 'max_depth': 8, 'n_estimators': 902, 'subsample': 0.7782630076369775, 'colsample_bytree': 0.898320455241861}. Best is trial 4 with value: 1.5783594846725464.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.2847596393377112, 'max_depth': 6, 'n_estimators': 118, 'subsample': 0.8106096262471523, 'colsample_bytree': 0.8541615181079478}\n",
            "[Fold 1 | Model: xception] MAE: 2.0752, MSE: 4.5690, R: 0.4221\n",
            "\n",
            "--- Fold 2/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
            "  model = create_fn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/25]: 100%|| 4/4 [00:01<00:00,  2.36it/s, MSE=112]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 152.6645\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/25]: 100%|| 4/4 [00:01<00:00,  2.28it/s, MSE=292]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 149.3465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/25]: 100%|| 4/4 [00:02<00:00,  1.51it/s, MSE=107]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 146.0686\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 4/25]: 100%|| 4/4 [00:02<00:00,  1.71it/s, MSE=54.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 4 Avg MSE: 143.1269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 5/25]: 100%|| 4/4 [00:01<00:00,  2.30it/s, MSE=280]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 5 Avg MSE: 140.1743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 6/25]: 100%|| 4/4 [00:01<00:00,  2.34it/s, MSE=50.8]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 6 Avg MSE: 136.9542\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 7/25]: 100%|| 4/4 [00:01<00:00,  2.32it/s, MSE=133]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 7 Avg MSE: 134.3782\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 8/25]: 100%|| 4/4 [00:01<00:00,  2.33it/s, MSE=96.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 8 Avg MSE: 131.6004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 9/25]: 100%|| 4/4 [00:01<00:00,  2.32it/s, MSE=62.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 9 Avg MSE: 128.3365\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 10/25]: 100%|| 4/4 [00:02<00:00,  1.68it/s, MSE=7.36]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 10 Avg MSE: 125.8022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 11/25]: 100%|| 4/4 [00:02<00:00,  1.52it/s, MSE=84.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 11 Avg MSE: 123.3440\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 12/25]: 100%|| 4/4 [00:02<00:00,  1.36it/s, MSE=84.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 12 Avg MSE: 120.4062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 13/25]: 100%|| 4/4 [00:03<00:00,  1.25it/s, MSE=109]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 13 Avg MSE: 118.0290\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 14/25]: 100%|| 4/4 [00:02<00:00,  1.58it/s, MSE=54.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 14 Avg MSE: 115.6502\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 15/25]: 100%|| 4/4 [00:02<00:00,  1.84it/s, MSE=241]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 15 Avg MSE: 113.1379\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 16/25]: 100%|| 4/4 [00:03<00:00,  1.32it/s, MSE=193]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 16 Avg MSE: 110.5828\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 17/25]: 100%|| 4/4 [00:01<00:00,  2.30it/s, MSE=186]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 17 Avg MSE: 107.6525\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 18/25]: 100%|| 4/4 [00:01<00:00,  2.30it/s, MSE=69.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 18 Avg MSE: 105.4512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 19/25]: 100%|| 4/4 [00:01<00:00,  2.29it/s, MSE=70.8]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 19 Avg MSE: 103.0542\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 20/25]: 100%|| 4/4 [00:01<00:00,  2.13it/s, MSE=65.2]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 20 Avg MSE: 99.9097\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 21/25]: 100%|| 4/4 [00:01<00:00,  2.25it/s, MSE=112]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 21 Avg MSE: 99.6577\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 22/25]: 100%|| 4/4 [00:02<00:00,  1.84it/s, MSE=29.8]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 22 Avg MSE: 96.2531\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 23/25]: 100%|| 4/4 [00:02<00:00,  1.42it/s, MSE=40.8]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 23 Avg MSE: 94.6983\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 24/25]: 100%|| 4/4 [00:02<00:00,  1.44it/s, MSE=85.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 24 Avg MSE: 91.9950\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 25/25]: 100%|| 4/4 [00:02<00:00,  1.79it/s, MSE=213]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 25 Avg MSE: 91.3779\n",
            "[INFO] Partial fine-tuning complete.\n",
            "[INFO] Saved fine-tuned model to saved_models/xception_fold2.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:01<00:00,  2.30it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.46it/s]\n",
            "[I 2025-10-07 11:23:47,355] A new study created in memory with name: no-name-32c25a07-bd78-4fad-a33d-f5e50f79ad6b\n",
            "[I 2025-10-07 11:23:47,696] Trial 0 finished with value: 1.9923020601272583 and parameters: {'learning_rate': 0.03304103760465188, 'max_depth': 6, 'n_estimators': 133, 'subsample': 0.6127821058142462, 'colsample_bytree': 0.6561444374937805}. Best is trial 0 with value: 1.9923020601272583.\n",
            "[I 2025-10-07 11:23:48,578] Trial 1 finished with value: 2.1763784885406494 and parameters: {'learning_rate': 0.1406510317359297, 'max_depth': 6, 'n_estimators': 728, 'subsample': 0.8113231153699112, 'colsample_bytree': 0.8965410702818304}. Best is trial 0 with value: 1.9923020601272583.\n",
            "[I 2025-10-07 11:23:49,518] Trial 2 finished with value: 2.4892661571502686 and parameters: {'learning_rate': 0.08288109833655186, 'max_depth': 8, 'n_estimators': 507, 'subsample': 0.5540954728649181, 'colsample_bytree': 0.8888923772167576}. Best is trial 0 with value: 1.9923020601272583.\n",
            "[I 2025-10-07 11:23:50,887] Trial 3 finished with value: 2.3288888931274414 and parameters: {'learning_rate': 0.013110183519662274, 'max_depth': 7, 'n_estimators': 469, 'subsample': 0.9820230694897203, 'colsample_bytree': 0.8899557218671741}. Best is trial 0 with value: 1.9923020601272583.\n",
            "[I 2025-10-07 11:23:53,443] Trial 4 finished with value: 1.5305863618850708 and parameters: {'learning_rate': 0.00939048162885377, 'max_depth': 7, 'n_estimators': 923, 'subsample': 0.6434640419728024, 'colsample_bytree': 0.9491850547608716}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:23:54,793] Trial 5 finished with value: 1.7465912103652954 and parameters: {'learning_rate': 0.0136738868867301, 'max_depth': 8, 'n_estimators': 560, 'subsample': 0.5853748642794347, 'colsample_bytree': 0.9209397780975703}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:23:55,008] Trial 6 finished with value: 2.2019574642181396 and parameters: {'learning_rate': 0.001178486343843563, 'max_depth': 6, 'n_estimators': 106, 'subsample': 0.6566531298921932, 'colsample_bytree': 0.6544903676439005}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:23:55,835] Trial 7 finished with value: 2.0332279205322266 and parameters: {'learning_rate': 0.08281910840729717, 'max_depth': 9, 'n_estimators': 589, 'subsample': 0.7228604573991259, 'colsample_bytree': 0.6276165861922625}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:23:56,395] Trial 8 finished with value: 1.9318571090698242 and parameters: {'learning_rate': 0.027758245376395093, 'max_depth': 9, 'n_estimators': 264, 'subsample': 0.5094900868497849, 'colsample_bytree': 0.9087665138685161}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:23:56,811] Trial 9 finished with value: 1.979240894317627 and parameters: {'learning_rate': 0.29489023177811896, 'max_depth': 7, 'n_estimators': 320, 'subsample': 0.5489368263178107, 'colsample_bytree': 0.7271869122068213}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:23:58,138] Trial 10 finished with value: 1.889237403869629 and parameters: {'learning_rate': 0.0029595595755121606, 'max_depth': 3, 'n_estimators': 997, 'subsample': 0.8369093602263649, 'colsample_bytree': 0.5291292437055086}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:24:00,112] Trial 11 finished with value: 1.5544042587280273 and parameters: {'learning_rate': 0.007651685400553416, 'max_depth': 4, 'n_estimators': 977, 'subsample': 0.6774845234185833, 'colsample_bytree': 0.9970837183443281}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:24:02,004] Trial 12 finished with value: 1.5994900465011597 and parameters: {'learning_rate': 0.0047361508225600354, 'max_depth': 4, 'n_estimators': 961, 'subsample': 0.694531796396977, 'colsample_bytree': 0.9927353958909537}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:24:03,761] Trial 13 finished with value: 1.6768404245376587 and parameters: {'learning_rate': 0.0046115038073086334, 'max_depth': 4, 'n_estimators': 832, 'subsample': 0.7992378069815813, 'colsample_bytree': 0.995320806543117}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:24:05,752] Trial 14 finished with value: 1.6486778259277344 and parameters: {'learning_rate': 0.009121292337196218, 'max_depth': 5, 'n_estimators': 824, 'subsample': 0.647089868374308, 'colsample_bytree': 0.7920674602619484}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:24:07,835] Trial 15 finished with value: 2.140209913253784 and parameters: {'learning_rate': 0.0016624950237324927, 'max_depth': 10, 'n_estimators': 845, 'subsample': 0.9055947805692433, 'colsample_bytree': 0.808821630179043}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:24:08,961] Trial 16 finished with value: 1.7539987564086914 and parameters: {'learning_rate': 0.006490375201386928, 'max_depth': 3, 'n_estimators': 690, 'subsample': 0.7359179519899738, 'colsample_bytree': 0.9946082871320916}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:24:10,819] Trial 17 finished with value: 1.6466799974441528 and parameters: {'learning_rate': 0.026217063685801, 'max_depth': 5, 'n_estimators': 911, 'subsample': 0.6658599408456515, 'colsample_bytree': 0.826376130617887}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:24:12,375] Trial 18 finished with value: 1.9114731550216675 and parameters: {'learning_rate': 0.002167846714916392, 'max_depth': 5, 'n_estimators': 722, 'subsample': 0.7713103599556105, 'colsample_bytree': 0.9468203602341664}. Best is trial 4 with value: 1.5305863618850708.\n",
            "[I 2025-10-07 11:24:14,004] Trial 19 finished with value: 1.86748206615448 and parameters: {'learning_rate': 0.00952455467422948, 'max_depth': 4, 'n_estimators': 899, 'subsample': 0.6134286847533763, 'colsample_bytree': 0.8444702300168834}. Best is trial 4 with value: 1.5305863618850708.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.00939048162885377, 'max_depth': 7, 'n_estimators': 923, 'subsample': 0.6434640419728024, 'colsample_bytree': 0.9491850547608716}\n",
            "[Fold 2 | Model: xception] MAE: 1.8770, MSE: 5.7855, R: -0.3947\n",
            "\n",
            "--- Fold 3/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
            "  model = create_fn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/25]: 100%|| 4/4 [00:01<00:00,  2.13it/s, MSE=142]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 146.2105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/25]: 100%|| 4/4 [00:01<00:00,  2.14it/s, MSE=110]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 143.1499\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/25]: 100%|| 4/4 [00:02<00:00,  1.52it/s, MSE=137]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 140.1269\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 4/25]: 100%|| 4/4 [00:02<00:00,  1.53it/s, MSE=285]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 4 Avg MSE: 137.3260\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 5/25]: 100%|| 4/4 [00:01<00:00,  2.13it/s, MSE=102]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 5 Avg MSE: 134.4126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 6/25]: 100%|| 4/4 [00:03<00:00,  1.11it/s, MSE=52.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 6 Avg MSE: 131.2085\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 7/25]: 100%|| 4/4 [00:04<00:00,  1.03s/it, MSE=126]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 7 Avg MSE: 128.6615\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 8/25]: 100%|| 4/4 [00:05<00:00,  1.41s/it, MSE=49.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 8 Avg MSE: 125.9654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 9/25]: 100%|| 4/4 [00:01<00:00,  2.17it/s, MSE=92.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 9 Avg MSE: 123.1828\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 10/25]: 100%|| 4/4 [00:01<00:00,  2.11it/s, MSE=119]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 10 Avg MSE: 121.1091\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 11/25]: 100%|| 4/4 [00:01<00:00,  2.14it/s, MSE=276]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 11 Avg MSE: 118.1478\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 12/25]: 100%|| 4/4 [00:01<00:00,  2.17it/s, MSE=115]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 12 Avg MSE: 115.5423\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 13/25]: 100%|| 4/4 [00:02<00:00,  1.77it/s, MSE=107]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 13 Avg MSE: 113.0085\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 14/25]: 100%|| 4/4 [00:02<00:00,  1.38it/s, MSE=244]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 14 Avg MSE: 110.3368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 15/25]: 100%|| 4/4 [00:01<00:00,  2.10it/s, MSE=3.72]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 15 Avg MSE: 107.9513\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 16/25]: 100%|| 4/4 [00:01<00:00,  2.19it/s, MSE=119]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 16 Avg MSE: 105.8063\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 17/25]: 100%|| 4/4 [00:01<00:00,  2.22it/s, MSE=76.2]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 17 Avg MSE: 103.4168\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 18/25]: 100%|| 4/4 [00:01<00:00,  2.15it/s, MSE=184]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 18 Avg MSE: 101.1240\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 19/25]: 100%|| 4/4 [00:01<00:00,  2.14it/s, MSE=77.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 19 Avg MSE: 99.4902\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 20/25]: 100%|| 4/4 [00:02<00:00,  1.52it/s, MSE=225]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 20 Avg MSE: 97.0278\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 21/25]: 100%|| 4/4 [00:02<00:00,  1.46it/s, MSE=1.69]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 21 Avg MSE: 94.5588\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 22/25]: 100%|| 4/4 [00:01<00:00,  2.15it/s, MSE=122]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 22 Avg MSE: 92.5174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 23/25]: 100%|| 4/4 [00:01<00:00,  2.17it/s, MSE=69.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 23 Avg MSE: 89.9647\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 24/25]: 100%|| 4/4 [00:01<00:00,  2.17it/s, MSE=67.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 24 Avg MSE: 88.1071\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 25/25]: 100%|| 4/4 [00:01<00:00,  2.20it/s, MSE=16.1]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 25 Avg MSE: 86.2693\n",
            "[INFO] Partial fine-tuning complete.\n",
            "[INFO] Saved fine-tuned model to saved_models/xception_fold3.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:02<00:00,  1.78it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.17it/s]\n",
            "[I 2025-10-07 11:25:27,900] A new study created in memory with name: no-name-17f6bf9a-a4b4-47c9-9057-18644f6d1038\n",
            "[I 2025-10-07 11:25:29,037] Trial 0 finished with value: 1.7966408729553223 and parameters: {'learning_rate': 0.001612028738096862, 'max_depth': 9, 'n_estimators': 468, 'subsample': 0.8645992180782063, 'colsample_bytree': 0.7660840947543395}. Best is trial 0 with value: 1.7966408729553223.\n",
            "[I 2025-10-07 11:25:30,572] Trial 1 finished with value: 1.797258734703064 and parameters: {'learning_rate': 0.029746248048136564, 'max_depth': 6, 'n_estimators': 588, 'subsample': 0.7628463117210817, 'colsample_bytree': 0.8384596807793037}. Best is trial 0 with value: 1.7966408729553223.\n",
            "[I 2025-10-07 11:25:32,211] Trial 2 finished with value: 1.85639226436615 and parameters: {'learning_rate': 0.0025643234995539606, 'max_depth': 9, 'n_estimators': 753, 'subsample': 0.8054327871708449, 'colsample_bytree': 0.906607746035546}. Best is trial 0 with value: 1.7966408729553223.\n",
            "[I 2025-10-07 11:25:33,505] Trial 3 finished with value: 1.7346197366714478 and parameters: {'learning_rate': 0.0011772642462138756, 'max_depth': 7, 'n_estimators': 785, 'subsample': 0.5297982631889783, 'colsample_bytree': 0.7157301261538817}. Best is trial 3 with value: 1.7346197366714478.\n",
            "[I 2025-10-07 11:25:35,033] Trial 4 finished with value: 2.031780481338501 and parameters: {'learning_rate': 0.03618001184823132, 'max_depth': 8, 'n_estimators': 771, 'subsample': 0.8605209999442271, 'colsample_bytree': 0.9059250875200486}. Best is trial 3 with value: 1.7346197366714478.\n",
            "[I 2025-10-07 11:25:35,833] Trial 5 finished with value: 1.7879928350448608 and parameters: {'learning_rate': 0.00211858441928869, 'max_depth': 7, 'n_estimators': 447, 'subsample': 0.7442461059883587, 'colsample_bytree': 0.6946742843963363}. Best is trial 3 with value: 1.7346197366714478.\n",
            "[I 2025-10-07 11:25:36,949] Trial 6 finished with value: 1.881417155265808 and parameters: {'learning_rate': 0.0059053900476940735, 'max_depth': 6, 'n_estimators': 521, 'subsample': 0.9009588508041775, 'colsample_bytree': 0.6652599721036747}. Best is trial 3 with value: 1.7346197366714478.\n",
            "[I 2025-10-07 11:25:37,791] Trial 7 finished with value: 1.5522536039352417 and parameters: {'learning_rate': 0.05859933711124699, 'max_depth': 6, 'n_estimators': 449, 'subsample': 0.7820757400042916, 'colsample_bytree': 0.7579087617414704}. Best is trial 7 with value: 1.5522536039352417.\n",
            "[I 2025-10-07 11:25:38,738] Trial 8 finished with value: 1.5176602602005005 and parameters: {'learning_rate': 0.01653551032740012, 'max_depth': 5, 'n_estimators': 481, 'subsample': 0.5669308412950236, 'colsample_bytree': 0.6811369935678262}. Best is trial 8 with value: 1.5176602602005005.\n",
            "[I 2025-10-07 11:25:39,418] Trial 9 finished with value: 1.8552407026290894 and parameters: {'learning_rate': 0.14399620656065612, 'max_depth': 6, 'n_estimators': 588, 'subsample': 0.8298324297725038, 'colsample_bytree': 0.6318919843667651}. Best is trial 8 with value: 1.5176602602005005.\n",
            "[I 2025-10-07 11:25:39,637] Trial 10 finished with value: 1.897758960723877 and parameters: {'learning_rate': 0.009385463633489707, 'max_depth': 3, 'n_estimators': 148, 'subsample': 0.5058499252952919, 'colsample_bytree': 0.5138453102260201}. Best is trial 8 with value: 1.5176602602005005.\n",
            "[I 2025-10-07 11:25:40,117] Trial 11 finished with value: 1.858835220336914 and parameters: {'learning_rate': 0.10562889233936847, 'max_depth': 4, 'n_estimators': 280, 'subsample': 0.6337293160004988, 'colsample_bytree': 0.5687221122394032}. Best is trial 8 with value: 1.5176602602005005.\n",
            "[I 2025-10-07 11:25:40,808] Trial 12 finished with value: 1.568609356880188 and parameters: {'learning_rate': 0.05592472663458643, 'max_depth': 4, 'n_estimators': 320, 'subsample': 0.6597369009175957, 'colsample_bytree': 0.8001318643989468}. Best is trial 8 with value: 1.5176602602005005.\n",
            "[I 2025-10-07 11:25:41,313] Trial 13 finished with value: 1.766783356666565 and parameters: {'learning_rate': 0.2691254094499304, 'max_depth': 5, 'n_estimators': 362, 'subsample': 0.9647401430618928, 'colsample_bytree': 0.6050579761385699}. Best is trial 8 with value: 1.5176602602005005.\n",
            "[I 2025-10-07 11:25:43,590] Trial 14 finished with value: 1.487013339996338 and parameters: {'learning_rate': 0.016143721091298996, 'max_depth': 5, 'n_estimators': 934, 'subsample': 0.6489432913868869, 'colsample_bytree': 0.8370086449918083}. Best is trial 14 with value: 1.487013339996338.\n",
            "[I 2025-10-07 11:25:45,561] Trial 15 finished with value: 1.3778756856918335 and parameters: {'learning_rate': 0.009213330475284097, 'max_depth': 4, 'n_estimators': 997, 'subsample': 0.6106894022155847, 'colsample_bytree': 0.9622554230944502}. Best is trial 15 with value: 1.3778756856918335.\n",
            "[I 2025-10-07 11:25:47,187] Trial 16 finished with value: 1.3036270141601562 and parameters: {'learning_rate': 0.0053937110584864854, 'max_depth': 3, 'n_estimators': 988, 'subsample': 0.6539689585760149, 'colsample_bytree': 0.9965945337608685}. Best is trial 16 with value: 1.3036270141601562.\n",
            "[I 2025-10-07 11:25:48,795] Trial 17 finished with value: 1.229722499847412 and parameters: {'learning_rate': 0.006304530893857061, 'max_depth': 3, 'n_estimators': 995, 'subsample': 0.6958490201496997, 'colsample_bytree': 0.9966755741548922}. Best is trial 17 with value: 1.229722499847412.\n",
            "[I 2025-10-07 11:25:50,237] Trial 18 finished with value: 1.3197203874588013 and parameters: {'learning_rate': 0.00369529207193285, 'max_depth': 3, 'n_estimators': 882, 'subsample': 0.7004434291012481, 'colsample_bytree': 0.9982607799878124}. Best is trial 17 with value: 1.229722499847412.\n",
            "[I 2025-10-07 11:25:51,325] Trial 19 finished with value: 1.5593124628067017 and parameters: {'learning_rate': 0.005031552892564248, 'max_depth': 3, 'n_estimators': 674, 'subsample': 0.7066243907039766, 'colsample_bytree': 0.9469781779496066}. Best is trial 17 with value: 1.229722499847412.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.006304530893857061, 'max_depth': 3, 'n_estimators': 995, 'subsample': 0.6958490201496997, 'colsample_bytree': 0.9966755741548922}\n",
            "[Fold 3 | Model: xception] MAE: 1.4968, MSE: 6.3027, R: 0.1038\n",
            "\n",
            "--- Fold 4/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
            "  model = create_fn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/25]: 100%|| 4/4 [00:02<00:00,  1.96it/s, MSE=79.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 146.4870\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/25]: 100%|| 4/4 [00:02<00:00,  1.97it/s, MSE=113]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 143.3644\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/25]: 100%|| 4/4 [00:02<00:00,  1.97it/s, MSE=292]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 140.3864\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 4/25]: 100%|| 4/4 [00:02<00:00,  1.53it/s, MSE=128]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 4 Avg MSE: 137.2348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 5/25]: 100%|| 4/4 [00:02<00:00,  1.37it/s, MSE=239]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 5 Avg MSE: 134.2850\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 6/25]: 100%|| 4/4 [00:02<00:00,  1.99it/s, MSE=129]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 6 Avg MSE: 131.5315\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 7/25]: 100%|| 4/4 [00:01<00:00,  2.00it/s, MSE=278]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 7 Avg MSE: 128.3075\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 8/25]: 100%|| 4/4 [00:02<00:00,  1.91it/s, MSE=271]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 8 Avg MSE: 125.5695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 9/25]: 100%|| 4/4 [00:02<00:00,  1.94it/s, MSE=111]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 9 Avg MSE: 122.5822\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 10/25]: 100%|| 4/4 [00:02<00:00,  1.66it/s, MSE=90.8]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 10 Avg MSE: 120.0773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 11/25]: 100%|| 4/4 [00:03<00:00,  1.30it/s, MSE=88.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 11 Avg MSE: 116.8475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 12/25]: 100%|| 4/4 [00:02<00:00,  1.98it/s, MSE=111]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 12 Avg MSE: 114.3423\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 13/25]: 100%|| 4/4 [00:01<00:00,  2.01it/s, MSE=81.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 13 Avg MSE: 111.3900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 14/25]: 100%|| 4/4 [00:02<00:00,  1.64it/s, MSE=83.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 14 Avg MSE: 109.7349\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 15/25]: 100%|| 4/4 [00:01<00:00,  2.03it/s, MSE=200]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 15 Avg MSE: 106.8534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 16/25]: 100%|| 4/4 [00:02<00:00,  1.64it/s, MSE=58.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 16 Avg MSE: 103.5332\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 17/25]: 100%|| 4/4 [00:03<00:00,  1.30it/s, MSE=3.6]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 17 Avg MSE: 101.4879\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 18/25]: 100%|| 4/4 [00:02<00:00,  1.97it/s, MSE=82.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 18 Avg MSE: 100.2885\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 19/25]: 100%|| 4/4 [00:02<00:00,  1.96it/s, MSE=101]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 19 Avg MSE: 97.9517\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 20/25]: 100%|| 4/4 [00:02<00:00,  1.98it/s, MSE=136]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 20 Avg MSE: 95.0770\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 21/25]: 100%|| 4/4 [00:02<00:00,  1.89it/s, MSE=93.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 21 Avg MSE: 93.5303\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 22/25]: 100%|| 4/4 [00:02<00:00,  1.72it/s, MSE=65.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 22 Avg MSE: 90.5519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 23/25]: 100%|| 4/4 [00:03<00:00,  1.22it/s, MSE=60.8]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 23 Avg MSE: 89.2105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 24/25]: 100%|| 4/4 [00:02<00:00,  1.99it/s, MSE=58.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 24 Avg MSE: 86.5991\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 25/25]: 100%|| 4/4 [00:02<00:00,  1.96it/s, MSE=56.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 25 Avg MSE: 84.3841\n",
            "[INFO] Partial fine-tuning complete.\n",
            "[INFO] Saved fine-tuned model to saved_models/xception_fold4.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:02<00:00,  1.98it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  2.42it/s]\n",
            "[I 2025-10-07 11:26:59,310] A new study created in memory with name: no-name-9bfc6476-e485-46a5-acb0-e7c15932f677\n",
            "[I 2025-10-07 11:26:59,815] Trial 0 finished with value: 2.1397669315338135 and parameters: {'learning_rate': 0.004874540579357328, 'max_depth': 7, 'n_estimators': 237, 'subsample': 0.5355230803137337, 'colsample_bytree': 0.5978633512403578}. Best is trial 0 with value: 2.1397669315338135.\n",
            "[I 2025-10-07 11:27:00,092] Trial 1 finished with value: 1.2956093549728394 and parameters: {'learning_rate': 0.20427598201798255, 'max_depth': 7, 'n_estimators': 163, 'subsample': 0.9850605599620197, 'colsample_bytree': 0.8085439590348433}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:01,090] Trial 2 finished with value: 1.9431190490722656 and parameters: {'learning_rate': 0.0010403731762642463, 'max_depth': 6, 'n_estimators': 445, 'subsample': 0.9243631883448116, 'colsample_bytree': 0.8703456398209379}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:02,184] Trial 3 finished with value: 2.652893304824829 and parameters: {'learning_rate': 0.07941041125001286, 'max_depth': 7, 'n_estimators': 822, 'subsample': 0.7992409175986127, 'colsample_bytree': 0.7620853204651066}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:03,385] Trial 4 finished with value: 1.715671420097351 and parameters: {'learning_rate': 0.0028645042620654214, 'max_depth': 3, 'n_estimators': 700, 'subsample': 0.6786119649096627, 'colsample_bytree': 0.8927775054891233}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:04,450] Trial 5 finished with value: 2.7867870330810547 and parameters: {'learning_rate': 0.29947383216381074, 'max_depth': 7, 'n_estimators': 853, 'subsample': 0.5894111944884903, 'colsample_bytree': 0.5598624918722763}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:06,761] Trial 6 finished with value: 1.7204357385635376 and parameters: {'learning_rate': 0.012423206917661578, 'max_depth': 5, 'n_estimators': 919, 'subsample': 0.7411929772050703, 'colsample_bytree': 0.9090857172793837}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:09,069] Trial 7 finished with value: 2.230088472366333 and parameters: {'learning_rate': 0.009882846438820958, 'max_depth': 6, 'n_estimators': 964, 'subsample': 0.7718619748008884, 'colsample_bytree': 0.8151028943474801}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:11,051] Trial 8 finished with value: 2.201406240463257 and parameters: {'learning_rate': 0.007942797544635952, 'max_depth': 9, 'n_estimators': 862, 'subsample': 0.733221147956763, 'colsample_bytree': 0.7543155359201811}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:13,096] Trial 9 finished with value: 2.336228370666504 and parameters: {'learning_rate': 0.0055857304837843656, 'max_depth': 7, 'n_estimators': 989, 'subsample': 0.8130096146023192, 'colsample_bytree': 0.5254534233842778}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:13,401] Trial 10 finished with value: 2.0436456203460693 and parameters: {'learning_rate': 0.05143801289033392, 'max_depth': 10, 'n_estimators': 110, 'subsample': 0.9985778163865044, 'colsample_bytree': 0.6606409264134963}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:14,360] Trial 11 finished with value: 2.121009111404419 and parameters: {'learning_rate': 0.0014363330566660448, 'max_depth': 3, 'n_estimators': 615, 'subsample': 0.6795617967938117, 'colsample_bytree': 0.9983366782488342}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:15,290] Trial 12 finished with value: 2.456450939178467 and parameters: {'learning_rate': 0.03644001675558962, 'max_depth': 3, 'n_estimators': 586, 'subsample': 0.638297374909115, 'colsample_bytree': 0.9539011668385586}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:15,805] Trial 13 finished with value: 1.6235698461532593 and parameters: {'learning_rate': 0.2095063921937811, 'max_depth': 4, 'n_estimators': 414, 'subsample': 0.9001828303465025, 'colsample_bytree': 0.8374892625635487}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:16,320] Trial 14 finished with value: 1.6605277061462402 and parameters: {'learning_rate': 0.266531197662649, 'max_depth': 5, 'n_estimators': 379, 'subsample': 0.8955417751490841, 'colsample_bytree': 0.6960822222221386}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:16,919] Trial 15 finished with value: 1.4803386926651 and parameters: {'learning_rate': 0.12436466969357667, 'max_depth': 9, 'n_estimators': 291, 'subsample': 0.9860417490393784, 'colsample_bytree': 0.8179837186007}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:17,418] Trial 16 finished with value: 1.4615124464035034 and parameters: {'learning_rate': 0.11165781378456704, 'max_depth': 9, 'n_estimators': 218, 'subsample': 0.9756444736906182, 'colsample_bytree': 0.8101534544762679}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:17,785] Trial 17 finished with value: 2.1223371028900146 and parameters: {'learning_rate': 0.02652975114689118, 'max_depth': 9, 'n_estimators': 133, 'subsample': 0.8627452385131685, 'colsample_bytree': 0.6881382326900758}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:18,284] Trial 18 finished with value: 1.6957043409347534 and parameters: {'learning_rate': 0.13220874232784743, 'max_depth': 8, 'n_estimators': 234, 'subsample': 0.9652840027724664, 'colsample_bytree': 0.7785517605696679}. Best is trial 1 with value: 1.2956093549728394.\n",
            "[I 2025-10-07 11:27:18,965] Trial 19 finished with value: 2.5011074542999268 and parameters: {'learning_rate': 0.08515766537462968, 'max_depth': 10, 'n_estimators': 319, 'subsample': 0.8522211705890014, 'colsample_bytree': 0.6478953071427643}. Best is trial 1 with value: 1.2956093549728394.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.20427598201798255, 'max_depth': 7, 'n_estimators': 163, 'subsample': 0.9850605599620197, 'colsample_bytree': 0.8085439590348433}\n",
            "[Fold 4 | Model: xception] MAE: 2.1240, MSE: 5.3903, R: 0.3179\n",
            "\n",
            "--- Fold 5/5 ---\n",
            "[INFO] Training samples: 25, Validation samples: 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/_factory.py:138: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n",
            "  model = create_fn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Starting partial fine-tuning on 25 labeled samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 1/25]: 100%|| 4/4 [00:01<00:00,  2.41it/s, MSE=112]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 1 Avg MSE: 150.0670\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 2/25]: 100%|| 4/4 [00:01<00:00,  2.46it/s, MSE=109]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 2 Avg MSE: 147.1511\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 3/25]: 100%|| 4/4 [00:01<00:00,  2.47it/s, MSE=243]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 3 Avg MSE: 143.8255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 4/25]: 100%|| 4/4 [00:01<00:00,  2.43it/s, MSE=72.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 4 Avg MSE: 140.5941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 5/25]: 100%|| 4/4 [00:01<00:00,  2.48it/s, MSE=278]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 5 Avg MSE: 137.8427\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 6/25]: 100%|| 4/4 [00:02<00:00,  1.78it/s, MSE=124]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 6 Avg MSE: 134.6917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 7/25]: 100%|| 4/4 [00:02<00:00,  1.61it/s, MSE=65.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 7 Avg MSE: 131.9708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 8/25]: 100%|| 4/4 [00:01<00:00,  2.42it/s, MSE=122]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 8 Avg MSE: 128.6058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 9/25]: 100%|| 4/4 [00:01<00:00,  2.41it/s, MSE=279]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 9 Avg MSE: 126.7788\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 10/25]: 100%|| 4/4 [00:01<00:00,  2.49it/s, MSE=88.2]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 10 Avg MSE: 123.4538\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 11/25]: 100%|| 4/4 [00:01<00:00,  2.40it/s, MSE=117]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 11 Avg MSE: 120.3473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 12/25]: 100%|| 4/4 [00:01<00:00,  2.46it/s, MSE=253]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 12 Avg MSE: 118.1038\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 13/25]: 100%|| 4/4 [00:01<00:00,  2.26it/s, MSE=79.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 13 Avg MSE: 115.5255\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 14/25]: 100%|| 4/4 [00:02<00:00,  1.48it/s, MSE=76.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 14 Avg MSE: 112.3434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 15/25]: 100%|| 4/4 [00:02<00:00,  1.94it/s, MSE=83.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 15 Avg MSE: 111.5282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 16/25]: 100%|| 4/4 [00:01<00:00,  2.42it/s, MSE=50.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 16 Avg MSE: 107.4598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 17/25]: 100%|| 4/4 [00:01<00:00,  2.37it/s, MSE=95.4]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 17 Avg MSE: 105.6579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 18/25]: 100%|| 4/4 [00:01<00:00,  2.50it/s, MSE=70.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 18 Avg MSE: 103.2576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 19/25]: 100%|| 4/4 [00:01<00:00,  2.49it/s, MSE=36.8]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 19 Avg MSE: 101.4084\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 20/25]: 100%|| 4/4 [00:01<00:00,  2.49it/s, MSE=94.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 20 Avg MSE: 98.2740\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 21/25]: 100%|| 4/4 [00:02<00:00,  1.76it/s, MSE=87.9]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 21 Avg MSE: 97.0331\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 22/25]: 100%|| 4/4 [00:03<00:00,  1.26it/s, MSE=61.7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 22 Avg MSE: 93.8528\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 23/25]: 100%|| 4/4 [00:01<00:00,  2.02it/s, MSE=74]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 23 Avg MSE: 91.5046\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 24/25]: 100%|| 4/4 [00:01<00:00,  2.46it/s, MSE=60.3]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 24 Avg MSE: 90.7877\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Fine-Tune Epoch 25/25]: 100%|| 4/4 [00:01<00:00,  2.43it/s, MSE=248]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Fine-tune Epoch 25 Avg MSE: 90.3373\n",
            "[INFO] Partial fine-tuning complete.\n",
            "[INFO] Saved fine-tuned model to saved_models/xception_fold5.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting embeddings: 100%|| 4/4 [00:01<00:00,  2.45it/s]\n",
            "Extracting embeddings: 100%|| 1/1 [00:00<00:00,  1.29it/s]\n",
            "[I 2025-10-07 11:28:09,414] A new study created in memory with name: no-name-a99b9031-4317-43ea-b69e-575372386d5f\n",
            "[I 2025-10-07 11:28:10,128] Trial 0 finished with value: 3.3820858001708984 and parameters: {'learning_rate': 0.0032055587561326338, 'max_depth': 5, 'n_estimators': 373, 'subsample': 0.5012351716311688, 'colsample_bytree': 0.721252813151927}. Best is trial 0 with value: 3.3820858001708984.\n",
            "[I 2025-10-07 11:28:10,330] Trial 1 finished with value: 3.0146732330322266 and parameters: {'learning_rate': 0.029548769454754802, 'max_depth': 6, 'n_estimators': 116, 'subsample': 0.5057895694911403, 'colsample_bytree': 0.512961129840998}. Best is trial 1 with value: 3.0146732330322266.\n",
            "[I 2025-10-07 11:28:11,106] Trial 2 finished with value: 3.2332470417022705 and parameters: {'learning_rate': 0.07532478975913214, 'max_depth': 8, 'n_estimators': 314, 'subsample': 0.6753356979137533, 'colsample_bytree': 0.9265976937230511}. Best is trial 1 with value: 3.0146732330322266.\n",
            "[I 2025-10-07 11:28:11,755] Trial 3 finished with value: 3.356316328048706 and parameters: {'learning_rate': 0.0015394151471883266, 'max_depth': 3, 'n_estimators': 373, 'subsample': 0.6655293636580145, 'colsample_bytree': 0.7289927781823441}. Best is trial 1 with value: 3.0146732330322266.\n",
            "[I 2025-10-07 11:28:13,414] Trial 4 finished with value: 3.276888847351074 and parameters: {'learning_rate': 0.0015549858610029697, 'max_depth': 6, 'n_estimators': 630, 'subsample': 0.7803603227540772, 'colsample_bytree': 0.8952087178367754}. Best is trial 1 with value: 3.0146732330322266.\n",
            "[I 2025-10-07 11:28:13,687] Trial 5 finished with value: 2.814581871032715 and parameters: {'learning_rate': 0.2429086839902613, 'max_depth': 6, 'n_estimators': 117, 'subsample': 0.886012139417313, 'colsample_bytree': 0.8962900725200227}. Best is trial 5 with value: 2.814581871032715.\n",
            "[I 2025-10-07 11:28:14,151] Trial 6 finished with value: 3.2527990341186523 and parameters: {'learning_rate': 0.259130458438897, 'max_depth': 5, 'n_estimators': 348, 'subsample': 0.7825626441700887, 'colsample_bytree': 0.8829491728738861}. Best is trial 5 with value: 2.814581871032715.\n",
            "[I 2025-10-07 11:28:14,730] Trial 7 finished with value: 3.003419876098633 and parameters: {'learning_rate': 0.008529673215693956, 'max_depth': 10, 'n_estimators': 263, 'subsample': 0.753550709390225, 'colsample_bytree': 0.906200079404829}. Best is trial 5 with value: 2.814581871032715.\n",
            "[I 2025-10-07 11:28:15,323] Trial 8 finished with value: 2.361124277114868 and parameters: {'learning_rate': 0.1710756301210561, 'max_depth': 9, 'n_estimators': 490, 'subsample': 0.8722831730244445, 'colsample_bytree': 0.7005554548921529}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:17,604] Trial 9 finished with value: 2.98009991645813 and parameters: {'learning_rate': 0.01623071593045777, 'max_depth': 9, 'n_estimators': 973, 'subsample': 0.8550542889382564, 'colsample_bytree': 0.7774261088112939}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:18,569] Trial 10 finished with value: 2.7719767093658447 and parameters: {'learning_rate': 0.05876361627884632, 'max_depth': 8, 'n_estimators': 661, 'subsample': 0.981444195717786, 'colsample_bytree': 0.5952253559454244}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:19,417] Trial 11 finished with value: 2.9556143283843994 and parameters: {'learning_rate': 0.07486610911455331, 'max_depth': 8, 'n_estimators': 673, 'subsample': 0.997105918101256, 'colsample_bytree': 0.5888471937238469}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:20,346] Trial 12 finished with value: 3.432553291320801 and parameters: {'learning_rate': 0.08619648120372986, 'max_depth': 8, 'n_estimators': 807, 'subsample': 0.9992793530144816, 'colsample_bytree': 0.6310742243342284}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:21,488] Trial 13 finished with value: 3.1113383769989014 and parameters: {'learning_rate': 0.03337871401244094, 'max_depth': 10, 'n_estimators': 524, 'subsample': 0.9141771479834158, 'colsample_bytree': 0.6476477330680643}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:22,126] Trial 14 finished with value: 2.841850519180298 and parameters: {'learning_rate': 0.12087490055261638, 'max_depth': 9, 'n_estimators': 524, 'subsample': 0.9277695211417015, 'colsample_bytree': 0.5183156632333987}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:23,490] Trial 15 finished with value: 2.967846632003784 and parameters: {'learning_rate': 0.04007381073403006, 'max_depth': 7, 'n_estimators': 771, 'subsample': 0.8399274968071949, 'colsample_bytree': 0.8183862437107723}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:24,152] Trial 16 finished with value: 2.9320030212402344 and parameters: {'learning_rate': 0.15390407445993629, 'max_depth': 9, 'n_estimators': 475, 'subsample': 0.947864315669423, 'colsample_bytree': 0.6885698617312939}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:25,775] Trial 17 finished with value: 2.9387099742889404 and parameters: {'learning_rate': 0.010039865879376933, 'max_depth': 7, 'n_estimators': 655, 'subsample': 0.8302970427523357, 'colsample_bytree': 0.6002628302377078}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:27,322] Trial 18 finished with value: 2.8786399364471436 and parameters: {'learning_rate': 0.04573331035523111, 'max_depth': 10, 'n_estimators': 858, 'subsample': 0.6778359026749028, 'colsample_bytree': 0.8074967561112019}. Best is trial 8 with value: 2.361124277114868.\n",
            "[I 2025-10-07 11:28:28,028] Trial 19 finished with value: 2.724569082260132 and parameters: {'learning_rate': 0.1646325488428594, 'max_depth': 3, 'n_estimators': 724, 'subsample': 0.9603170839823342, 'colsample_bytree': 0.9788066761631464}. Best is trial 8 with value: 2.361124277114868.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Optuna] Best trial: {'learning_rate': 0.1710756301210561, 'max_depth': 9, 'n_estimators': 490, 'subsample': 0.8722831730244445, 'colsample_bytree': 0.7005554548921529}\n",
            "[Fold 5 | Model: xception] MAE: 2.6404, MSE: 14.1769, R: 0.0873\n",
            "\n",
            "===== Final Cross-Validation Results =====\n",
            "   fold     Model       MAE        MSE        R2\n",
            "0     1  xception  2.075172   4.569046  0.422058\n",
            "1     2  xception  1.876989   5.785467 -0.394742\n",
            "2     3  xception  1.496834   6.302677  0.103778\n",
            "3     4  xception  2.123972   5.390264  0.317880\n",
            "4     5  xception  2.640420  14.176872  0.087277\n",
            "\n",
            "Mean MAE: 2.0426774263381957\n",
            "Mean R: 0.10725014209747315\n"
          ]
        }
      ],
      "source": [
        "results = run_cnn_xgb_kfold_finetune(df, backbone_name=\"xception\", n_splits=5, fine_tune_epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJMqdPfUHDh"
      },
      "source": [
        "### Modular Partial Finetuning Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrmTMh7PUGtc"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------\n",
        "# cnn_xgb_dynamic_pipeline.py\n",
        "# --------------------------------------------------\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ============================================================\n",
        "# 1 Dataset class\n",
        "# ============================================================\n",
        "class ImageRegressionDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[\"Filename\"]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        label = torch.tensor(row[\"Hb\"], dtype=torch.float32)\n",
        "        return img, label\n",
        "\n",
        "# ============================================================\n",
        "# 2 Backbone Factory\n",
        "# ============================================================\n",
        "def get_backbone(backbone_name, pretrained=True, device=\"cuda\"):\n",
        "    \"\"\"Return CNN backbone and input size for the model\"\"\"\n",
        "    backbone_name = backbone_name.lower()\n",
        "    if backbone_name == \"resnet50\":\n",
        "        model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        input_size = 224\n",
        "    elif backbone_name == \"densenet121\":\n",
        "        model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1 if pretrained else None)\n",
        "        input_size = 224\n",
        "    elif backbone_name.startswith(\"efficientnet\"):\n",
        "        # EfficientNet B0B3\n",
        "        weights_attr = getattr(models, f\"{backbone_name.upper()}_Weights\", None)\n",
        "        model = getattr(models, backbone_name)(weights=weights_attr.IMAGENET1K_V1 if pretrained else None)\n",
        "        input_size = 224\n",
        "    elif backbone_name == \"xception\":\n",
        "        # Xception via pretrained torch hub (TensorFlow implementation)\n",
        "        model = torch.hub.load('tensorflow/models', 'xception', pretrained=True)\n",
        "        input_size = 299\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
        "\n",
        "    model.to(device)\n",
        "    return model, input_size\n",
        "\n",
        "# ============================================================\n",
        "# 3 Partial fine-tuning\n",
        "# ============================================================\n",
        "def fine_tune_backbone_partial(backbone, train_df, transform, device, epochs=3, batch_size=8, lr=1e-4):\n",
        "    print(f\"[INFO] Starting partial fine-tuning on {len(train_df)} labeled samples...\")\n",
        "\n",
        "    # Freeze all layers except last block and FC\n",
        "    for name, param in backbone.named_parameters():\n",
        "        if \"layer4\" not in name and \"fc\" not in name:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Replace final head with regression\n",
        "    if hasattr(backbone, \"fc\"):\n",
        "        in_features = backbone.fc.in_features\n",
        "        backbone.fc = nn.Linear(in_features, 1)\n",
        "    elif hasattr(backbone, \"classifier\"):\n",
        "        if isinstance(backbone.classifier, nn.Linear):\n",
        "            in_features = backbone.classifier.in_features\n",
        "            backbone.classifier = nn.Linear(in_features, 1)\n",
        "        else:\n",
        "            # For complex classifiers like Xception\n",
        "            backbone.classifier = nn.Linear(backbone.classifier[1].in_features, 1)\n",
        "    else:\n",
        "        raise ValueError(\"Backbone has unknown classifier head\")\n",
        "\n",
        "    dataset = ImageRegressionDataset(train_df, transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, backbone.parameters()), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    backbone.train()\n",
        "    backbone.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(loader, desc=f\"[Fine-Tune Epoch {epoch+1}/{epochs}]\")\n",
        "        for imgs, targets in pbar:\n",
        "            imgs, targets = imgs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = backbone(imgs).view(-1)\n",
        "            loss = criterion(preds, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item() * imgs.size(0)\n",
        "            pbar.set_postfix({\"MSE\": loss.item()})\n",
        "        avg_loss = total_loss / len(dataset)\n",
        "        print(f\"[INFO] Fine-tune Epoch {epoch+1} Avg MSE: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"[INFO] Partial fine-tuning complete.\")\n",
        "    return backbone\n",
        "\n",
        "# ============================================================\n",
        "# 4 Feature extraction\n",
        "# ============================================================\n",
        "def extract_embeddings(backbone, df, transform, device, batch_size=8):\n",
        "    backbone.eval()\n",
        "    embeddings, labels = [], []\n",
        "\n",
        "    # Remove final classifier for feature extraction\n",
        "    if hasattr(backbone, \"fc\"):\n",
        "        feature_extractor = nn.Sequential(*list(backbone.children())[:-1]).to(device)\n",
        "    elif hasattr(backbone, \"classifier\"):\n",
        "        feature_extractor = nn.Sequential(*list(backbone.children())[:-1]).to(device)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown backbone head\")\n",
        "\n",
        "    loader = DataLoader(ImageRegressionDataset(df, transform), batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, targets in tqdm(loader, desc=\"Extracting embeddings\"):\n",
        "            imgs = imgs.to(device)\n",
        "            feats = feature_extractor(imgs)\n",
        "            pooled = feats.view(feats.size(0), -1)\n",
        "            embeddings.append(pooled.cpu().numpy())\n",
        "            labels.append(targets.numpy())\n",
        "\n",
        "    embeddings = np.concatenate(embeddings)\n",
        "    labels = np.concatenate(labels)\n",
        "    return embeddings, labels\n",
        "\n",
        "# ============================================================\n",
        "# 5 XGBoost with Optuna\n",
        "# ============================================================\n",
        "def tune_xgb_with_optuna(X_train, y_train, X_val, y_val, n_trials=20):\n",
        "    def objective(trial):\n",
        "        params = {\n",
        "            \"objective\": \"reg:squarederror\",\n",
        "            \"eval_metric\": \"mae\",\n",
        "            \"tree_method\": \"hist\",\n",
        "            \"device\": \"cuda\",\n",
        "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
        "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        }\n",
        "        model = xgb.XGBRegressor(**params)\n",
        "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
        "        preds = model.predict(X_val)\n",
        "        return mean_absolute_error(y_val, preds)\n",
        "\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "    print(\"[Optuna] Best trial:\", study.best_trial.params)\n",
        "    return study.best_trial.params\n",
        "\n",
        "# ============================================================\n",
        "# 6 Full pipeline\n",
        "# ============================================================\n",
        "def run_cnn_xgb_kfold_finetune(\n",
        "    labeled_df,\n",
        "    backbone_name=\"resnet50\",\n",
        "    n_splits=5,\n",
        "    batch_size=8,\n",
        "    fine_tune_epochs=3,\n",
        "    xgb_trials=20,\n",
        "    device=None,\n",
        "    save_dir=\"models\"\n",
        "):\n",
        "    device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    backbone, input_size = get_backbone(backbone_name, pretrained=True, device=device)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((input_size, input_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    metrics = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(labeled_df), 1):\n",
        "        print(f\"\\n--- Fold {fold}/{n_splits} ---\")\n",
        "        train_df, val_df = labeled_df.iloc[train_idx], labeled_df.iloc[val_idx]\n",
        "\n",
        "        print(f\"[INFO] Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
        "\n",
        "        backbone_fold = fine_tune_backbone_partial(backbone, train_df, transform, device, epochs=fine_tune_epochs, batch_size=batch_size)\n",
        "        X_train, y_train = extract_embeddings(backbone_fold, train_df, transform, device, batch_size)\n",
        "        X_val, y_val = extract_embeddings(backbone_fold, val_df, transform, device, batch_size)\n",
        "\n",
        "        best_params = tune_xgb_with_optuna(X_train, y_train, X_val, y_val, n_trials=xgb_trials)\n",
        "        xgb_model = xgb.XGBRegressor(**best_params)\n",
        "        xgb_model.fit(X_train, y_train)\n",
        "\n",
        "        preds = xgb_model.predict(X_val)\n",
        "        mae = mean_absolute_error(y_val, preds)\n",
        "        mse = mean_squared_error(y_val, preds)\n",
        "        r2 = r2_score(y_val, preds)\n",
        "        print(f\"[Fold {fold}] MAE: {mae:.4f}, MSE: {mse:.4f}, R: {r2:.4f}\")\n",
        "        metrics.append({\"fold\": fold, \"MAE\": mae, \"MSE\": mse, \"R2\": r2, \"Backbone\": backbone_name})\n",
        "\n",
        "        # Save CNN backbone\n",
        "        torch.save(backbone_fold.state_dict(), os.path.join(save_dir, f\"{backbone_name}_fold{fold}.pth\"))\n",
        "        # Save XGBoost model\n",
        "        joblib.dump(xgb_model, os.path.join(save_dir, f\"xgb_{backbone_name}_fold{fold}.joblib\"))\n",
        "\n",
        "    results_df = pd.DataFrame(metrics)\n",
        "    print(\"\\n===== Final Cross-Validation Results =====\")\n",
        "    print(results_df)\n",
        "    print(\"\\nMean MAE:\", results_df[\"MAE\"].mean())\n",
        "    print(\"Mean R:\", results_df[\"R2\"].mean())\n",
        "    return results_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQKYXLCNURyR"
      },
      "source": [
        "#### Run Modular Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "48HuGWSmUU7u",
        "outputId": "5bf53d03-1584-4f16-c730-d43fee6a54fc"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "run_cnn_xgb_kfold_finetune() got an unexpected keyword argument 'backbone_fn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1492992531.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_cnn_xgb_kfold_finetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrun_cnn_xgb_kfold_finetune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackbone_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdensenet121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: run_cnn_xgb_kfold_finetune() got an unexpected keyword argument 'backbone_fn'"
          ]
        }
      ],
      "source": [
        "run_cnn_xgb_kfold_finetune(df, backbone_fn=models.resnet50)\n",
        "run_cnn_xgb_kfold_finetune(df, backbone_fn=models.densenet121)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MveOfdeN9aWm"
      },
      "source": [
        "# Resnet 34 has weaker performance and is much slower\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "la-evHwQ7M9Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "def run_pipeline_loocv(df,\n",
        "                        path_col=\"Filename\",\n",
        "                        target_col=\"Hb\",\n",
        "                        ssl_epochs=20,\n",
        "                        ssl_batch=8,\n",
        "                        ssl_lr=1e-3,\n",
        "                        extract_batch=8,\n",
        "                        optuna_trials=30,\n",
        "                        run_base_dir=\"models\"):\n",
        "    run_dir = make_run_dir(base=run_base_dir)\n",
        "    print(f\"Run artifacts will be saved in: {run_dir}\")\n",
        "\n",
        "    n_samples = len(df)\n",
        "    print(f\"Dataset size: {n_samples} samples\")\n",
        "\n",
        "    # Save original dataset\n",
        "    df.reset_index(drop=True).to_csv(os.path.join(run_dir, \"df_full.csv\"), index=False)\n",
        "\n",
        "    # ---------------------------\n",
        "    # SSL Pretraining on full dataset\n",
        "    # ---------------------------\n",
        "    print(\"\\n=== SSL Pretraining ===\")\n",
        "    backbone, proj_head, ssl_losses = pretrain_ssl(df, ssl_transform,\n",
        "                                                   epochs=ssl_epochs,\n",
        "                                                   batch_size=ssl_batch,\n",
        "                                                   lr=ssl_lr,\n",
        "                                                   num_workers=2,\n",
        "                                                   run_dir=run_dir,\n",
        "                                                   save_projection=True)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Extract embeddings for entire dataset\n",
        "    # ---------------------------\n",
        "    print(\"\\n=== Extracting Embeddings for LOOCV ===\")\n",
        "    X_all, y_all = extract_embeddings(df, backbone, val_transform,\n",
        "                                      path_col=path_col, target_col=target_col,\n",
        "                                      batch_size=extract_batch, num_workers=2)\n",
        "    X_all = combine_metadata(X_all, df)\n",
        "    print(f\"Feature matrix shape: {X_all.shape}\")\n",
        "\n",
        "    # Save embeddings\n",
        "    np.save(os.path.join(run_dir, \"X_all.npy\"), X_all)\n",
        "    np.save(os.path.join(run_dir, \"y_all.npy\"), y_all)\n",
        "    print(\" Saved extracted embeddings for full dataset.\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # LOOCV Evaluation\n",
        "    # ---------------------------\n",
        "    print(\"\\n=== LOOCV Hyperparameter Tuning + Evaluation ===\")\n",
        "    loo = LeaveOneOut()\n",
        "    y_preds = np.zeros_like(y_all)\n",
        "\n",
        "    # Optuna study for hyperparameter selection\n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
        "    # Use a simple subsample for tuning (all data minus 1 for LOOCV can be slow)\n",
        "    study.optimize(lambda t: objective(t, X_all, y_all, cv_splits=min(5, n_samples-1)), n_trials=optuna_trials)\n",
        "    best_params = study.best_params\n",
        "    print(f\"Best XGBoost params: {best_params}\")\n",
        "\n",
        "    try:\n",
        "        joblib.dump(study, os.path.join(run_dir, \"optuna_study.pkl\"))\n",
        "        print(\" Optuna study saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: couldn't save Optuna study object: {e}\")\n",
        "\n",
        "    # Train and predict using LOOCV\n",
        "    rmses = []\n",
        "    maes = []\n",
        "    for train_idx, val_idx in loo.split(X_all):\n",
        "        X_train, X_val = X_all[train_idx], X_all[val_idx]\n",
        "        y_train, y_val = y_all[train_idx], y_all[val_idx]\n",
        "\n",
        "        xgb_params = best_params.copy()\n",
        "        xgb_params.update({\"random_state\": RANDOM_SEED, \"n_jobs\": -1, \"tree_method\": \"hist\"})\n",
        "        model = xgb.XGBRegressor(**xgb_params)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_val)\n",
        "\n",
        "        y_preds[val_idx] = y_pred\n",
        "        rmses.append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
        "        maes.append(mean_absolute_error(y_val, y_pred))\n",
        "\n",
        "    rmse_val = np.mean(rmses)\n",
        "    mae_val = np.mean(maes)\n",
        "    print(f\"LOOCV RMSE: {rmse_val:.4f}\")\n",
        "    print(f\"LOOCV MAE: {mae_val:.4f}\")\n",
        "\n",
        "    # Save evaluation metrics\n",
        "    metrics_df = pd.DataFrame({\n",
        "        \"RMSE\": [rmse_val],\n",
        "        \"MAE\": [mae_val],\n",
        "        \"n_samples\": [n_samples],\n",
        "        \"optuna_trials\": [optuna_trials]\n",
        "    })\n",
        "    metrics_df.to_csv(os.path.join(run_dir, \"evaluation_metrics.csv\"), index=False)\n",
        "    print(f\" Evaluation metrics saved.\")\n",
        "\n",
        "    # Scatter plot\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.scatter(y_all, y_preds, alpha=0.6)\n",
        "    minv, maxv = float(np.min(y_all)), float(np.max(y_all))\n",
        "    plt.plot([minv, maxv], [minv, maxv], 'r--', lw=2)\n",
        "    plt.xlabel(\"Actual Hb\")\n",
        "    plt.ylabel(\"Predicted Hb\")\n",
        "    plt.title(f\"SSL Embeddings + XGBoost LOOCV\\nRMSE: {rmse_val:.4f} | MAE: {mae_val:.4f}\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(run_dir, \"loocv_validation_plot.png\"), dpi=300)\n",
        "    plt.close()\n",
        "    print(f\" LOOCV validation plot saved.\")\n",
        "\n",
        "    # Save model metadata / summary\n",
        "    summary = {\n",
        "        \"run_dir\": run_dir,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"device\": str(device),\n",
        "        \"ssl_epochs\": ssl_epochs,\n",
        "        \"ssl_batch\": ssl_batch,\n",
        "        \"optuna_trials\": optuna_trials,\n",
        "        \"best_params\": best_params,\n",
        "        \"rmse_val\": rmse_val,\n",
        "        \"mae_val\": mae_val,\n",
        "        \"n_samples\": n_samples\n",
        "    }\n",
        "    pd.Series(summary).to_json(os.path.join(run_dir, \"run_summary.json\"))\n",
        "\n",
        "    print(\"\\n=== LOOCV Run complete ===\")\n",
        "    return {\n",
        "        \"run_dir\": run_dir,\n",
        "        \"backbone_state_dict\": os.path.join(run_dir, \"ssl_backbone_state_dict.pth\"),\n",
        "        \"proj_head_state_dict\": os.path.join(run_dir, \"ssl_projection_head_state_dict.pth\"),\n",
        "        \"metrics\": os.path.join(run_dir, \"evaluation_metrics.csv\")\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSSbLpdXOgPX",
        "outputId": "22c53399-bbcd-46a2-8b2f-27890bc6790f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Run artifacts will be saved in: models/run_2025-10-06_15-58-35\n",
            "Dataset size: 31 samples\n",
            "\n",
            "=== SSL Pretraining ===\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|| 44.7M/44.7M [00:00<00:00, 56.3MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20 - SSL Loss: 1.7780\n",
            "Epoch 2/20 - SSL Loss: 1.6152\n",
            "Epoch 3/20 - SSL Loss: 1.4395\n",
            "Epoch 4/20 - SSL Loss: 1.5888\n",
            "Epoch 5/20 - SSL Loss: 1.4263\n",
            "Epoch 6/20 - SSL Loss: 1.4234\n",
            "Epoch 7/20 - SSL Loss: 1.5212\n",
            "Epoch 8/20 - SSL Loss: 1.6132\n",
            "Epoch 9/20 - SSL Loss: 1.8122\n",
            "Epoch 10/20 - SSL Loss: 1.4418\n",
            "Epoch 11/20 - SSL Loss: 1.4537\n",
            "Epoch 12/20 - SSL Loss: 1.6981\n",
            "Epoch 13/20 - SSL Loss: 1.2882\n",
            "Epoch 14/20 - SSL Loss: 1.4169\n",
            "Epoch 15/20 - SSL Loss: 1.2274\n",
            "Epoch 16/20 - SSL Loss: 1.3953\n",
            "Epoch 17/20 - SSL Loss: 1.3130\n",
            "Epoch 18/20 - SSL Loss: 1.3658\n",
            "Epoch 19/20 - SSL Loss: 1.3964\n",
            "Epoch 20/20 - SSL Loss: 1.6397\n",
            " SSL backbone state_dict saved to models/run_2025-10-06_15-58-35/ssl_backbone_state_dict.pth\n",
            " SSL projection head state_dict saved to models/run_2025-10-06_15-58-35/ssl_projection_head_state_dict.pth\n",
            "\n",
            "=== Extracting Embeddings for LOOCV ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-06 15:59:55,921] A new study created in memory with name: no-name-dffe7dc9-8c79-43c7-90e8-14888d819ce7\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature matrix shape: (31, 513)\n",
            " Saved extracted embeddings for full dataset.\n",
            "\n",
            "=== LOOCV Hyperparameter Tuning + Evaluation ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-10-06 15:59:57,663] Trial 0 finished with value: 3.060211280569418 and parameters: {'n_estimators': 218, 'max_depth': 10, 'learning_rate': 0.1205712628744377, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182}. Best is trial 0 with value: 3.060211280569418.\n",
            "[I 2025-10-06 15:59:58,239] Trial 1 finished with value: 3.0538722616530363 and parameters: {'n_estimators': 120, 'max_depth': 2, 'learning_rate': 0.19030368381735815, 'subsample': 0.8005575058716043, 'colsample_bytree': 0.8540362888980227}. Best is trial 1 with value: 3.0538722616530363.\n",
            "[I 2025-10-06 15:59:58,827] Trial 2 finished with value: 3.2767687527109843 and parameters: {'n_estimators': 59, 'max_depth': 10, 'learning_rate': 0.16967533607196555, 'subsample': 0.6061695553391381, 'colsample_bytree': 0.5909124836035503}. Best is trial 1 with value: 3.0538722616530363.\n",
            "[I 2025-10-06 16:00:00,388] Trial 3 finished with value: 2.9975918128786456 and parameters: {'n_estimators': 132, 'max_depth': 4, 'learning_rate': 0.05958389350068958, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021}. Best is trial 3 with value: 2.9975918128786456.\n",
            "[I 2025-10-06 16:00:03,356] Trial 4 finished with value: 2.954545467106916 and parameters: {'n_estimators': 325, 'max_depth': 3, 'learning_rate': 0.027010527749605478, 'subsample': 0.6831809216468459, 'colsample_bytree': 0.728034992108518}. Best is trial 4 with value: 2.954545467106916.\n",
            "[I 2025-10-06 16:00:05,233] Trial 5 finished with value: 3.1419708577289898 and parameters: {'n_estimators': 404, 'max_depth': 3, 'learning_rate': 0.05748924681991978, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 4 with value: 2.954545467106916.\n",
            "[I 2025-10-06 16:00:08,027] Trial 6 finished with value: 3.291737115663365 and parameters: {'n_estimators': 324, 'max_depth': 3, 'learning_rate': 0.012476394272569451, 'subsample': 0.9744427686266666, 'colsample_bytree': 0.9828160165372797}. Best is trial 4 with value: 2.954545467106916.\n",
            "[I 2025-10-06 16:00:11,590] Trial 7 finished with value: 3.081933857230844 and parameters: {'n_estimators': 414, 'max_depth': 4, 'learning_rate': 0.013940346079873234, 'subsample': 0.8421165132560784, 'colsample_bytree': 0.7200762468698007}. Best is trial 4 with value: 2.954545467106916.\n",
            "[I 2025-10-06 16:00:12,610] Trial 8 finished with value: 3.044512173144773 and parameters: {'n_estimators': 105, 'max_depth': 6, 'learning_rate': 0.011240768803005551, 'subsample': 0.954660201039391, 'colsample_bytree': 0.6293899908000085}. Best is trial 4 with value: 2.954545467106916.\n",
            "[I 2025-10-06 16:00:16,388] Trial 9 finished with value: 3.1481277126847047 and parameters: {'n_estimators': 348, 'max_depth': 4, 'learning_rate': 0.05864129169696527, 'subsample': 0.7733551396716398, 'colsample_bytree': 0.5924272277627636}. Best is trial 4 with value: 2.954545467106916.\n",
            "[I 2025-10-06 16:00:22,310] Trial 10 finished with value: 2.9744916830677965 and parameters: {'n_estimators': 480, 'max_depth': 7, 'learning_rate': 0.023969116039403743, 'subsample': 0.5089809378074098, 'colsample_bytree': 0.8200442512337782}. Best is trial 4 with value: 2.954545467106916.\n",
            "[I 2025-10-06 16:00:28,641] Trial 11 finished with value: 2.9246093580601853 and parameters: {'n_estimators': 460, 'max_depth': 7, 'learning_rate': 0.02610016466426293, 'subsample': 0.5324266600311697, 'colsample_bytree': 0.8218563683119117}. Best is trial 11 with value: 2.9246093580601853.\n",
            "[I 2025-10-06 16:00:36,424] Trial 12 finished with value: 2.8836609639896267 and parameters: {'n_estimators': 498, 'max_depth': 8, 'learning_rate': 0.026438783878681333, 'subsample': 0.6332990521098741, 'colsample_bytree': 0.7598978217164294}. Best is trial 12 with value: 2.8836609639896267.\n",
            "[I 2025-10-06 16:00:43,429] Trial 13 finished with value: 2.983630435977344 and parameters: {'n_estimators': 487, 'max_depth': 8, 'learning_rate': 0.028398209987516264, 'subsample': 0.5025309641374691, 'colsample_bytree': 0.8395385992590928}. Best is trial 12 with value: 2.8836609639896267.\n",
            "[I 2025-10-06 16:00:49,365] Trial 14 finished with value: 3.0065865282118223 and parameters: {'n_estimators': 428, 'max_depth': 8, 'learning_rate': 0.03720181861619697, 'subsample': 0.6122178017071738, 'colsample_bytree': 0.932267465370485}. Best is trial 12 with value: 2.8836609639896267.\n",
            "[I 2025-10-06 16:00:51,772] Trial 15 finished with value: 2.952718311607496 and parameters: {'n_estimators': 236, 'max_depth': 6, 'learning_rate': 0.018557958741270655, 'subsample': 0.5897854639658634, 'colsample_bytree': 0.8008525255027212}. Best is trial 12 with value: 2.8836609639896267.\n",
            "[I 2025-10-06 16:00:57,310] Trial 16 finished with value: 3.0929628583204165 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.08719209000999652, 'subsample': 0.6636313316285372, 'colsample_bytree': 0.9047894591765572}. Best is trial 12 with value: 2.8836609639896267.\n",
            "[I 2025-10-06 16:01:02,285] Trial 17 finished with value: 2.835843721734254 and parameters: {'n_estimators': 447, 'max_depth': 9, 'learning_rate': 0.03812162884608817, 'subsample': 0.5544242831476445, 'colsample_bytree': 0.7581379014222452}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:06,309] Trial 18 finished with value: 2.9080088640208968 and parameters: {'n_estimators': 390, 'max_depth': 9, 'learning_rate': 0.04104975015072629, 'subsample': 0.5739209174853697, 'colsample_bytree': 0.6746391632498865}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:08,425] Trial 19 finished with value: 3.0500532176409862 and parameters: {'n_estimators': 283, 'max_depth': 9, 'learning_rate': 0.2702272241241035, 'subsample': 0.6531209891432997, 'colsample_bytree': 0.7817453660591143}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:12,784] Trial 20 finished with value: 2.949798743354006 and parameters: {'n_estimators': 377, 'max_depth': 9, 'learning_rate': 0.08640538171836266, 'subsample': 0.5523223651814315, 'colsample_bytree': 0.7567050185620947}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:17,505] Trial 21 finished with value: 2.9461852786039144 and parameters: {'n_estimators': 451, 'max_depth': 9, 'learning_rate': 0.03894135852215067, 'subsample': 0.5751337112738503, 'colsample_bytree': 0.6835838243965024}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:22,658] Trial 22 finished with value: 2.9002088623405298 and parameters: {'n_estimators': 381, 'max_depth': 10, 'learning_rate': 0.043854005947887416, 'subsample': 0.6293066655416109, 'colsample_bytree': 0.6904168369538362}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:27,884] Trial 23 finished with value: 2.934381765133157 and parameters: {'n_estimators': 443, 'max_depth': 10, 'learning_rate': 0.01811351035917006, 'subsample': 0.6400277172366741, 'colsample_bytree': 0.6955786023963347}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:32,085] Trial 24 finished with value: 2.9481457235765336 and parameters: {'n_estimators': 348, 'max_depth': 7, 'learning_rate': 0.04471660241762028, 'subsample': 0.6803395985027783, 'colsample_bytree': 0.7552101503990056}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:36,610] Trial 25 finished with value: 3.0938536475736433 and parameters: {'n_estimators': 375, 'max_depth': 10, 'learning_rate': 0.07477274499016062, 'subsample': 0.7426025608345339, 'colsample_bytree': 0.7278705067404733}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:42,905] Trial 26 finished with value: 2.86316069789551 and parameters: {'n_estimators': 460, 'max_depth': 8, 'learning_rate': 0.019496182621765272, 'subsample': 0.6275309441969383, 'colsample_bytree': 0.8789876711794249}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:50,843] Trial 27 finished with value: 3.006730626127367 and parameters: {'n_estimators': 461, 'max_depth': 8, 'learning_rate': 0.017708374928644008, 'subsample': 0.7268302678751436, 'colsample_bytree': 0.8801147088820881}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:56,780] Trial 28 finished with value: 3.1611469294036674 and parameters: {'n_estimators': 426, 'max_depth': 5, 'learning_rate': 0.03199938542426571, 'subsample': 0.8941468837236076, 'colsample_bytree': 0.9987182963515119}. Best is trial 17 with value: 2.835843721734254.\n",
            "[I 2025-10-06 16:01:58,902] Trial 29 finished with value: 2.856689633321119 and parameters: {'n_estimators': 193, 'max_depth': 7, 'learning_rate': 0.02260913262163145, 'subsample': 0.5437869894948617, 'colsample_bytree': 0.9385466467210914}. Best is trial 17 with value: 2.835843721734254.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best XGBoost params: {'n_estimators': 447, 'max_depth': 9, 'learning_rate': 0.03812162884608817, 'subsample': 0.5544242831476445, 'colsample_bytree': 0.7581379014222452}\n",
            " Optuna study saved.\n",
            "LOOCV RMSE: 1.8309\n",
            "LOOCV MAE: 1.8309\n",
            " Evaluation metrics saved.\n",
            " LOOCV validation plot saved.\n",
            "\n",
            "=== LOOCV Run complete ===\n",
            "Artifacts saved to: models/run_2025-10-06_15-58-35\n"
          ]
        }
      ],
      "source": [
        "results = run_pipeline_loocv(df,\n",
        "                           path_col=\"Filename\",\n",
        "                           target_col=\"Hb\",\n",
        "                           ssl_epochs=20,\n",
        "                           ssl_batch=8,\n",
        "                           ssl_lr=1e-3,\n",
        "                           extract_batch=8,\n",
        "                           optuna_trials=30,\n",
        "                           run_base_dir=\"models\")\n",
        "\n",
        "print(\"Artifacts saved to:\", results[\"run_dir\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "156ef0eda8504397aa5495e5dcd2ba43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22e06509b67c4c098760a4a3dd0d4c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23830de5000342f2af13414addefa744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_390db69d337644bd9a984d4b7e94ac1e",
            "max": 102469840,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_156ef0eda8504397aa5495e5dcd2ba43",
            "value": 102469840
          }
        },
        "390db69d337644bd9a984d4b7e94ac1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75b3ddf6450e403bb1dbb8d6b5d8a417": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce78233a233b47bdb25e9d43c38784ed",
            "placeholder": "",
            "style": "IPY_MODEL_22e06509b67c4c098760a4a3dd0d4c09",
            "value": "102M/102M[00:01&lt;00:00,131MB/s]"
          }
        },
        "79da2476cb934d329ec0650f4146dca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f477111d3f0a48558e1bcc42674e6d94",
            "placeholder": "",
            "style": "IPY_MODEL_dbe03b9b96b24f1b979af43d2beb1601",
            "value": "model.safetensors:100%"
          }
        },
        "7ba41850fa8d4100828e5f655899321d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79da2476cb934d329ec0650f4146dca3",
              "IPY_MODEL_23830de5000342f2af13414addefa744",
              "IPY_MODEL_75b3ddf6450e403bb1dbb8d6b5d8a417"
            ],
            "layout": "IPY_MODEL_92ef309f4e16493893c5bfa19eaf4019"
          }
        },
        "92ef309f4e16493893c5bfa19eaf4019": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce78233a233b47bdb25e9d43c38784ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe03b9b96b24f1b979af43d2beb1601": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f477111d3f0a48558e1bcc42674e6d94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
